[
  {
    "objectID": "w13/slides.html#roadmap",
    "href": "w13/slides.html#roadmap",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Roadmap",
    "text": "Roadmap\n\n\n\n\n What makes causation different from correlation?\n\n\n\nWhy can‚Äôt we use, e.g., Regression to infer causal effects? \\(\\uparrow X\\) by 1 unit causes \\(\\uparrow Y\\) by \\(\\beta\\) units?\n\\(\\leadsto\\) Fundamental Problem of Causal Inference\n\n\n\n\n\n\n\n\n\n Key to resolving Fundamental Problem: Match similar observations\n\n\n\nApples to apples: If \\(j\\) receives drug while \\(i\\) doesn‚Äôt, and they‚Äôre \\(s_{ij}\\%\\) similar otherwise (age, height)‚Ä¶\nHigher \\(s_{ij}\\) \\(\\implies\\) more confidence in attributing difference in outcomes \\(\\boxed{\\Delta y = y_j - y_i}\\) to drug!\n\\(\\leadsto\\) Propensity Score Matching (\\(\\approx\\) Logistic Regression)\n\n\n\n\n\n\n\n\n\n How can ML help us infer counterfactual effects?\n\n\n\nPatient \\(i\\) didn‚Äôt receive treatment, reported VAS pain level \\(y^0_i = 80\\)‚Ä¶\nIf \\(i\\) had received treatment, what would their pain level \\(y_i^1\\) be?\n\\(\\leadsto\\) Causal Forests, to estimate \\(\\boxed{\\Delta y_i = y^1_i - y^0_i}\\)"
  },
  {
    "objectID": "w13/slides.html#the-fundamental-problem-of-causal-inference-1",
    "href": "w13/slides.html#the-fundamental-problem-of-causal-inference-1",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ‚Äú\\(X\\) causes \\(Y\\)‚Äù:\n\n\n\n\n Defining Causality (Hume 1739)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where everything is exactly the same‚Ä¶\n‚Ä¶except that \\(\\boxed{X = 0 \\text{  in  } W_0}\\) and \\(\\boxed{X = 1 \\text{  in  } W_1}\\),\n\\(\\boxed{Y = 0 \\text{  in  } W_0}\\) and \\(\\boxed{Y = 1 \\text{  in  } W_1}\\).\n\n\n\n\n\n\n\nThe problem? We live in one world, not two simultaneous worlds üò≠"
  },
  {
    "objectID": "w13/slides.html#cant-we-just-use-temporal-precedence",
    "href": "w13/slides.html#cant-we-just-use-temporal-precedence",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Can‚Äôt We Just Use Temporal Precedence?",
    "text": "Can‚Äôt We Just Use Temporal Precedence?\n\nCan‚Äôt we just pretend that \\(W_0\\) is our world at time \\(t\\) and \\(W_1\\) is our world at time \\(t + 1\\)?\nDid throwing the eraser at Sam at time \\(t\\) cause him to be upset at time \\(t + 1\\)?\nNo, because at time \\(t\\), simultaneous with my eraser-throwing, a cockroach scuttled across his foot, the true cause of him being upset at time \\(t + 1\\)\nWithout knowing that the worlds are identical except for the posited cause-event, we can‚Äôt exclude the possibility of some other cause-event"
  },
  {
    "objectID": "w13/slides.html#extreme-example-super-mario-64-speedrunning",
    "href": "w13/slides.html#extreme-example-super-mario-64-speedrunning",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Extreme Example: Super Mario 64 Speedrunning",
    "text": "Extreme Example: Super Mario 64 Speedrunning\nSeemingly-reasonable assumption: Button-pushes cause outcomes in games‚Ä¶\n\n\n\nDuring the race, an ionizing particle from outer space collided with DOTA_Teabag‚Äôs N64, flipping the eighth bit of Mario‚Äôs first height byte. Specifically, it flipped the byte from 11000101 to 11000100, from ‚ÄúC5‚Äù to ‚ÄúC4‚Äù. This resulted in a height change from C5837800 to C4837800, which by complete chance, happened to be the exact amount needed to warp Mario up to the higher floor at that exact moment.\n\n\n\n\n\nArticle from TheGamer.com\n\n\n\n\nThis was tested by pannenkoek12 - the same person who put up the bounty - using a script that manually flipped that particular bit at the right time, confirming the suspicion of a bit flip."
  },
  {
    "objectID": "w13/slides.html#what-about-a-b-testing",
    "href": "w13/slides.html#what-about-a-b-testing",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "What About A-B Testing?",
    "text": "What About A-B Testing?\n\nGets us significantly closer, but methods for recovering causal effect require a condition called SUTVA\nStable Unit Treatment Value Assumption: Treatment applied to \\(i\\) does not affect outcome for another person \\(j\\)\nIf we A-B test an app redesign (A = old design, B = new design), and outcome = length of time spent on app‚Ä¶\nPerson \\(i\\) seeing design A may like the new design, causing them to spend more time on the app\nPerson \\(i\\) may then message person \\(j\\) ‚ÄúCheck out [app], they redesigned everything!‚Äù, causing \\(j\\) to spend more time on app regardless of treatment (network spillover ‚ùå)"
  },
  {
    "objectID": "w13/slides.html#what-is-to-be-done",
    "href": "w13/slides.html#what-is-to-be-done",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w13/slides.html#case-study-military-inequality-leadsto-military-success",
    "href": "w13/slides.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)"
  },
  {
    "objectID": "w13/slides.html#does-inequality-cause-poor-military-performance",
    "href": "w13/slides.html#does-inequality-cause-poor-military-performance",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes"
  },
  {
    "objectID": "w13/slides.html#glorified-logistic-regression",
    "href": "w13/slides.html#glorified-logistic-regression",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "‚Ä¶Glorified Logistic Regression!",
    "text": "‚Ä¶Glorified Logistic Regression!\n\n\nSimilarity score via Logistic Regression! Let‚Äôs look at a program that built health clinics in several villages: did health clinics cause lower infant mortality?\n\n\n\n\n\nCode\nlibrary(tidyverse)\nvillage_df &lt;- tribble(\n  ~village_id, ~T, ~inf_mortality, \n  1, 1, 10,\n  2, 1, 15,\n  3, 1, 22,\n  4, 1, 19,\n  5, 0, 25,\n  6, 0, 19,\n  7, 0, 4,\n  8, 0, 8,\n  9, 0, 6\n) |&gt; mutate(T = factor(T))\nvillage_df\n\n\n\n\n\n\nvillage_id\nT\ninf_mortality\n\n\n\n\n1\n1\n10\n\n\n2\n1\n15\n\n\n3\n1\n22\n\n\n4\n1\n19\n\n\n5\n0\n25\n\n\n6\n0\n19\n\n\n7\n0\n4\n\n\n8\n0\n8\n\n\n9\n0\n6\n\n\n\n\n\n\n\n\n\nCode\nvillage_df |&gt; group_by(T) |&gt;\n  summarize(mean_mortality = mean(inf_mortality)) |&gt;\n  arrange(desc(T))\n\n\n\n\n\n\nT\nmean_mortality\n\n\n\n\n1\n16.5\n\n\n0\n12.4\n\n\n\n\n\n\nHealth clinics increased mortality by 4.1?"
  },
  {
    "objectID": "w13/slides.html#from-controlling-for-to-how-well-are-we-controlling-for",
    "href": "w13/slides.html#from-controlling-for-to-how-well-are-we-controlling-for",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "From ‚ÄúControlling For‚Äù to ‚ÄúHow Well Are We Controlling For?‚Äù",
    "text": "From ‚ÄúControlling For‚Äù to ‚ÄúHow Well Are We Controlling For?‚Äù\n\nBy introducing covariates, we can see the selection bias at play‚Ä¶\n\n\n\nCode\ncovar_df &lt;- tribble(\n  ~poverty_rate, ~docs_per_capita,\n  0.5, 0.01,\n  0.6, 0.02,\n  0.7, 0.01,\n  0.6, 0.02,\n  0.6, 0.01,\n  0.5, 0.02,\n  0.1, 0.04,\n  0.3, 0.05,\n  0.2, 0.04,\n)\nvillage_df &lt;- bind_cols(village_df, covar_df)\nvillage_df\n\n\n\n\n\n\nvillage_id\nT\ninf_mortality\npoverty_rate\ndocs_per_capita\n\n\n\n\n1\n1\n10\n0.5\n0.01\n\n\n2\n1\n15\n0.6\n0.02\n\n\n3\n1\n22\n0.7\n0.01\n\n\n4\n1\n19\n0.6\n0.02\n\n\n5\n0\n25\n0.6\n0.01\n\n\n6\n0\n19\n0.5\n0.02\n\n\n7\n0\n4\n0.1\n0.04\n\n\n8\n0\n8\n0.3\n0.05\n\n\n9\n0\n6\n0.2\n0.04"
  },
  {
    "objectID": "w13/slides.html#selection-bias",
    "href": "w13/slides.html#selection-bias",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Selection Bias",
    "text": "Selection Bias\n\n\n\n\nCode\nvillage_df |&gt; ggplot(aes(x = poverty_rate, fill=T)) +\n  geom_density(alpha=0.5) +\n  theme_dsan(base_size=30) +\n  labs(\n    title = \"Poverty Rate by Treatment\",\n    x = \"Poverty Rate\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nvillage_df |&gt; ggplot(aes(x = docs_per_capita, fill=T)) +\n  geom_density(alpha=0.5) +\n  theme_dsan(base_size=30) +\n  labs(\n    title = \"Doctors per Capita by Treatment\",\n    x = \"Doctors per Capita\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\\(\\leadsto\\) We‚Äôre not comparing apples to apples! (‚ÄúWell, we‚Äôre both villages‚Äù)"
  },
  {
    "objectID": "w13/slides.html#logistic-regression-of-treatment",
    "href": "w13/slides.html#logistic-regression-of-treatment",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Logistic Regression of Treatment",
    "text": "Logistic Regression of Treatment\n\n\nCode\nprop_model &lt;- glm(\n  T ~ poverty_rate + docs_per_capita,\n  data=village_df, family=\"binomial\"\n)\nsummary(prop_model)\n\n\n\nCall:\nglm(formula = T ~ poverty_rate + docs_per_capita, family = \"binomial\", \n    data = village_df)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)       -7.498      8.992  -0.834    0.404\npoverty_rate      14.500     13.651   1.062    0.288\ndocs_per_capita   -8.880    143.595  -0.062    0.951\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 12.3653  on 8  degrees of freedom\nResidual deviance:  6.9987  on 6  degrees of freedom\nAIC: 12.999\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nWe now have a model of selection bias! \\(\\leadsto\\) match observations with similar \\(\\Pr(T)\\)"
  },
  {
    "objectID": "w13/slides.html#propensity-score-logistic-regression-estimate",
    "href": "w13/slides.html#propensity-score-logistic-regression-estimate",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Propensity Score = Logistic Regression Estimate",
    "text": "Propensity Score = Logistic Regression Estimate\n\n\nCode\nvillage_df$ps &lt;- predict(prop_model, village_df, type=\"response\")\nvillage_df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvillage_id\nT\ninf_mortality\npoverty_rate\ndocs_per_capita\nps\n\n\n\n\n1\n1\n10\n0.5\n0.01\n0.4165712\n\n\n2\n1\n15\n0.6\n0.02\n0.7358171\n\n\n3\n1\n22\n0.7\n0.01\n0.9284516\n\n\n4\n1\n19\n0.6\n0.02\n0.7358171\n\n\n5\n0\n25\n0.6\n0.01\n0.7527140\n\n\n6\n0\n19\n0.5\n0.02\n0.3951619\n\n\n7\n0\n4\n0.1\n0.04\n0.0016534\n\n\n8\n0\n8\n0.3\n0.05\n0.0268029\n\n\n9\n0\n6\n0.2\n0.04\n0.0070107"
  },
  {
    "objectID": "w13/slides.html#propensity-score-matching-distance-metric",
    "href": "w13/slides.html#propensity-score-matching-distance-metric",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Propensity Score Matching = Distance Metric!",
    "text": "Propensity Score Matching = Distance Metric!\n\n\n\nCode\ncur_T &lt;- village_df[1,\"T\"] |&gt; pull()\ncur_ps &lt;- village_df[1,\"ps\"] |&gt; pull()\nwriteLines(paste0(\"Current village: T = \",cur_T,\", ps = \",cur_ps))\n\n\nCurrent village: T = 1, ps = 0.416571242858422\n\n\nCode\nother_df &lt;- village_df |&gt; filter(T != cur_T) |&gt;\n  mutate(\n    ps_dist = abs(ps - cur_ps)\n  )\nother_df |&gt; select(-c(inf_mortality))\n\n\n\n\n\n\nvillage_id\nT\npoverty_rate\ndocs_per_capita\nps\nps_dist\n\n\n\n\n5\n0\n0.6\n0.01\n0.7527140\n0.3361428\n\n\n6\n0\n0.5\n0.02\n0.3951619\n0.0214093\n\n\n7\n0\n0.1\n0.04\n0.0016534\n0.4149179\n\n\n8\n0\n0.3\n0.05\n0.0268029\n0.3897683\n\n\n9\n0\n0.2\n0.04\n0.0070107\n0.4095605"
  },
  {
    "objectID": "w13/slides.html#now-in-a-for-loop",
    "href": "w13/slides.html#now-in-a-for-loop",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Now in a For Loop‚Ä¶",
    "text": "Now in a For Loop‚Ä¶\n\n\nCode\nfor (i in 1:9) {\n  cur_T &lt;- village_df[i,\"T\"] |&gt; pull()\n  cur_ps &lt;- village_df[i,\"ps\"] |&gt; pull()\n  # writeLines(paste0(\"Current village: T = \",cur_T,\", ps = \",cur_ps))\n  other_df &lt;- village_df |&gt; filter(T != cur_T) |&gt;\n    mutate(\n      ps_dist = abs(ps - cur_ps)\n    )\n  match_id &lt;- names(which.min(other_df$ps_dist))\n  village_df[i,\"match\"] &lt;- as.numeric(match_id)\n}\nvillage_df |&gt; select(-inf_mortality)\n\n\n\n\n\n\nvillage_id\nT\npoverty_rate\ndocs_per_capita\nps\nmatch\n\n\n\n\n1\n1\n0.5\n0.01\n0.4165712\n6\n\n\n2\n1\n0.6\n0.02\n0.7358171\n5\n\n\n3\n1\n0.7\n0.01\n0.9284516\n5\n\n\n4\n1\n0.6\n0.02\n0.7358171\n5\n\n\n5\n0\n0.6\n0.01\n0.7527140\n2\n\n\n6\n0\n0.5\n0.02\n0.3951619\n1\n\n\n7\n0\n0.1\n0.04\n0.0016534\n1\n\n\n8\n0\n0.3\n0.05\n0.0268029\n1\n\n\n9\n0\n0.2\n0.04\n0.0070107\n1"
  },
  {
    "objectID": "w13/slides.html#and-now-we-compare-apples-to-apples",
    "href": "w13/slides.html#and-now-we-compare-apples-to-apples",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "And Now We Compare Apples to Apples‚Ä¶",
    "text": "And Now We Compare Apples to Apples‚Ä¶\n\n\nCode\ntreated_df &lt;- village_df |&gt; filter(T == 1)\n(matched_df &lt;- treated_df |&gt; left_join(village_df, join_by(match == village_id)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvillage_id\nT.x\ninf_mortality.x\npoverty_rate.x\ndocs_per_capita.x\nps.x\nmatch\nT.y\ninf_mortality.y\npoverty_rate.y\ndocs_per_capita.y\nps.y\nmatch.y\n\n\n\n\n1\n1\n10\n0.5\n0.01\n0.4165712\n6\n0\n19\n0.5\n0.02\n0.3951619\n1\n\n\n2\n1\n15\n0.6\n0.02\n0.7358171\n5\n0\n25\n0.6\n0.01\n0.7527140\n2\n\n\n3\n1\n22\n0.7\n0.01\n0.9284516\n5\n0\n25\n0.6\n0.01\n0.7527140\n2\n\n\n4\n1\n19\n0.6\n0.02\n0.7358171\n5\n0\n25\n0.6\n0.01\n0.7527140\n2\n\n\n\n\n\n\n\n\nCode\nmatched_df |&gt; summarize(\n  mean_tr = mean(inf_mortality.x),\n  mean_control = mean(inf_mortality.y)\n)\n\n\n\n\n\n\nmean_tr\nmean_control\n\n\n\n\n16.5\n23.5\n\n\n\n\n\n\n\n\\(\\leadsto\\) Treatment effect \\(\\approx\\) -7 ü•≥"
  },
  {
    "objectID": "w13/slides.html#references",
    "href": "w13/slides.html#references",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "References",
    "text": "References\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press."
  },
  {
    "objectID": "w13/index.html",
    "href": "w13/index.html",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#roadmap",
    "href": "w13/index.html#roadmap",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Roadmap",
    "text": "Roadmap\n\n\n\n\n\n\n What makes causation different from correlation?\n\n\n\n\nWhy can‚Äôt we use, e.g., Regression to infer causal effects? \\(\\uparrow X\\) by 1 unit causes \\(\\uparrow Y\\) by \\(\\beta\\) units?\n\\(\\leadsto\\) Fundamental Problem of Causal Inference\n\n\n\n\n\n\n\n\n\n Key to resolving Fundamental Problem: Match similar observations\n\n\n\n\nApples to apples: If \\(j\\) receives drug while \\(i\\) doesn‚Äôt, and they‚Äôre \\(s_{ij}\\%\\) similar otherwise (age, height)‚Ä¶\nHigher \\(s_{ij}\\) \\(\\implies\\) more confidence in attributing difference in outcomes \\(\\boxed{\\Delta y = y_j - y_i}\\) to drug!\n\\(\\leadsto\\) Propensity Score Matching (\\(\\approx\\) Logistic Regression)\n\n\n\n\n\n\n\n\n\n How can ML help us infer counterfactual effects?\n\n\n\n\nPatient \\(i\\) didn‚Äôt receive treatment, reported VAS pain level \\(y^0_i = 80\\)‚Ä¶\nIf \\(i\\) had received treatment, what would their pain level \\(y_i^1\\) be?\n\\(\\leadsto\\) Causal Forests, to estimate \\(\\boxed{\\Delta y_i = y^1_i - y^0_i}\\)",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#the-fundamental-problem-of-causal-inference-1",
    "href": "w13/index.html#the-fundamental-problem-of-causal-inference-1",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ‚Äú\\(X\\) causes \\(Y\\)‚Äù:\n\n\n\n\n\n\n Defining Causality (Hume 1739)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where everything is exactly the same‚Ä¶\n‚Ä¶except that \\(\\boxed{X = 0 \\text{  in  } W_0}\\) and \\(\\boxed{X = 1 \\text{  in  } W_1}\\),\n\\(\\boxed{Y = 0 \\text{  in  } W_0}\\) and \\(\\boxed{Y = 1 \\text{  in  } W_1}\\).\n\n\n\n\n\nThe problem? We live in one world, not two simultaneous worlds üò≠",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#cant-we-just-use-temporal-precedence",
    "href": "w13/index.html#cant-we-just-use-temporal-precedence",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Can‚Äôt We Just Use Temporal Precedence?",
    "text": "Can‚Äôt We Just Use Temporal Precedence?\n\nCan‚Äôt we just pretend that \\(W_0\\) is our world at time \\(t\\) and \\(W_1\\) is our world at time \\(t + 1\\)?\nDid throwing the eraser at Sam at time \\(t\\) cause him to be upset at time \\(t + 1\\)?\nNo, because at time \\(t\\), simultaneous with my eraser-throwing, a cockroach scuttled across his foot, the true cause of him being upset at time \\(t + 1\\)\nWithout knowing that the worlds are identical except for the posited cause-event, we can‚Äôt exclude the possibility of some other cause-event",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#extreme-example-super-mario-64-speedrunning",
    "href": "w13/index.html#extreme-example-super-mario-64-speedrunning",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Extreme Example: Super Mario 64 Speedrunning",
    "text": "Extreme Example: Super Mario 64 Speedrunning\nSeemingly-reasonable assumption: Button-pushes cause outcomes in games‚Ä¶\n\n\n\nDuring the race, an ionizing particle from outer space collided with DOTA_Teabag‚Äôs N64, flipping the eighth bit of Mario‚Äôs first height byte. Specifically, it flipped the byte from 11000101 to 11000100, from ‚ÄúC5‚Äù to ‚ÄúC4‚Äù. This resulted in a height change from C5837800 to C4837800, which by complete chance, happened to be the exact amount needed to warp Mario up to the higher floor at that exact moment.\n\n\n\n\n\nArticle from TheGamer.com\n\n\n\n\n\nThis was tested by pannenkoek12 - the same person who put up the bounty - using a script that manually flipped that particular bit at the right time, confirming the suspicion of a bit flip.",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#what-about-a-b-testing",
    "href": "w13/index.html#what-about-a-b-testing",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "What About A-B Testing?",
    "text": "What About A-B Testing?\n\nGets us significantly closer, but methods for recovering causal effect require a condition called SUTVA\nStable Unit Treatment Value Assumption: Treatment applied to \\(i\\) does not affect outcome for another person \\(j\\)\nIf we A-B test an app redesign (A = old design, B = new design), and outcome = length of time spent on app‚Ä¶\nPerson \\(i\\) seeing design A may like the new design, causing them to spend more time on the app\nPerson \\(i\\) may then message person \\(j\\) ‚ÄúCheck out [app], they redesigned everything!‚Äù, causing \\(j\\) to spend more time on app regardless of treatment (network spillover ‚ùå)",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#what-is-to-be-done",
    "href": "w13/index.html#what-is-to-be-done",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#case-study-military-inequality-leadsto-military-success",
    "href": "w13/index.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#does-inequality-cause-poor-military-performance",
    "href": "w13/index.html#does-inequality-cause-poor-military-performance",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#glorified-logistic-regression",
    "href": "w13/index.html#glorified-logistic-regression",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "‚Ä¶Glorified Logistic Regression!",
    "text": "‚Ä¶Glorified Logistic Regression!\n\n\nSimilarity score via Logistic Regression! Let‚Äôs look at a program that built health clinics in several villages: did health clinics cause lower infant mortality?\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.3     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nvillage_df &lt;- tribble(\n  ~village_id, ~T, ~inf_mortality, \n  1, 1, 10,\n  2, 1, 15,\n  3, 1, 22,\n  4, 1, 19,\n  5, 0, 25,\n  6, 0, 19,\n  7, 0, 4,\n  8, 0, 8,\n  9, 0, 6\n) |&gt; mutate(T = factor(T))\nvillage_df\n\n\n\n\n\n\nvillage_id\nT\ninf_mortality\n\n\n\n\n1\n1\n10\n\n\n2\n1\n15\n\n\n3\n1\n22\n\n\n4\n1\n19\n\n\n5\n0\n25\n\n\n6\n0\n19\n\n\n7\n0\n4\n\n\n8\n0\n8\n\n\n9\n0\n6\n\n\n\n\n\n\n\n\n\nCode\nvillage_df |&gt; group_by(T) |&gt;\n  summarize(mean_mortality = mean(inf_mortality)) |&gt;\n  arrange(desc(T))\n\n\n\n\n\n\nT\nmean_mortality\n\n\n\n\n1\n16.5\n\n\n0\n12.4\n\n\n\n\n\n\nHealth clinics increased mortality by 4.1?",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#from-controlling-for-to-how-well-are-we-controlling-for",
    "href": "w13/index.html#from-controlling-for-to-how-well-are-we-controlling-for",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "From ‚ÄúControlling For‚Äù to ‚ÄúHow Well Are We Controlling For?‚Äù",
    "text": "From ‚ÄúControlling For‚Äù to ‚ÄúHow Well Are We Controlling For?‚Äù\n\nBy introducing covariates, we can see the selection bias at play‚Ä¶\n\n\n\nCode\ncovar_df &lt;- tribble(\n  ~poverty_rate, ~docs_per_capita,\n  0.5, 0.01,\n  0.6, 0.02,\n  0.7, 0.01,\n  0.6, 0.02,\n  0.6, 0.01,\n  0.5, 0.02,\n  0.1, 0.04,\n  0.3, 0.05,\n  0.2, 0.04,\n)\nvillage_df &lt;- bind_cols(village_df, covar_df)\nvillage_df\n\n\n\n\n\n\nvillage_id\nT\ninf_mortality\npoverty_rate\ndocs_per_capita\n\n\n\n\n1\n1\n10\n0.5\n0.01\n\n\n2\n1\n15\n0.6\n0.02\n\n\n3\n1\n22\n0.7\n0.01\n\n\n4\n1\n19\n0.6\n0.02\n\n\n5\n0\n25\n0.6\n0.01\n\n\n6\n0\n19\n0.5\n0.02\n\n\n7\n0\n4\n0.1\n0.04\n\n\n8\n0\n8\n0.3\n0.05\n\n\n9\n0\n6\n0.2\n0.04",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#selection-bias",
    "href": "w13/index.html#selection-bias",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Selection Bias",
    "text": "Selection Bias\n\n\n\n\nCode\nvillage_df |&gt; ggplot(aes(x = poverty_rate, fill=T)) +\n  geom_density(alpha=0.5) +\n  theme_dsan(base_size=30) +\n  labs(\n    title = \"Poverty Rate by Treatment\",\n    x = \"Poverty Rate\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nvillage_df |&gt; ggplot(aes(x = docs_per_capita, fill=T)) +\n  geom_density(alpha=0.5) +\n  theme_dsan(base_size=30) +\n  labs(\n    title = \"Doctors per Capita by Treatment\",\n    x = \"Doctors per Capita\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\leadsto\\) We‚Äôre not comparing apples to apples! (‚ÄúWell, we‚Äôre both villages‚Äù)",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#logistic-regression-of-treatment",
    "href": "w13/index.html#logistic-regression-of-treatment",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Logistic Regression of Treatment",
    "text": "Logistic Regression of Treatment\n\n\nCode\nprop_model &lt;- glm(\n  T ~ poverty_rate + docs_per_capita,\n  data=village_df, family=\"binomial\"\n)\nsummary(prop_model)\n\n\n\nCall:\nglm(formula = T ~ poverty_rate + docs_per_capita, family = \"binomial\", \n    data = village_df)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)       -7.498      8.992  -0.834    0.404\npoverty_rate      14.500     13.651   1.062    0.288\ndocs_per_capita   -8.880    143.595  -0.062    0.951\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 12.3653  on 8  degrees of freedom\nResidual deviance:  6.9987  on 6  degrees of freedom\nAIC: 12.999\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nWe now have a model of selection bias! \\(\\leadsto\\) match observations with similar \\(\\Pr(T)\\)",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#propensity-score-logistic-regression-estimate",
    "href": "w13/index.html#propensity-score-logistic-regression-estimate",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Propensity Score = Logistic Regression Estimate",
    "text": "Propensity Score = Logistic Regression Estimate\n\n\nCode\nvillage_df$ps &lt;- predict(prop_model, village_df, type=\"response\")\nvillage_df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvillage_id\nT\ninf_mortality\npoverty_rate\ndocs_per_capita\nps\n\n\n\n\n1\n1\n10\n0.5\n0.01\n0.4165712\n\n\n2\n1\n15\n0.6\n0.02\n0.7358171\n\n\n3\n1\n22\n0.7\n0.01\n0.9284516\n\n\n4\n1\n19\n0.6\n0.02\n0.7358171\n\n\n5\n0\n25\n0.6\n0.01\n0.7527140\n\n\n6\n0\n19\n0.5\n0.02\n0.3951619\n\n\n7\n0\n4\n0.1\n0.04\n0.0016534\n\n\n8\n0\n8\n0.3\n0.05\n0.0268029\n\n\n9\n0\n6\n0.2\n0.04\n0.0070107",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#propensity-score-matching-distance-metric",
    "href": "w13/index.html#propensity-score-matching-distance-metric",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Propensity Score Matching = Distance Metric!",
    "text": "Propensity Score Matching = Distance Metric!\n\n\n\nCode\ncur_T &lt;- village_df[1,\"T\"] |&gt; pull()\ncur_ps &lt;- village_df[1,\"ps\"] |&gt; pull()\nwriteLines(paste0(\"Current village: T = \",cur_T,\", ps = \",cur_ps))\n\n\nCurrent village: T = 1, ps = 0.416571242858422\n\n\nCode\nother_df &lt;- village_df |&gt; filter(T != cur_T) |&gt;\n  mutate(\n    ps_dist = abs(ps - cur_ps)\n  )\nother_df |&gt; select(-c(inf_mortality))\n\n\n\n\n\n\nvillage_id\nT\npoverty_rate\ndocs_per_capita\nps\nps_dist\n\n\n\n\n5\n0\n0.6\n0.01\n0.7527140\n0.3361428\n\n\n6\n0\n0.5\n0.02\n0.3951619\n0.0214093\n\n\n7\n0\n0.1\n0.04\n0.0016534\n0.4149179\n\n\n8\n0\n0.3\n0.05\n0.0268029\n0.3897683\n\n\n9\n0\n0.2\n0.04\n0.0070107\n0.4095605",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#now-in-a-for-loop",
    "href": "w13/index.html#now-in-a-for-loop",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "Now in a For Loop‚Ä¶",
    "text": "Now in a For Loop‚Ä¶\n\n\nCode\nfor (i in 1:9) {\n  cur_T &lt;- village_df[i,\"T\"] |&gt; pull()\n  cur_ps &lt;- village_df[i,\"ps\"] |&gt; pull()\n  # writeLines(paste0(\"Current village: T = \",cur_T,\", ps = \",cur_ps))\n  other_df &lt;- village_df |&gt; filter(T != cur_T) |&gt;\n    mutate(\n      ps_dist = abs(ps - cur_ps)\n    )\n  match_id &lt;- names(which.min(other_df$ps_dist))\n  village_df[i,\"match\"] &lt;- as.numeric(match_id)\n}\nvillage_df |&gt; select(-inf_mortality)\n\n\n\n\n\n\nvillage_id\nT\npoverty_rate\ndocs_per_capita\nps\nmatch\n\n\n\n\n1\n1\n0.5\n0.01\n0.4165712\n6\n\n\n2\n1\n0.6\n0.02\n0.7358171\n5\n\n\n3\n1\n0.7\n0.01\n0.9284516\n5\n\n\n4\n1\n0.6\n0.02\n0.7358171\n5\n\n\n5\n0\n0.6\n0.01\n0.7527140\n2\n\n\n6\n0\n0.5\n0.02\n0.3951619\n1\n\n\n7\n0\n0.1\n0.04\n0.0016534\n1\n\n\n8\n0\n0.3\n0.05\n0.0268029\n1\n\n\n9\n0\n0.2\n0.04\n0.0070107\n1",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#and-now-we-compare-apples-to-apples",
    "href": "w13/index.html#and-now-we-compare-apples-to-apples",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "And Now We Compare Apples to Apples‚Ä¶",
    "text": "And Now We Compare Apples to Apples‚Ä¶\n\n\nCode\ntreated_df &lt;- village_df |&gt; filter(T == 1)\n(matched_df &lt;- treated_df |&gt; left_join(village_df, join_by(match == village_id)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvillage_id\nT.x\ninf_mortality.x\npoverty_rate.x\ndocs_per_capita.x\nps.x\nmatch\nT.y\ninf_mortality.y\npoverty_rate.y\ndocs_per_capita.y\nps.y\nmatch.y\n\n\n\n\n1\n1\n10\n0.5\n0.01\n0.4165712\n6\n0\n19\n0.5\n0.02\n0.3951619\n1\n\n\n2\n1\n15\n0.6\n0.02\n0.7358171\n5\n0\n25\n0.6\n0.01\n0.7527140\n2\n\n\n3\n1\n22\n0.7\n0.01\n0.9284516\n5\n0\n25\n0.6\n0.01\n0.7527140\n2\n\n\n4\n1\n19\n0.6\n0.02\n0.7358171\n5\n0\n25\n0.6\n0.01\n0.7527140\n2\n\n\n\n\n\n\n\n\nCode\nmatched_df |&gt; summarize(\n  mean_tr = mean(inf_mortality.x),\n  mean_control = mean(inf_mortality.y)\n)\n\n\n\n\n\n\nmean_tr\nmean_control\n\n\n\n\n16.5\n23.5\n\n\n\n\n\n\n\n\\(\\leadsto\\) Treatment effect \\(\\approx\\) -7 ü•≥",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w13/index.html#references",
    "href": "w13/index.html#references",
    "title": "Week 13: Machine Learning for Causal Inference",
    "section": "References",
    "text": "References\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.",
    "crumbs": [
      "Week 13: {{< var w13.date-md >}}"
    ]
  },
  {
    "objectID": "w06/slides.html#reminder-w04-new-goal-generalizability",
    "href": "w06/slides.html#reminder-w04-new-goal-generalizability",
    "title": "Week 6: Regularization for Model Selection",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n Goal 2.0: Statistical Learning\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì"
  },
  {
    "objectID": "w06/slides.html#clarification-target-diagrams",
    "href": "w06/slides.html#clarification-target-diagrams",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Clarification: Target Diagrams",
    "text": "Clarification: Target Diagrams\n\n\n\n\n\n\n\n\n\n\n\n\nLow Variance\nHigh Variance\n\n\n\n\nLow Bias\n\n\n\n\nHigh Bias\n\n\n\n\n\n\n\nFigure¬†1: Adapted from Fortmann-Roe (2012), ‚ÄúUnderstanding the Bias-Variance Tradeoff‚Äù"
  },
  {
    "objectID": "w06/slides.html#why-was-this-helpful-for-5100",
    "href": "w06/slides.html#why-was-this-helpful-for-5100",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Why Was This Helpful for 5100?",
    "text": "Why Was This Helpful for 5100?\n\nLaw of Large Numbers:\n\nAvg(many sample means \\(s\\)) \\(\\leadsto\\) true mean \\(\\mu\\)\n\n\\(\\widehat{\\theta}\\) unbiased estimator for \\(\\theta\\):\n\nAvg(Estimates \\(\\widehat{\\theta}\\)) \\(\\leadsto\\) true \\(\\theta\\)\n\n\n\nThe Low Bias, High Variance case"
  },
  {
    "objectID": "w06/slides.html#relevance-for-cv-error",
    "href": "w06/slides.html#relevance-for-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Relevance for CV Error",
    "text": "Relevance for CV Error\n\nIn Goal 2.0 world, we choose models on the basis of estimated test error (before, with Goal 1.0, we only used e.g.¬†MSE, RSS, \\(R^2\\), which was fine for linear regression)\nData \\(\\mathbf{D}\\) = single realization of DGP (for 5300, only relevance is why we don‚Äôt look at test set)\n\\(\\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right]\\) = random permutation of \\(\\mathbf{D}\\)\nBullseye on target = true test error(We could compute this, but then we‚Äôd have to end the study, collect more data‚Ä¶ better alternative on next slide!)\nDarts thrown around bullseye = estimated test errors (CV fold errors!)\n\nThey don‚Äôt hit bullseye because we‚Äôre inferring DGP from from sample\n\nTrue test error = \\(f(\\mathbf{D}) = f\\left( \\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right] \\right)\\)\nValidation error = \\(f(\\mathbf{D}_{\\text{Train}}) = f\\left( \\left[ \\mathbf{D}_{\\text{SubTr}} \\middle| \\mathbf{D}_{\\text{Val}} \\right] \\right)\\),\n\n\\(\\implies\\) Validation error is an estimate, using a smaller sample \\(\\mathbf{D}_{\\text{Train}}\\) drawn from the same distribution (DGP) as true test error!"
  },
  {
    "objectID": "w06/slides.html#true-test-error-vs.-cv-error",
    "href": "w06/slides.html#true-test-error-vs.-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "True Test Error vs.¬†CV Error",
    "text": "True Test Error vs.¬†CV Error\nNote the icons! Test set = Lake monster: pulling out of water to evaluate kills it üòµ\n\n\n\n\n\n\n True Test Error \\(\\varepsilon_{\\text{Test}} = \\text{Err}_{\\text{Test}}\\)\n\n\n Data \\(\\mathbf{D}\\) ‚Äúarises‚Äù out of (unobservable) DGP\n Randomly chop \\(\\mathbf{D}\\) into \\(\\left[ \\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}} \\right]\\)\n \\(\\underbrace{\\text{Err}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{no cap}}} = f(\\mathbf{D}_{\\text{Train}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Test}}}_{\\text{This kills monster üò¢}})\\)\nIssue: can only be evaluated once, ever üò±\n\n\n\n\n\n\n\n\n\n Validation Set Error \\(\\varepsilon_{\\text{Val}} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{Err}}_{\\text{Test}}\\)\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\)\n Leave \\(\\mathbf{D}_{\\text{Test}}\\) alone until end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\([\\mathbf{D}_{\\text{SubTr}} \\mid \\mathbf{D}_{\\text{Val}}]\\)\n \\(\\underbrace{\\widehat{\\text{Err}}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{capping a bit}}} = f(\\mathbf{D}_{\\text{SubTr}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Val}}}_{\\text{Monster still alive!}})\\)\n\n\n\n\n\n\n\n\n\n \\(K\\)-Fold Cross-Validation Error \\(\\varepsilon_{(K)} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{E}}\\text{rr}_{\\text{Test}}\\)\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\);  Leave \\(\\mathbf{D}_{\\text{Test}}\\) for end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\(\\left[ \\mathbf{D}_{\\text{TrFold}}^{(1)} \\middle| \\mathbf{D}_{\\text{TrFold}}^{(2)} \\middle| \\cdots \\middle| \\mathbf{D}_{\\text{TrFold}}^{(K)} \\right]\\)\n For \\(i \\in \\{1, \\ldots, K\\}\\):\n¬†¬†¬†¬† \\(\\varepsilon_{\\text{ValFold}}^{(i)} = f\\left( \\mathbf{D}_{\\text{TrFold}}^{(-i)} \\overset{\\text{fit}}{\\longrightarrow} \\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{TrFold}}^{(i)} \\right)\\)\n \\(\\underbrace{\\widehat{\\text{E}}\\text{rr}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{less cap!}}} = \\boxed{\\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{(i)}_{\\text{ValFold}}}~\\) (‚Ä¶monster still alive, even after all that!)"
  },
  {
    "objectID": "w06/slides.html#general-issue-with-cv-its-halfway-there",
    "href": "w06/slides.html#general-issue-with-cv-its-halfway-there",
    "title": "Week 6: Regularization for Model Selection",
    "section": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There",
    "text": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There\nCV plots will often look like (complexity on \\(x\\)-axis and CV error on \\(y\\)-axis):\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\ncpl_label &lt;- TeX(\"$M_0$\")\nsim1k_delta_df &lt;- tibble(\n    complexity=1:7,\n    cv_err=c(8, 2, 1, 1, 1, 1, 2),\n    label=c(\"\",\"\",TeX(\"$M_3$\"),\"\",\"\",TeX(\"$M_6$\"),\"\")\n)\nsim1k_delta_df |&gt; ggplot(aes(x=complexity, y=cv_err, label=label)) +\n  geom_line(linewidth=1) +\n  geom_point(size=(2/3)*g_pointsize) +\n  geom_text(vjust=-0.7, size=10, parse=TRUE) +\n  scale_x_continuous(\n    breaks=seq(from=1,to=7,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(\n    title=\"Generic CV Error Plot\",\n    x = \"Complexity\",\n    y = \"CV Error\"\n  )\n\n\n\n\nWe ‚Äúknow‚Äù \\(\\mathcal{M}_3\\) preferable to \\(\\mathcal{M}_6\\) (same error yet, less overfitting) \\(\\implies\\) ‚Äú1SE rule‚Äù\nBut‚Ä¶ heuristic \\(\\;\\nimplies\\) optimal! What are we gaining/losing as we move \\(\\mathcal{M}_6 \\rightarrow \\mathcal{M}_3\\)?\nEnter REGULARIZATION!"
  },
  {
    "objectID": "w06/slides.html#cv-now-goes-into-your-toolbox",
    "href": "w06/slides.html#cv-now-goes-into-your-toolbox",
    "title": "Week 6: Regularization for Model Selection",
    "section": "CV Now Goes Into Your Toolbox",
    "text": "CV Now Goes Into Your Toolbox\n\n(We will take it back out later, I promise!)"
  },
  {
    "objectID": "w06/slides.html#optimal-but-infeasible-best-subset-selection",
    "href": "w06/slides.html#optimal-but-infeasible-best-subset-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Optimal but Infeasible: Best Subset Selection",
    "text": "Optimal but Infeasible: Best Subset Selection\n\n\n\n\nAlgorithm: Best Subset Selection\n\n\n Let \\(\\mathcal{M}^*_0\\) be null model: Predicts \\(\\widehat{y}(x_i) = \\overline{y}\\) for any \\(x_i\\)\n For \\(k = 1, 2, \\ldots, J\\):\n¬†¬†¬†¬† Fit all \\(\\binom{J}{k}\\) possible models with \\(k\\) predictors, \\(\\mathcal{M}^*_k\\) is model with lowest RSS\n Choose from \\(\\mathcal{M}^*_0, \\ldots, \\mathcal{M}^*_J\\) using CV or heuristics: AIC, BIC, adjusted \\(R^2\\)"
  },
  {
    "objectID": "w06/slides.html#feasible-but-suboptimal-stepwise-selection",
    "href": "w06/slides.html#feasible-but-suboptimal-stepwise-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Feasible but Suboptimal: Stepwise Selection",
    "text": "Feasible but Suboptimal: Stepwise Selection\n\n\n\n\nAlgorithm: Forward Stepwise Selection\n\n\n Let \\(\\mathcal{M}_0\\) be null model: Predicts \\(\\widehat{y}(x_i) = \\overline{y}\\) for any \\(x_i\\)\n For \\(k = 0, 1, \\ldots, J - 1\\):\n¬†¬†¬†¬† Fit \\(J - k\\) models, each adds single feature to \\(\\mathcal{M}_k\\); call ‚Äúbest‚Äù model \\(\\mathcal{M}_{k+1}\\)\n Choose from \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_J\\) using CV error (or heuristics: AIC, BIC, adjusted \\(R^2\\))\n\n\n\n\n\n\n\n\nAlgorithm: Backward Stepwise Selection\n\n\n Let \\(\\mathcal{M}_J\\) be full model: Contains all features\n For \\(k = J, J - 1, \\ldots, 1\\):\n¬†¬†¬†¬† Fit \\(k\\) models, each removes single feature from \\(\\mathcal{M}_k\\); call ‚Äúbest‚Äù model \\(\\mathcal{M}_{k-1}\\)\n Choose from \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_J\\) using CV error (or heuristics: AIC, BIC, adjusted \\(R^2\\))"
  },
  {
    "objectID": "w06/slides.html#stepwise-selection-algorithms-are-greedy",
    "href": "w06/slides.html#stepwise-selection-algorithms-are-greedy",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Stepwise Selection Algorithms are Greedy",
    "text": "Stepwise Selection Algorithms are Greedy\n\n\n\n\nLike a mouse who chases closest cheese \\(\\neq\\) path with most cheese\nCan get ‚Äútrapped‚Äù in sub-optimal model, if (e.g.) feature is in \\(\\mathcal{M}^*_4\\) but not in \\(\\mathcal{M}^*_1, \\mathcal{M}^*_2, \\mathcal{M}^*_3\\)!\n\n\n\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(\\mathcal{M}^*_k\\) (Best Subset)\n\\(\\mathcal{M}_k\\) (Forward Stepwise) \n\n\n\n\n1\nrating\nrating\n\n\n2\nrating, income\nrating, income\n\n\n3\nrating, income, student\nrating, income, student\n\n\n4\ncards, income, student, limit\nrating, income, student, limit"
  },
  {
    "objectID": "w06/slides.html#key-building-block-lp-norms",
    "href": "w06/slides.html#key-building-block-lp-norms",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Key Building Block: \\(L^p\\) Norms",
    "text": "Key Building Block: \\(L^p\\) Norms\n\nTechnically you have seen these before: distance metrics!\n\n\\[\n\\textsf{sim}(\\mathbf{u}, \\mathbf{v}) \\triangleq \\underbrace{\\| \\mathbf{v} - \\mathbf{u} \\|_p}_{L^p\\text{ norm of }\\mathbf{u} - \\mathbf{v}} = \\left( \\sum_{i=1}^{n} |v_i - u_i|^p \\right)^{1/p}\n\\]\n\n\\(\\implies\\) Euclidean distance is \\(L^2\\) norm: if \\(\\mathbf{u} = (0,0)\\) and \\(\\mathbf{v} = (3,4)\\),\n\n\\[\n\\| \\mathbf{v} - \\mathbf{u} \\|_2 = \\left( |3-0|^2 + |4-0|^2 \\right)^{1/2} = 25^{1/2} = \\sqrt{25} = 5\n\\]\n\nWe‚Äôll use even simpler form: distance of coefficients \\(\\boldsymbol\\beta\\) from zero (so, \\(\\mathbf{u} = \\vec{\\mathbf{0}}\\))\n\n\\[\n\\| \\boldsymbol\\beta \\|_p = \\left( \\sum_{j=1}^{J} |\\beta_j|^p \\right)^{1/p}\n\\]"
  },
  {
    "objectID": "w06/slides.html#l1-and-l2-norms",
    "href": "w06/slides.html#l1-and-l2-norms",
    "title": "Week 6: Regularization for Model Selection",
    "section": "\\(L^1\\) and \\(L^2\\) Norms",
    "text": "\\(L^1\\) and \\(L^2\\) Norms\n\n\\(L^1\\) norm has a nice closed form expression as a sum (efficient, vectorizable!):\n\n\\[\n\\| \\boldsymbol\\beta \\|_1 = \\left( \\sum_{j=1}^{J}|\\beta_j|^1 \\right)^{1/1} = \\boxed{\\sum_{j=1}^{J}|\\beta_j|}\n\\]\n\n\\(L^2\\) norm almost a similarly nice expression, besides this zigzag line thing (\\(\\sqrt{~}\\))\n\n\\[\n\\| \\boldsymbol\\beta \\|_2 = \\left( \\sum_{j=1}^{J}|\\beta_j|^2 \\right)^{1/2} = \\sqrt{\\sum_{j=1}^{J}\\beta_j^2} \\; \\; ü§®\n\\]\n\nCan always convert bound on true Euclidean distance like \\(\\| \\boldsymbol\\beta \\|_2 \\leq 10\\) into bound on squared Euclidean distance like \\(\\| \\boldsymbol\\beta \\|_2^2 \\leq 100\\). Hence we‚Äôll use squared \\(L^p\\) norm:\n\n\\[\n\\| \\boldsymbol\\beta \\|_2^2 = \\left( \\left( \\sum_{j=1}^{J}|\\beta_j|^p \\right)^{1/2} \\right)^{2} = \\boxed{\\sum_{j=1}^{J}\\beta_j^2} \\; \\; üíÜ\n\\]"
  },
  {
    "objectID": "w06/slides.html#different-norms-leftrightarrow-different-distances-from-vecmathbf0",
    "href": "w06/slides.html#different-norms-leftrightarrow-different-distances-from-vecmathbf0",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Different Norms \\(\\leftrightarrow\\) Different Distances from \\(\\vec{\\mathbf{0}}\\)",
    "text": "Different Norms \\(\\leftrightarrow\\) Different Distances from \\(\\vec{\\mathbf{0}}\\)\n\n\n\nUnit Disk in \\(L^2\\): All points \\(\\mathbf{v} = (v_x,v_y) \\in \\mathbb{R}^2\\) such that\n\n\\[\n\\| \\mathbf{v} \\|_2 \\leq 1 \\iff \\| \\mathbf{v} \\|_2^2 \\leq 1\n\\]\n\n\n\n\n\n\n\nUnit Disk in \\(L^1\\): All points \\(\\mathbf{v} = (v_x, v_y) \\in \\mathbf{R}^2\\) such that\n\n\\[\n\\| \\mathbf{v} \\|_1 \\leq 1\n\\]"
  },
  {
    "objectID": "w06/slides.html#regularized-regression-finally",
    "href": "w06/slides.html#regularized-regression-finally",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Regularized Regression (Finally!)",
    "text": "Regularized Regression (Finally!)\nGeneral Form:\n\\[\n\\boldsymbol\\beta^*_{\\text{reg}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\overbrace{\\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2}^{\\text{MSE from before}} \\; \\; + \\; \\; \\overbrace{\\lambda}^{\\mathclap{\\text{Penalty for}}} \\underbrace{\\| \\boldsymbol\\beta \\|_{2}^{2}}_{\\mathclap{\\text{Dist from }\\mathbf{0}}} \\; \\; \\; \\right]\n\\]"
  },
  {
    "objectID": "w06/slides.html#three-main-types-of-regularized-regression",
    "href": "w06/slides.html#three-main-types-of-regularized-regression",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Three Main Types of Regularized Regression",
    "text": "Three Main Types of Regularized Regression\nRidge Regression:\n\\[\n\\boldsymbol\\beta^*_{\\text{ridge}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{2}^{2} \\right]\n\\]\nLASSO:\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]\nElastic Net:\n\\[\n\\boldsymbol\\beta^*_{\\text{EN}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda_2 \\| \\boldsymbol\\beta \\|_{2}^{2} + \\lambda_1 \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]\n(Does anyone recognize \\(\\lambda\\) from Lagrange multipliers?)"
  },
  {
    "objectID": "w06/slides.html#top-secret-equivalent-forms",
    "href": "w06/slides.html#top-secret-equivalent-forms",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Top Secret Equivalent Forms",
    "text": "Top Secret Equivalent Forms\nRidge Regression:\n\\[\n\\boldsymbol\\beta^*_{\\text{ridge}} = \\arg \\left\\{\\min_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right] \\; \\text{ subject to } \\; \\| \\boldsymbol\\beta \\|_{2}^{2} \\leq s \\right\\}\n\\]\nLASSO:\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\arg \\left\\{\\min_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right] \\; \\text{ subject to } \\; \\| \\boldsymbol\\beta \\|_{1} \\leq s \\right\\}\n\\]\nElastic Net:\n\\[\n\\boldsymbol\\beta^*_{\\text{EN}} = \\arg \\left\\{\\min_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right] \\; \\text{ subject to } \\; \\begin{matrix}\n  \\| \\boldsymbol\\beta \\|_{2}^{2} \\leq s_2 \\\\\n  \\| \\boldsymbol\\beta \\|_{1} \\leq s_1\n  \\end{matrix}\n\\right\\}\n\\]"
  },
  {
    "objectID": "w06/slides.html#the-key-plot",
    "href": "w06/slides.html#the-key-plot",
    "title": "Week 6: Regularization for Model Selection",
    "section": "The Key Plot",
    "text": "The Key Plot\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(ggforce) |&gt; suppressPackageStartupMessages()\nlibrary(patchwork) |&gt; suppressPackageStartupMessages()\n# Bounding the space\nxbound &lt;- c(-1, 1)\nybound &lt;- c(0, 1.65)\nstepsize &lt;- 0.05\ndx &lt;- 0.605\ndy &lt;- 1.6\n# The actual function we're plotting contours for\nb_inter &lt;- 1.5\nmy_f &lt;- function(x,y) 8^(b_inter*(x-dx)*(y-dy) - (x-dx)^2 - (y-dy)^2)\nx_vals &lt;- seq(from=xbound[1], to=xbound[2], by=stepsize)\ny_vals &lt;- seq(from=ybound[1], to=ybound[2], by=stepsize)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\n# Optimal beta df\nbeta_opt_df &lt;- tibble(\n  x=121/200, y=8/5, label=c(TeX(\"$\\\\beta^*_{OLS}$\"))\n)\n# Ridge optimal beta\nridge_opt_df &lt;- tibble(\n  x=0.111, y=0.998, label=c(TeX(\"$\\\\beta^*_{ridge}$\"))\n)\n# Lasso diamond\nlasso_df &lt;- tibble(x=c(1,0,-1,0,1), y=c(0,1,0,-1,0), z=c(1,1,1,1,1))\nlasso_opt_df &lt;- tibble(x=0, y=1, label=c(TeX(\"$\\\\beta^*_{lasso}$\")))\n\n# And plot\nbase_plot &lt;- ggplot() +\n  geom_contour_filled(\n    data=data_df, aes(x=x, y=y, z=z),\n    alpha=0.8, binwidth = 0.04, color='black', linewidth=0.65\n  ) +\n  # y-axis\n  geom_segment(aes(x=0, xend=0, y=-Inf, yend=Inf), color='white', linewidth=0.5, linetype=\"solid\") +\n  # Unconstrained optimal beta\n  geom_point(data=beta_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=beta_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.45, vjust=0.65, parse=TRUE, alpha=0.9\n  ) +\n  scale_fill_viridis_d(option=\"C\") +\n  #coord_equal() +\n  labs(\n    #title = \"Model Selection: Ridge vs. Lasso Constraints\",\n    x = TeX(\"$\\\\beta_1$\"),\n    y = TeX(\"$\\\\beta_2$\")\n  )\nridge_plot &lt;- base_plot +\n  geom_circle(\n    aes(x0=0, y0=0, r=1, alpha=I(0.1), linetype=\"circ\", color='circ'), fill=NA, linewidth=0.5\n  )\n  # geom_point(\n  #   data=data.frame(x=0, y=0), aes(x=x, y=y),\n  #   shape=21, size=135.8, color='white', stroke=1.2, linestyle=\"dashed\"\n  # )\nlasso_plot &lt;- ridge_plot +\n  geom_polygon(\n    data=lasso_df, aes(x=x, y=y, linetype=\"diamond\", color=\"diamond\"),\n    fill='white',\n    alpha=0.5,\n    linewidth=1\n  ) +\n  # Ridge beta\n  geom_point(data=ridge_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=ridge_opt_df, aes(x=x, y=y, label=label),\n    hjust=2, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  # Lasso beta\n  geom_point(data=lasso_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=lasso_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.75, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  ylim(ybound[1], ybound[2]) +\n  # xlim(xbound[1], xbound[2]) +\n  scale_linetype_manual(\"Line\", values=c(\"diamond\"=\"solid\", \"circ\"=\"dashed\"), labels=c(\"a\",\"b\")) +\n  scale_color_manual(\"Color\", values=c(\"diamond\"=\"white\", \"circ\"=\"white\"), labels=c(\"c\",\"d\")) +\n  # scale_fill_manual(\"Test\") +\n  # x-axis\n  geom_segment(aes(x=-Inf, xend=Inf, y=0, yend=0), color='white') +\n  theme_dsan(base_size=16) +\n  coord_fixed() +\n  theme(\n    legend.position = \"none\",\n    axis.line = element_blank(),\n    axis.ticks = element_blank()\n  )\nlasso_plot"
  },
  {
    "objectID": "w06/slides.html#bayesian-interpretation",
    "href": "w06/slides.html#bayesian-interpretation",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Bayesian Interpretation",
    "text": "Bayesian Interpretation\n\n\n\nBelief \\(A\\): Most/all of the features you included have important effect on \\(Y\\)\n\\(A \\implies\\) Gaussian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), pdf of \\(X\\) is\n\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[ -\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma}\\right)^2 \\right]\n\\]\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nprior_labs &lt;- labs(\n  x = TeX(\"$\\\\beta_j$\"),\n  y = TeX(\"$f(\\\\beta_j)$\")\n)\nggplot() +\n  stat_function(fun=dnorm, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nGaussian prior \\(\\leadsto\\) Ridge Regression!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(\\sigma^2\\))\n\n\n\nBelief \\(B\\): Only a few of the features you included have important effect on \\(Y\\)\n\\(B \\implies\\) Laplacian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{L}(\\mu, b)\\), pdf of \\(X\\) is\n\n\\[\nf(x) = \\frac{1}{2b}\\exp\\left[ -\\left| \\frac{x - \\mu}{b} \\right| \\right]\n\\]\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(extraDistr) |&gt; suppressPackageStartupMessages()\nggplot() +\n  stat_function(fun=dlaplace, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nLaplacian prior \\(\\leadsto\\) Lasso!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(b\\))"
  },
  {
    "objectID": "w06/slides.html#ok-so-how-do-we-find-these-magical-lambda-values",
    "href": "w06/slides.html#ok-so-how-do-we-find-these-magical-lambda-values",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Ok So‚Ä¶ How Do We Find These Magical \\(\\lambda\\) Values?",
    "text": "Ok So‚Ä¶ How Do We Find These Magical \\(\\lambda\\) Values?\n\nCross-Validation!\n(‚Ä¶that‚Äôs, uh, that‚Äôs it! That‚Äôs the whole slide!)\n(But let‚Äôs look at what we find when we use CV‚Ä¶)"
  },
  {
    "objectID": "w06/slides.html#varying-lambda-leadsto-tradeoff-curve",
    "href": "w06/slides.html#varying-lambda-leadsto-tradeoff-curve",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Varying \\(\\lambda\\) \\(\\leadsto\\) Tradeoff Curve!",
    "text": "Varying \\(\\lambda\\) \\(\\leadsto\\) Tradeoff Curve!\n\nRidge Regression Case:\n\n\n\n\n\nIncreasing \\(\\lambda\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[\\text{RSS} + \\lambda \\| \\boldsymbol\\beta \\|_2^2 \\right]\\)\n\n\n\n\\(\\equiv\\)\n\n\n\nDecreasing \\(s\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[ \\text{RSS} \\right] \\text{ s.t. }\\|\\boldsymbol\\beta\\|_2^2 \\leq s\\)"
  },
  {
    "objectID": "w06/slides.html#only-change-l1-instead-of-l2-norm",
    "href": "w06/slides.html#only-change-l1-instead-of-l2-norm",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Only Change: \\(L^1\\) instead of \\(L^2\\) Norm ü§Ø",
    "text": "Only Change: \\(L^1\\) instead of \\(L^2\\) Norm ü§Ø\n\nLasso Case:\n\n\n\n\n\nIncreasing \\(\\lambda\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[\\text{RSS} + \\lambda \\| \\boldsymbol\\beta \\|_1\\right]\\)\n\n\n\n\\(\\equiv\\)\n\n\n\nDecreasing \\(s\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[ \\text{RSS} \\right] \\text{ s.t. }\\|\\boldsymbol\\beta\\|_1 \\leq s\\)"
  },
  {
    "objectID": "w06/slides.html#week-7-preview-linear-functions",
    "href": "w06/slides.html#week-7-preview-linear-functions",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Week 7 Preview: Linear Functions",
    "text": "Week 7 Preview: Linear Functions\n\n\nYou‚Äôve already seen polynomial regression:\n\n\\[\nY = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\cdots + \\beta_d X^d\n\\]\n\nPlus (technically) ‚ÄúFourier regression‚Äù:\n\n\\[\n\\begin{align*}\nY = \\beta_0 + \\beta_1 \\cos(\\pi X) + \\beta_2 \\sin(\\pi X) + \\cdots + \\beta_{2d-1}\\cos(\\pi d X) + \\beta_{2d}\\sin(\\pi d X)\n\\end{align*}\n\\]\n\nImage source"
  },
  {
    "objectID": "w06/slides.html#new-regression-just-dropped",
    "href": "w06/slides.html#new-regression-just-dropped",
    "title": "Week 6: Regularization for Model Selection",
    "section": "New Regression Just Dropped",
    "text": "New Regression Just Dropped\nPiecewise regression:\n Choose \\(K\\) cutpoints \\(c_1, \\ldots, c_K\\)\n Let \\(C_k(X) = \\mathbb{1}[c_{k-1} \\leq X &lt; c_k]\\), (\\(c_0 \\equiv -\\infty\\), \\(c_{K+1} \\equiv \\infty\\))\n\\[\nY = \\beta_0 + \\beta_1C_1(X) + \\beta_2C_2(X) + \\cdots + \\beta_KC_K(X)\n\\]"
  },
  {
    "objectID": "w06/slides.html#decomposing-fancy-regressions-into-core-pieces",
    "href": "w06/slides.html#decomposing-fancy-regressions-into-core-pieces",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Decomposing Fancy Regressions into Core ‚ÄúPieces‚Äù",
    "text": "Decomposing Fancy Regressions into Core ‚ÄúPieces‚Äù\n\nQ: What do all these types of regression have in common?\nA: They can all be written in the form\n\\[\nY = \\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\cdots + \\beta_d b_d(X)\n\\]\nWhere \\(b(\\cdot)\\) is called a basis function\n\nLinear (\\(d = 1\\)): \\(b_1(X) = X\\)\nPolynomial: \\(b_j(X) = X^j\\)\nPiecewise: \\(b_j(X) = \\mathbb{1}[c_{j-1} \\leq X &lt; c_j]\\)"
  },
  {
    "objectID": "w06/index.html",
    "href": "w06/index.html",
    "title": "Week 6: Regularization for Model Selection",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#reminder-w04-new-goal-generalizability",
    "href": "w06/index.html#reminder-w04-new-goal-generalizability",
    "title": "Week 6: Regularization for Model Selection",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n\n\n Goal 2.0: Statistical Learning\n\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#clarification-target-diagrams",
    "href": "w06/index.html#clarification-target-diagrams",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Clarification: Target Diagrams",
    "text": "Clarification: Target Diagrams\n\n\n\n\n\n\n\n\n\n\n\n\nLow Variance\nHigh Variance\n\n\n\n\nLow Bias\n\n\n\n\nHigh Bias\n\n\n\n\n\n\n\nFigure¬†1: Adapted from Fortmann-Roe (2012), ‚ÄúUnderstanding the Bias-Variance Tradeoff‚Äù",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#why-was-this-helpful-for-5100",
    "href": "w06/index.html#why-was-this-helpful-for-5100",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Why Was This Helpful for 5100?",
    "text": "Why Was This Helpful for 5100?\n\nLaw of Large Numbers:\n\nAvg(many sample means \\(s\\)) \\(\\leadsto\\) true mean \\(\\mu\\)\n\n\\(\\widehat{\\theta}\\) unbiased estimator for \\(\\theta\\):\n\nAvg(Estimates \\(\\widehat{\\theta}\\)) \\(\\leadsto\\) true \\(\\theta\\)\n\n\n\n\n\nThe Low Bias, High Variance case",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#relevance-for-cv-error",
    "href": "w06/index.html#relevance-for-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Relevance for CV Error",
    "text": "Relevance for CV Error\n\nIn Goal 2.0 world, we choose models on the basis of estimated test error (before, with Goal 1.0, we only used e.g.¬†MSE, RSS, \\(R^2\\), which was fine for linear regression)\nData \\(\\mathbf{D}\\) = single realization of DGP (for 5300, only relevance is why we don‚Äôt look at test set)\n\\(\\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right]\\) = random permutation of \\(\\mathbf{D}\\)\nBullseye on target = true test error(We could compute this, but then we‚Äôd have to end the study, collect more data‚Ä¶ better alternative on next slide!)\nDarts thrown around bullseye = estimated test errors (CV fold errors!)\n\nThey don‚Äôt hit bullseye because we‚Äôre inferring DGP from from sample\n\nTrue test error = \\(f(\\mathbf{D}) = f\\left( \\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right] \\right)\\)\nValidation error = \\(f(\\mathbf{D}_{\\text{Train}}) = f\\left( \\left[ \\mathbf{D}_{\\text{SubTr}} \\middle| \\mathbf{D}_{\\text{Val}} \\right] \\right)\\),\n\n\\(\\implies\\) Validation error is an estimate, using a smaller sample \\(\\mathbf{D}_{\\text{Train}}\\) drawn from the same distribution (DGP) as true test error!",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#true-test-error-vs.-cv-error",
    "href": "w06/index.html#true-test-error-vs.-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "True Test Error vs.¬†CV Error",
    "text": "True Test Error vs.¬†CV Error\nNote the icons! Test set = Lake monster: pulling out of water to evaluate kills it üòµ\n\n\n\n\n\n\n\n\n True Test Error \\(\\varepsilon_{\\text{Test}} = \\text{Err}_{\\text{Test}}\\)\n\n\n\n Data \\(\\mathbf{D}\\) ‚Äúarises‚Äù out of (unobservable) DGP\n Randomly chop \\(\\mathbf{D}\\) into \\(\\left[ \\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}} \\right]\\)\n \\(\\underbrace{\\text{Err}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{no cap}}} = f(\\mathbf{D}_{\\text{Train}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Test}}}_{\\text{This kills monster üò¢}})\\)\nIssue: can only be evaluated once, ever üò±\n\n\n\n\n\n\n\n\n\n Validation Set Error \\(\\varepsilon_{\\text{Val}} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{Err}}_{\\text{Test}}\\)\n\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\)\n Leave \\(\\mathbf{D}_{\\text{Test}}\\) alone until end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\([\\mathbf{D}_{\\text{SubTr}} \\mid \\mathbf{D}_{\\text{Val}}]\\)\n \\(\\underbrace{\\widehat{\\text{Err}}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{capping a bit}}} = f(\\mathbf{D}_{\\text{SubTr}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Val}}}_{\\text{Monster still alive!}})\\)\n\n\n\n\n\n\n\n\n\n\n \\(K\\)-Fold Cross-Validation Error \\(\\varepsilon_{(K)} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{E}}\\text{rr}_{\\text{Test}}\\)\n\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\);  Leave \\(\\mathbf{D}_{\\text{Test}}\\) for end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\(\\left[ \\mathbf{D}_{\\text{TrFold}}^{(1)} \\middle| \\mathbf{D}_{\\text{TrFold}}^{(2)} \\middle| \\cdots \\middle| \\mathbf{D}_{\\text{TrFold}}^{(K)} \\right]\\)\n For \\(i \\in \\{1, \\ldots, K\\}\\):\n¬†¬†¬†¬† \\(\\varepsilon_{\\text{ValFold}}^{(i)} = f\\left( \\mathbf{D}_{\\text{TrFold}}^{(-i)} \\overset{\\text{fit}}{\\longrightarrow} \\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{TrFold}}^{(i)} \\right)\\)\n \\(\\underbrace{\\widehat{\\text{E}}\\text{rr}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{less cap!}}} = \\boxed{\\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{(i)}_{\\text{ValFold}}}~\\) (‚Ä¶monster still alive, even after all that!)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#general-issue-with-cv-its-halfway-there",
    "href": "w06/index.html#general-issue-with-cv-its-halfway-there",
    "title": "Week 6: Regularization for Model Selection",
    "section": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There",
    "text": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There\nCV plots will often look like (complexity on \\(x\\)-axis and CV error on \\(y\\)-axis):\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\ncpl_label &lt;- TeX(\"$M_0$\")\nsim1k_delta_df &lt;- tibble(\n    complexity=1:7,\n    cv_err=c(8, 2, 1, 1, 1, 1, 2),\n    label=c(\"\",\"\",TeX(\"$M_3$\"),\"\",\"\",TeX(\"$M_6$\"),\"\")\n)\nsim1k_delta_df |&gt; ggplot(aes(x=complexity, y=cv_err, label=label)) +\n  geom_line(linewidth=1) +\n  geom_point(size=(2/3)*g_pointsize) +\n  geom_text(vjust=-0.7, size=10, parse=TRUE) +\n  scale_x_continuous(\n    breaks=seq(from=1,to=7,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(\n    title=\"Generic CV Error Plot\",\n    x = \"Complexity\",\n    y = \"CV Error\"\n  )\n\n\n\n\n\n\n\n\n\n\nWe ‚Äúknow‚Äù \\(\\mathcal{M}_3\\) preferable to \\(\\mathcal{M}_6\\) (same error yet, less overfitting) \\(\\implies\\) ‚Äú1SE rule‚Äù\nBut‚Ä¶ heuristic \\(\\;\\nimplies\\) optimal! What are we gaining/losing as we move \\(\\mathcal{M}_6 \\rightarrow \\mathcal{M}_3\\)?\nEnter REGULARIZATION!",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#cv-now-goes-into-your-toolbox",
    "href": "w06/index.html#cv-now-goes-into-your-toolbox",
    "title": "Week 6: Regularization for Model Selection",
    "section": "CV Now Goes Into Your Toolbox",
    "text": "CV Now Goes Into Your Toolbox\n\n\n\n\n\n(We will take it back out later, I promise!)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#optimal-but-infeasible-best-subset-selection",
    "href": "w06/index.html#optimal-but-infeasible-best-subset-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Optimal but Infeasible: Best Subset Selection",
    "text": "Optimal but Infeasible: Best Subset Selection\n\n\n\n\n\n\nAlgorithm: Best Subset Selection\n\n\n\n Let \\(\\mathcal{M}^*_0\\) be null model: Predicts \\(\\widehat{y}(x_i) = \\overline{y}\\) for any \\(x_i\\)\n For \\(k = 1, 2, \\ldots, J\\):\n¬†¬†¬†¬† Fit all \\(\\binom{J}{k}\\) possible models with \\(k\\) predictors, \\(\\mathcal{M}^*_k\\) is model with lowest RSS\n Choose from \\(\\mathcal{M}^*_0, \\ldots, \\mathcal{M}^*_J\\) using CV or heuristics: AIC, BIC, adjusted \\(R^2\\)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#feasible-but-suboptimal-stepwise-selection",
    "href": "w06/index.html#feasible-but-suboptimal-stepwise-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Feasible but Suboptimal: Stepwise Selection",
    "text": "Feasible but Suboptimal: Stepwise Selection\n\n\n\n\n\n\nAlgorithm: Forward Stepwise Selection\n\n\n\n Let \\(\\mathcal{M}_0\\) be null model: Predicts \\(\\widehat{y}(x_i) = \\overline{y}\\) for any \\(x_i\\)\n For \\(k = 0, 1, \\ldots, J - 1\\):\n¬†¬†¬†¬† Fit \\(J - k\\) models, each adds single feature to \\(\\mathcal{M}_k\\); call ‚Äúbest‚Äù model \\(\\mathcal{M}_{k+1}\\)\n Choose from \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_J\\) using CV error (or heuristics: AIC, BIC, adjusted \\(R^2\\))\n\n\n\n\n\n\n\n\nAlgorithm: Backward Stepwise Selection\n\n\n\n Let \\(\\mathcal{M}_J\\) be full model: Contains all features\n For \\(k = J, J - 1, \\ldots, 1\\):\n¬†¬†¬†¬† Fit \\(k\\) models, each removes single feature from \\(\\mathcal{M}_k\\); call ‚Äúbest‚Äù model \\(\\mathcal{M}_{k-1}\\)\n Choose from \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_J\\) using CV error (or heuristics: AIC, BIC, adjusted \\(R^2\\))",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#stepwise-selection-algorithms-are-greedy",
    "href": "w06/index.html#stepwise-selection-algorithms-are-greedy",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Stepwise Selection Algorithms are Greedy",
    "text": "Stepwise Selection Algorithms are Greedy\n\n\n\n\nLike a mouse who chases closest cheese \\(\\neq\\) path with most cheese\nCan get ‚Äútrapped‚Äù in sub-optimal model, if (e.g.) feature is in \\(\\mathcal{M}^*_4\\) but not in \\(\\mathcal{M}^*_1, \\mathcal{M}^*_2, \\mathcal{M}^*_3\\)!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(\\mathcal{M}^*_k\\) (Best Subset)\n\\(\\mathcal{M}_k\\) (Forward Stepwise) \n\n\n\n\n1\nrating\nrating\n\n\n2\nrating, income\nrating, income\n\n\n3\nrating, income, student\nrating, income, student\n\n\n4\ncards, income, student, limit\nrating, income, student, limit",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#key-building-block-lp-norms",
    "href": "w06/index.html#key-building-block-lp-norms",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Key Building Block: \\(L^p\\) Norms",
    "text": "Key Building Block: \\(L^p\\) Norms\n\nTechnically you have seen these before: distance metrics!\n\n\\[\n\\textsf{sim}(\\mathbf{u}, \\mathbf{v}) \\triangleq \\underbrace{\\| \\mathbf{v} - \\mathbf{u} \\|_p}_{L^p\\text{ norm of }\\mathbf{u} - \\mathbf{v}} = \\left( \\sum_{i=1}^{n} |v_i - u_i|^p \\right)^{1/p}\n\\]\n\n\\(\\implies\\) Euclidean distance is \\(L^2\\) norm: if \\(\\mathbf{u} = (0,0)\\) and \\(\\mathbf{v} = (3,4)\\),\n\n\\[\n\\| \\mathbf{v} - \\mathbf{u} \\|_2 = \\left( |3-0|^2 + |4-0|^2 \\right)^{1/2} = 25^{1/2} = \\sqrt{25} = 5\n\\]\n\nWe‚Äôll use even simpler form: distance of coefficients \\(\\boldsymbol\\beta\\) from zero (so, \\(\\mathbf{u} = \\vec{\\mathbf{0}}\\))\n\n\\[\n\\| \\boldsymbol\\beta \\|_p = \\left( \\sum_{j=1}^{J} |\\beta_j|^p \\right)^{1/p}\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#l1-and-l2-norms",
    "href": "w06/index.html#l1-and-l2-norms",
    "title": "Week 6: Regularization for Model Selection",
    "section": "\\(L^1\\) and \\(L^2\\) Norms",
    "text": "\\(L^1\\) and \\(L^2\\) Norms\n\n\\(L^1\\) norm has a nice closed form expression as a sum (efficient, vectorizable!):\n\n\\[\n\\| \\boldsymbol\\beta \\|_1 = \\left( \\sum_{j=1}^{J}|\\beta_j|^1 \\right)^{1/1} = \\boxed{\\sum_{j=1}^{J}|\\beta_j|}\n\\]\n\n\\(L^2\\) norm almost a similarly nice expression, besides this zigzag line thing (\\(\\sqrt{~}\\))\n\n\\[\n\\| \\boldsymbol\\beta \\|_2 = \\left( \\sum_{j=1}^{J}|\\beta_j|^2 \\right)^{1/2} = \\sqrt{\\sum_{j=1}^{J}\\beta_j^2} \\; \\; ü§®\n\\]\n\nCan always convert bound on true Euclidean distance like \\(\\| \\boldsymbol\\beta \\|_2 \\leq 10\\) into bound on squared Euclidean distance like \\(\\| \\boldsymbol\\beta \\|_2^2 \\leq 100\\). Hence we‚Äôll use squared \\(L^p\\) norm:\n\n\\[\n\\| \\boldsymbol\\beta \\|_2^2 = \\left( \\left( \\sum_{j=1}^{J}|\\beta_j|^p \\right)^{1/2} \\right)^{2} = \\boxed{\\sum_{j=1}^{J}\\beta_j^2} \\; \\; üíÜ\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#different-norms-leftrightarrow-different-distances-from-vecmathbf0",
    "href": "w06/index.html#different-norms-leftrightarrow-different-distances-from-vecmathbf0",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Different Norms \\(\\leftrightarrow\\) Different Distances from \\(\\vec{\\mathbf{0}}\\)",
    "text": "Different Norms \\(\\leftrightarrow\\) Different Distances from \\(\\vec{\\mathbf{0}}\\)\n\n\n\nUnit Disk in \\(L^2\\): All points \\(\\mathbf{v} = (v_x,v_y) \\in \\mathbb{R}^2\\) such that\n\n\\[\n\\| \\mathbf{v} \\|_2 \\leq 1 \\iff \\| \\mathbf{v} \\|_2^2 \\leq 1\n\\]\n\n\n\n\n\n\n\nUnit Disk in \\(L^1\\): All points \\(\\mathbf{v} = (v_x, v_y) \\in \\mathbf{R}^2\\) such that\n\n\\[\n\\| \\mathbf{v} \\|_1 \\leq 1\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#regularized-regression-finally",
    "href": "w06/index.html#regularized-regression-finally",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Regularized Regression (Finally!)",
    "text": "Regularized Regression (Finally!)\nGeneral Form:\n\\[\n\\boldsymbol\\beta^*_{\\text{reg}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\overbrace{\\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2}^{\\text{MSE from before}} \\; \\; + \\; \\; \\overbrace{\\lambda}^{\\mathclap{\\text{Penalty for}}} \\underbrace{\\| \\boldsymbol\\beta \\|_{2}^{2}}_{\\mathclap{\\text{Dist from }\\mathbf{0}}} \\; \\; \\; \\right]\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#three-main-types-of-regularized-regression",
    "href": "w06/index.html#three-main-types-of-regularized-regression",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Three Main Types of Regularized Regression",
    "text": "Three Main Types of Regularized Regression\nRidge Regression:\n\\[\n\\boldsymbol\\beta^*_{\\text{ridge}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{2}^{2} \\right]\n\\]\nLASSO:\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]\nElastic Net:\n\\[\n\\boldsymbol\\beta^*_{\\text{EN}} = \\argmin_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda_2 \\| \\boldsymbol\\beta \\|_{2}^{2} + \\lambda_1 \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]\n(Does anyone recognize \\(\\lambda\\) from Lagrange multipliers?)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#top-secret-equivalent-forms",
    "href": "w06/index.html#top-secret-equivalent-forms",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Top Secret Equivalent Forms",
    "text": "Top Secret Equivalent Forms\nRidge Regression:\n\\[\n\\boldsymbol\\beta^*_{\\text{ridge}} = \\arg \\left\\{\\min_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right] \\; \\text{ subject to } \\; \\| \\boldsymbol\\beta \\|_{2}^{2} \\leq s \\right\\}\n\\]\nLASSO:\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\arg \\left\\{\\min_{\\boldsymbol\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right] \\; \\text{ subject to } \\; \\| \\boldsymbol\\beta \\|_{1} \\leq s \\right\\}\n\\]\nElastic Net:\n\\[\n\\boldsymbol\\beta^*_{\\text{EN}} = \\arg \\left\\{\\min_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right] \\; \\text{ subject to } \\; \\begin{matrix}\n  \\| \\boldsymbol\\beta \\|_{2}^{2} \\leq s_2 \\\\\n  \\| \\boldsymbol\\beta \\|_{1} \\leq s_1\n  \\end{matrix}\n\\right\\}\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#the-key-plot",
    "href": "w06/index.html#the-key-plot",
    "title": "Week 6: Regularization for Model Selection",
    "section": "The Key Plot",
    "text": "The Key Plot\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(ggforce) |&gt; suppressPackageStartupMessages()\nlibrary(patchwork) |&gt; suppressPackageStartupMessages()\n# Bounding the space\nxbound &lt;- c(-1, 1)\nybound &lt;- c(0, 1.65)\nstepsize &lt;- 0.05\ndx &lt;- 0.605\ndy &lt;- 1.6\n# The actual function we're plotting contours for\nb_inter &lt;- 1.5\nmy_f &lt;- function(x,y) 8^(b_inter*(x-dx)*(y-dy) - (x-dx)^2 - (y-dy)^2)\nx_vals &lt;- seq(from=xbound[1], to=xbound[2], by=stepsize)\ny_vals &lt;- seq(from=ybound[1], to=ybound[2], by=stepsize)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\n# Optimal beta df\nbeta_opt_df &lt;- tibble(\n  x=121/200, y=8/5, label=c(TeX(\"$\\\\beta^*_{OLS}$\"))\n)\n# Ridge optimal beta\nridge_opt_df &lt;- tibble(\n  x=0.111, y=0.998, label=c(TeX(\"$\\\\beta^*_{ridge}$\"))\n)\n# Lasso diamond\nlasso_df &lt;- tibble(x=c(1,0,-1,0,1), y=c(0,1,0,-1,0), z=c(1,1,1,1,1))\nlasso_opt_df &lt;- tibble(x=0, y=1, label=c(TeX(\"$\\\\beta^*_{lasso}$\")))\n\n# And plot\nbase_plot &lt;- ggplot() +\n  geom_contour_filled(\n    data=data_df, aes(x=x, y=y, z=z),\n    alpha=0.8, binwidth = 0.04, color='black', linewidth=0.65\n  ) +\n  # y-axis\n  geom_segment(aes(x=0, xend=0, y=-Inf, yend=Inf), color='white', linewidth=0.5, linetype=\"solid\") +\n  # Unconstrained optimal beta\n  geom_point(data=beta_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=beta_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.45, vjust=0.65, parse=TRUE, alpha=0.9\n  ) +\n  scale_fill_viridis_d(option=\"C\") +\n  #coord_equal() +\n  labs(\n    #title = \"Model Selection: Ridge vs. Lasso Constraints\",\n    x = TeX(\"$\\\\beta_1$\"),\n    y = TeX(\"$\\\\beta_2$\")\n  )\nridge_plot &lt;- base_plot +\n  geom_circle(\n    aes(x0=0, y0=0, r=1, alpha=I(0.1), linetype=\"circ\", color='circ'), fill=NA, linewidth=0.5\n  )\n  # geom_point(\n  #   data=data.frame(x=0, y=0), aes(x=x, y=y),\n  #   shape=21, size=135.8, color='white', stroke=1.2, linestyle=\"dashed\"\n  # )\nlasso_plot &lt;- ridge_plot +\n  geom_polygon(\n    data=lasso_df, aes(x=x, y=y, linetype=\"diamond\", color=\"diamond\"),\n    fill='white',\n    alpha=0.5,\n    linewidth=1\n  ) +\n  # Ridge beta\n  geom_point(data=ridge_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=ridge_opt_df, aes(x=x, y=y, label=label),\n    hjust=2, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  # Lasso beta\n  geom_point(data=lasso_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=lasso_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.75, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  ylim(ybound[1], ybound[2]) +\n  # xlim(xbound[1], xbound[2]) +\n  scale_linetype_manual(\"Line\", values=c(\"diamond\"=\"solid\", \"circ\"=\"dashed\"), labels=c(\"a\",\"b\")) +\n  scale_color_manual(\"Color\", values=c(\"diamond\"=\"white\", \"circ\"=\"white\"), labels=c(\"c\",\"d\")) +\n  # scale_fill_manual(\"Test\") +\n  # x-axis\n  geom_segment(aes(x=-Inf, xend=Inf, y=0, yend=0), color='white') +\n  theme_dsan(base_size=16) +\n  coord_fixed() +\n  theme(\n    legend.position = \"none\",\n    axis.line = element_blank(),\n    axis.ticks = element_blank()\n  )\nlasso_plot",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#bayesian-interpretation",
    "href": "w06/index.html#bayesian-interpretation",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Bayesian Interpretation",
    "text": "Bayesian Interpretation\n\n\n\nBelief \\(A\\): Most/all of the features you included have important effect on \\(Y\\)\n\\(A \\implies\\) Gaussian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), pdf of \\(X\\) is\n\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[ -\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma}\\right)^2 \\right]\n\\]\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nprior_labs &lt;- labs(\n  x = TeX(\"$\\\\beta_j$\"),\n  y = TeX(\"$f(\\\\beta_j)$\")\n)\nggplot() +\n  stat_function(fun=dnorm, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nGaussian prior \\(\\leadsto\\) Ridge Regression!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(\\sigma^2\\))\n\n\n\nBelief \\(B\\): Only a few of the features you included have important effect on \\(Y\\)\n\\(B \\implies\\) Laplacian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{L}(\\mu, b)\\), pdf of \\(X\\) is\n\n\\[\nf(x) = \\frac{1}{2b}\\exp\\left[ -\\left| \\frac{x - \\mu}{b} \\right| \\right]\n\\]\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(extraDistr) |&gt; suppressPackageStartupMessages()\nggplot() +\n  stat_function(fun=dlaplace, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nLaplacian prior \\(\\leadsto\\) Lasso!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(b\\))",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#ok-so-how-do-we-find-these-magical-lambda-values",
    "href": "w06/index.html#ok-so-how-do-we-find-these-magical-lambda-values",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Ok So‚Ä¶ How Do We Find These Magical \\(\\lambda\\) Values?",
    "text": "Ok So‚Ä¶ How Do We Find These Magical \\(\\lambda\\) Values?\n\nCross-Validation!\n(‚Ä¶that‚Äôs, uh, that‚Äôs it! That‚Äôs the whole slide!)\n(But let‚Äôs look at what we find when we use CV‚Ä¶)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#varying-lambda-leadsto-tradeoff-curve",
    "href": "w06/index.html#varying-lambda-leadsto-tradeoff-curve",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Varying \\(\\lambda\\) \\(\\leadsto\\) Tradeoff Curve!",
    "text": "Varying \\(\\lambda\\) \\(\\leadsto\\) Tradeoff Curve!\n\nRidge Regression Case:\n\n\n\n\n\n\n\n\n\nIncreasing \\(\\lambda\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[\\text{RSS} + \\lambda \\| \\boldsymbol\\beta \\|_2^2 \\right]\\)\n\n\n\n\\(\\equiv\\)\n\n\n\nDecreasing \\(s\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[ \\text{RSS} \\right] \\text{ s.t. }\\|\\boldsymbol\\beta\\|_2^2 \\leq s\\)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#only-change-l1-instead-of-l2-norm",
    "href": "w06/index.html#only-change-l1-instead-of-l2-norm",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Only Change: \\(L^1\\) instead of \\(L^2\\) Norm ü§Ø",
    "text": "Only Change: \\(L^1\\) instead of \\(L^2\\) Norm ü§Ø\n\nLasso Case:\n\n\n\n\n\n\n\n\n\nIncreasing \\(\\lambda\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[\\text{RSS} + \\lambda \\| \\boldsymbol\\beta \\|_1\\right]\\)\n\n\n\n\\(\\equiv\\)\n\n\n\nDecreasing \\(s\\) in \\(\\displaystyle \\min_{\\boldsymbol\\beta}\\left[ \\text{RSS} \\right] \\text{ s.t. }\\|\\boldsymbol\\beta\\|_1 \\leq s\\)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#week-7-preview-linear-functions",
    "href": "w06/index.html#week-7-preview-linear-functions",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Week 7 Preview: Linear Functions",
    "text": "Week 7 Preview: Linear Functions\n\n\nYou‚Äôve already seen polynomial regression:\n\n\\[\nY = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\cdots + \\beta_d X^d\n\\]\n\nPlus (technically) ‚ÄúFourier regression‚Äù:\n\n\\[\n\\begin{align*}\nY = \\beta_0 + \\beta_1 \\cos(\\pi X) + \\beta_2 \\sin(\\pi X) + \\cdots + \\beta_{2d-1}\\cos(\\pi d X) + \\beta_{2d}\\sin(\\pi d X)\n\\end{align*}\n\\]\n\n\n\nImage source",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#new-regression-just-dropped",
    "href": "w06/index.html#new-regression-just-dropped",
    "title": "Week 6: Regularization for Model Selection",
    "section": "New Regression Just Dropped",
    "text": "New Regression Just Dropped\nPiecewise regression:\n Choose \\(K\\) cutpoints \\(c_1, \\ldots, c_K\\)\n Let \\(C_k(X) = \\mathbb{1}[c_{k-1} \\leq X &lt; c_k]\\), (\\(c_0 \\equiv -\\infty\\), \\(c_{K+1} \\equiv \\infty\\))\n\\[\nY = \\beta_0 + \\beta_1C_1(X) + \\beta_2C_2(X) + \\cdots + \\beta_KC_K(X)\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#decomposing-fancy-regressions-into-core-pieces",
    "href": "w06/index.html#decomposing-fancy-regressions-into-core-pieces",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Decomposing Fancy Regressions into Core ‚ÄúPieces‚Äù",
    "text": "Decomposing Fancy Regressions into Core ‚ÄúPieces‚Äù\n\nQ: What do all these types of regression have in common?\nA: They can all be written in the form\n\\[\nY = \\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\cdots + \\beta_d b_d(X)\n\\]\nWhere \\(b(\\cdot)\\) is called a basis function\n\nLinear (\\(d = 1\\)): \\(b_1(X) = X\\)\nPolynomial: \\(b_j(X) = X^j\\)\nPiecewise: \\(b_j(X) = \\mathbb{1}[c_{j-1} \\leq X &lt; c_j]\\)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w08/slides.html#quick-roadmap",
    "href": "w08/slides.html#quick-roadmap",
    "title": "Week 8: Support Vector Machines",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nWeeks 1-7: Regression was central motivating problem\n\n(Even when we looked at classification via Logistic Regression, it was regression and then a final thresholding step on the regression result)\n\nThis Week:\n\nOn the one hand, we shift focus to binary classification through learning decision boundaries\nOn the other hand, everything we‚Äôve discussed about handling non-linearity will come into play!"
  },
  {
    "objectID": "w08/slides.html#decision-boundaries-leadsto-max-margin-classifiers",
    "href": "w08/slides.html#decision-boundaries-leadsto-max-margin-classifiers",
    "title": "Week 8: Support Vector Machines",
    "section": "Decision Boundaries \\(\\leadsto\\) Max-Margin Classifiers",
    "text": "Decision Boundaries \\(\\leadsto\\) Max-Margin Classifiers\n\nMost important building block for SVMs‚Ä¶\nBut should not be used on its own! For reasons we‚Äôll see shortly\n\n(Like ‚ÄúLinear Probability Models‚Äù or ‚ÄúQuadratic Splines‚Äù, MMCs highlight problem that [usable method] solves!)\n\nMMC \\(\\leadsto\\) Support Vector Classifiers (can be used)\nMMC \\(\\leadsto\\) Support Vector Machines (can be used)"
  },
  {
    "objectID": "w08/slides.html#separating-hyperplanes",
    "href": "w08/slides.html#separating-hyperplanes",
    "title": "Week 8: Support Vector Machines",
    "section": "Separating Hyperplanes",
    "text": "Separating Hyperplanes\n\nAny line which perfectly separates positive from negative cases is a separating hyperplane\n\n\n\nCode\nbase_plot +\n  geom_abline(\n    data=rand_lines_df, aes(slope=slope, intercept=intercept, linetype=is_sep)\n  ) +\n  geom_abline(\n    data=rand_lines_df |&gt; filter(is_sep),\n    aes(slope=slope, intercept=intercept),\n    linewidth=3, color=cb_palette[4], alpha=0.333\n  ) +\n  scale_linetype_manual(\"Separating?\", values=c(\"dotted\", \"dashed\")) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  labs(\n    title = paste0(\"The Like vs. Dislike Boundary: 10 Guesses\"),\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )\n\n\n\n\nWhy might the left line be better? (Think of DGP!)"
  },
  {
    "objectID": "w08/slides.html#max-margin-hyperplanes",
    "href": "w08/slides.html#max-margin-hyperplanes",
    "title": "Week 8: Support Vector Machines",
    "section": "Max-Margin Hyperplanes",
    "text": "Max-Margin Hyperplanes\n\nAmong the two, left hyperplane establishes a larger margin between the two classes!\nMargin (of hyperplane) = Smallest distance to datapoint (Think of hyperplane as middle of separating slab: how wide is the slab?)\nImportant: Note how margin is not the same as average distance!\n\n\n\n\n\nCode\nsep_lines_df &lt;- rand_lines_df |&gt; filter(is_sep) |&gt; mutate(\n  norm_slope = (-1)/slope\n)\ncur_line_df &lt;- sep_lines_df |&gt; filter(slope &gt; 0)\n# left_line_df\n# And make one copy per point\ncur_sup_df &lt;- uncount(cur_line_df, nrow(house_df))\ncur_sup_df &lt;- bind_cols(cur_sup_df, house_df)\ncur_sup_df &lt;- cur_sup_df |&gt; mutate(\n  norm_intercept = yrs - norm_slope * sqm,\n  margin_intercept = yrs - slope * sqm,\n  margin_intercept_gap = intercept - margin_intercept,\n  margin_intercept_inv = intercept + margin_intercept_gap,\n  norm_cross_x = -(norm_intercept - intercept) / (norm_slope - slope),\n  x_gap = norm_cross_x - sqm,\n  norm_cross_y = yrs + x_gap * norm_slope,\n  vec_margin = label * (b0 + b1 * sqm + b2 * yrs),\n  is_sv = vec_margin &lt;= 240\n)\nbase_plot +\n  geom_abline(\n    data=cur_line_df, aes(slope=slope, intercept=intercept), linetype=\"solid\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept\n    ),\n    linetype=\"dashed\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept_inv\n    ),\n    linetype=\"dashed\"\n  ) +\n  geom_segment(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y),\n    color=cb_palette[4], linewidth=3\n  ) +\n  geom_segment(\n    data=cur_sup_df,\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y, linetype=is_sv)\n  ) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  scale_linetype_manual(\"Support\\nVector?\", values=c(\"dotted\", \"solid\")) +\n  labs(\n    title = paste0(\"Left Hyperplane Distances\"),\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# New calculation: line with same slope but that hits the SV\n# y - y1 = m(x - x1), so...\n# y - yrs = m(x - sqm) &lt;=&gt; y = m(x-sqm) + yrs &lt;=&gt; y = mx  - m*sqm + yrs\n# &lt;=&gt; b = yrs - -m*sqm\ncur_line_df &lt;- sep_lines_df |&gt; filter(slope &lt; 0)\n# left_line_df\n# And make one copy per point\ncur_sup_df &lt;- uncount(cur_line_df, nrow(house_df))\ncur_sup_df &lt;- bind_cols(cur_sup_df, house_df)\ncur_sup_df &lt;- cur_sup_df |&gt; mutate(\n  norm_intercept = yrs - norm_slope * sqm,\n  margin_intercept = yrs - slope * sqm,\n  margin_intercept_gap = intercept - margin_intercept,\n  margin_intercept_inv = intercept + margin_intercept_gap,\n  norm_cross_x = -(norm_intercept - intercept) / (norm_slope - slope),\n  x_gap = norm_cross_x - sqm,\n  norm_cross_y = yrs + x_gap * norm_slope,\n  vec_margin = abs(label * (b0 + b1 * sqm + b2 * yrs)),\n  is_sv = vec_margin &lt;= 25\n)\nbase_plot +\n  geom_abline(\n    data=cur_line_df, aes(slope=slope, intercept=intercept), linetype=\"solid\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept\n    ),\n    linetype=\"dashed\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept_inv\n    ),\n    linetype=\"dashed\"\n  ) +\n  # geom_abline(\n  #   data=cur_line_df,\n  #   aes(slope=slope, intercept=intercept),\n  #   linewidth=3, color=cb_palette[4], alpha=0.333\n  # ) +\n  geom_segment(\n    data=cur_sup_df |&gt; filter(vec_margin &lt;= 18),\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y),\n    color=cb_palette[4], linewidth=3\n  ) +\n  geom_segment(\n    data=cur_sup_df,\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y, linetype=is_sv)\n  ) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  scale_linetype_manual(\"Support\\nVector?\", values=c(\"dotted\", \"solid\")) +\n  labs(\n    title = paste0(\"Right Hyperplane Margin\"),\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )"
  },
  {
    "objectID": "w08/slides.html#optimal-max-margin-hyperplane",
    "href": "w08/slides.html#optimal-max-margin-hyperplane",
    "title": "Week 8: Support Vector Machines",
    "section": "Optimal Max-Margin Hyperplane",
    "text": "Optimal Max-Margin Hyperplane\n\nOn previous slides we chose from a small set of randomly-generated lines‚Ä¶\nNow that we have max-margin objective, we can optimize over all possible lines!\nAny hyperplane can be written as \\(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} = 0\\)\n\n\n\nLet \\(y_i = \\begin{cases} +1 &\\text{if house }i\\text{ Liked} \\\\ -1 &\\text{if house }i\\text{ Disliked}\\end{cases}\\)\n\\[\n\\begin{align*}\n\\underset{\\beta_0, \\beta_1, \\beta_2, M}{\\text{maximize}}\\text{  } & M \\\\\n\\text{s.t.  } & y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}) \\geq M, \\\\\n~ & \\beta_0^2 + \\beta_1^2 + \\beta_2^2 = 1\n\\end{align*}\n\\]\n\n\\(\\beta_0^2 + \\beta_1^2 + \\beta_2^2 = 1\\) just ensures that \\(y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2})\\) gives distance from hyperplane to \\(x_i\\)\n\n\n\n\nCode\nlibrary(e1071)\nliked &lt;- as.factor(house_df$Rating == \"Liked\")\ncent_df &lt;- house_df\ncent_df$sqm &lt;- scale(cent_df$sqm)\ncent_df$yrs &lt;- scale(cent_df$yrs)\nsvm_model &lt;- svm(liked ~ sqm + yrs, data=cent_df, kernel=\"linear\")\ncf &lt;- coef(svm_model)\nsep_intercept &lt;- -cf[1] / cf[3]\nsep_slope &lt;- -cf[2] / cf[3]\n# Invert Z-scores\nsd_ratio &lt;- sd(house_df$yrs) / sd(house_df$sqm)\ninv_slope &lt;- sd_ratio * sep_slope\ninv_intercept &lt;- mean(house_df$yrs) - inv_slope * mean(house_df$sqm) + sd(house_df$yrs)*sep_intercept\n# And the margin boundary\nsv_index &lt;- svm_model$index[1]\nsv_sqm &lt;- house_df$sqm[sv_index]\nsv_yrs &lt;- house_df$yrs[sv_index]\nmargin_intercept &lt;- sv_yrs - inv_slope * sv_sqm\nmargin_diff &lt;- inv_intercept - margin_intercept\nmargin_intercept_inv &lt;- inv_intercept + margin_diff\nbase_plot +\n  coord_equal() +\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=14) +\n  geom_abline(\n    intercept=inv_intercept, slope=inv_slope, linetype=\"solid\"\n  ) +\n  geom_abline(\n    intercept=margin_intercept, slope=inv_slope, linetype=\"dashed\"\n  ) +\n  geom_abline(\n    intercept=margin_intercept_inv, slope=inv_slope, linetype=\"dashed\"\n  ) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  scale_linetype_manual(\"Support\\nVector?\", values=c(\"dotted\", \"solid\")) +\n  labs(\n    title = \"Optimal Max-Margin Hyperplane\",\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )"
  },
  {
    "objectID": "w08/slides.html#when-does-this-fail",
    "href": "w08/slides.html#when-does-this-fail",
    "title": "Week 8: Support Vector Machines",
    "section": "When Does This Fail?",
    "text": "When Does This Fail?\n\nWhat do we do in this case? (No separating hyperplane!)\n\n\n\nCode\n# Generate gaussian blob of disliked + gaussian\n# blob of liked :3\nlibrary(mvtnorm) |&gt; suppressPackageStartupMessages()\nset.seed(5304)\nnum_houses &lt;- 100\n# Shared covariance matrix\nSigma_all &lt;- matrix(c(12,0,0,20), nrow=2, ncol=2, byrow=TRUE)\n# Negative datapoints\nmu_neg &lt;- c(10, 12.5)\nneg_matrix &lt;- rmvnorm(num_houses/2, mean=mu_neg, sigma=Sigma_all)\ncolnames(neg_matrix) &lt;- c(\"sqm\", \"yrs\")\nneg_df &lt;- as_tibble(neg_matrix) |&gt; mutate(Rating=\"Disliked\")\n# Positive datapoints\nmu_pos &lt;- c(21, 12.5)\npos_matrix &lt;- rmvnorm(num_houses/2, mean=mu_pos, sigma=Sigma_all)\ncolnames(pos_matrix) &lt;- c(\"sqm\", \"yrs\")\npos_df &lt;- as_tibble(pos_matrix) |&gt; mutate(Rating=\"Liked\")\n# And combine\nnonsep_df &lt;- bind_rows(neg_df, pos_df)\nnonsep_df &lt;- nonsep_df |&gt; filter(yrs &gt;= 5 & sqm &lt;= 24 & sqm &gt;= 7)\n# Plot\nnonsep_plot &lt;- nonsep_df |&gt; ggplot(aes(x=sqm, y=yrs)) +\n  labs(\n    title = \"Jeff's House Search\",\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  ) +\n  # xlim(6,25) + ylim(2,22) +\n  coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=22)\nnonsep_plot +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.25,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=4, stroke=0.75, alpha=0.333)\n\n\n\n\nHere we need to keep in mind how our goal is relative to the Data-Generating Process!"
  },
  {
    "objectID": "w08/slides.html#no-separating-hyperplane",
    "href": "w08/slides.html#no-separating-hyperplane",
    "title": "Week 8: Support Vector Machines",
    "section": "No Separating Hyperplane? üòß",
    "text": "No Separating Hyperplane? üòß\n\n\nDon‚Äôt let the perfect be the enemy of the good!"
  },
  {
    "objectID": "w08/slides.html#perfect-vs.-good",
    "href": "w08/slides.html#perfect-vs.-good",
    "title": "Week 8: Support Vector Machines",
    "section": "Perfect vs.¬†Good",
    "text": "Perfect vs.¬†Good\n\nIn fact, ‚Äúsub-optimal‚Äù may be better than optimal, in terms of overfitting!\n\n\nISLR Figure 9.5: Notice how the dashed line on the right side may be better in terms of generalization, even though the solid line is the optimal Max-Margin Hyperplane!"
  },
  {
    "objectID": "w08/slides.html#handling-non-linearly-separable-data",
    "href": "w08/slides.html#handling-non-linearly-separable-data",
    "title": "Week 8: Support Vector Machines",
    "section": "Handling Non-Linearly-Separable Data",
    "text": "Handling Non-Linearly-Separable Data\n\\[\n\\begin{align*}\n\\underset{\\beta_0, \\beta_1, \\beta_2, \\varepsilon_1, \\ldots, \\varepsilon_n, M}{\\text{maximize}}\\text{ } \\; & M \\\\\n\\text{s.t. } \\; & y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}) \\geq M(1 - \\varepsilon_i), \\\\\n~ & \\varepsilon_i \\geq 0, \\sum_{i=1}^{n}\\varepsilon_i \\leq C, \\\\\n~ & \\beta_0^2 + \\beta_1^2 + \\beta_2^2 = 1\n\\end{align*}\n\\]\n\n\\(C = 0\\) \\(\\implies\\) Max-Margin Classifier\nWe want to choose \\(C\\) to optimize generalizability to unseen data‚Ä¶ So, choose via Cross-Validation"
  },
  {
    "objectID": "w08/slides.html#slack-variables",
    "href": "w08/slides.html#slack-variables",
    "title": "Week 8: Support Vector Machines",
    "section": "‚ÄúSlack‚Äù Variables",
    "text": "‚ÄúSlack‚Äù Variables\n\nThe \\(\\varepsilon_i\\) values also give us valuable information!\n\\(\\varepsilon_i = 0 \\implies\\) observation \\(i\\) on correct side of margin\n\\(0 &lt; \\varepsilon_i \\leq 1 \\implies\\) observation \\(i\\) is within margin but on correct side of hyperplane\n\\(\\varepsilon_i &gt; 1 \\implies\\) observation \\(i\\) on wrong side of hyperplane\n\n\nImage source"
  },
  {
    "objectID": "w08/slides.html#our-non-linearly-separable-example",
    "href": "w08/slides.html#our-non-linearly-separable-example",
    "title": "Week 8: Support Vector Machines",
    "section": "Our Non-Linearly-Separable Example",
    "text": "Our Non-Linearly-Separable Example\n¬†\n\n\nCode\nlibrary(e1071)\nliked &lt;- as.factor(nonsep_df$Rating == \"Liked\")\nnonsep_cent_df &lt;- nonsep_df\nnonsep_cent_df$sqm &lt;- scale(nonsep_cent_df$sqm)\nnonsep_cent_df$yrs &lt;- scale(nonsep_cent_df$yrs)\n# Compute boundary for different cost budgets\nbudget_vals &lt;- c(0.01, 1, 5)\nsvm_df &lt;- tibble(\n  sep_slope=numeric(), sep_intercept=numeric(),\n  inv_slope=numeric(), inv_intercept=numeric(),\n  margin_intercept=numeric(), margin_intercept_inv=numeric(),\n  budget=numeric(), budget_label=character()\n)\nfor (cur_c in budget_vals) {\n  # print(cur_c)\n  svm_model &lt;- svm(liked ~ sqm + yrs, data=nonsep_cent_df, kernel=\"linear\", cost=cur_c)\n  cf &lt;- coef(svm_model)\n  sep_intercept &lt;- -cf[1] / cf[3]\n  sep_slope &lt;- -cf[2] / cf[3]\n  # Invert Z-scores\n  sd_ratio &lt;- sd(nonsep_df$yrs) / sd(nonsep_df$sqm)\n  inv_slope &lt;- sd_ratio * sep_slope\n  inv_intercept &lt;- mean(nonsep_df$yrs) - inv_slope * mean(nonsep_df$sqm) + sd(nonsep_df$yrs)*sep_intercept\n  # And the margin boundary\n  sv_index &lt;- svm_model$index[1]\n  sv_sqm &lt;- nonsep_df$sqm[sv_index]\n  sv_yrs &lt;- nonsep_df$yrs[sv_index]\n  margin_intercept &lt;- sv_yrs - inv_slope * sv_sqm\n  margin_diff &lt;- inv_intercept - margin_intercept\n  margin_intercept_inv &lt;- inv_intercept + margin_diff\n  cur_svm_row &lt;- tibble_row(\n    budget = cur_c,\n    budget_label = paste0(\"Penalty = \",cur_c),\n    sep_slope = sep_slope,\n    sep_intercept = sep_intercept,\n    inv_slope = inv_slope,\n    inv_intercept = inv_intercept,\n    margin_intercept = margin_intercept,\n    margin_intercept_inv = margin_intercept_inv\n  )\n  svm_df &lt;- bind_rows(svm_df, cur_svm_row)\n}\nggplot() +\n  # xlim(6,25) + ylim(2,22) +\n  coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=14) +\n  geom_abline(\n    data=svm_df,\n    aes(intercept=inv_intercept, slope=inv_slope),\n    linetype=\"solid\"\n  ) +\n  geom_abline(\n    data=svm_df,\n    aes(intercept=margin_intercept, slope=inv_slope),\n    linetype=\"dashed\"\n  ) +\n  geom_abline(\n    data=svm_df,\n    aes(intercept=margin_intercept_inv, slope=inv_slope),\n    linetype=\"dashed\"\n  ) +\n  geom_point(\n    data=nonsep_df,\n    aes(x=sqm, y=yrs, color=Rating, shape=Rating), size=g_pointsize * 0.1,\n    stroke=5\n  ) +\n  geom_point(\n    data=nonsep_df,\n    aes(x=sqm, y=yrs, fill=Rating), color='black', shape=21, size=3, stroke=0.75, alpha=0.333\n  ) +\n  labs(\n    title = \"Support Vector Classifier\",\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  ) +\n  facet_wrap(vars(budget_label), nrow=1) +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, size = 0.4)\n  )"
  },
  {
    "objectID": "w08/slides.html#but-why-restrict-ourselves-to-lines",
    "href": "w08/slides.html#but-why-restrict-ourselves-to-lines",
    "title": "Week 8: Support Vector Machines",
    "section": "But‚Ä¶ Why Restrict Ourselves to Lines?",
    "text": "But‚Ä¶ Why Restrict Ourselves to Lines?\n\nWe just spent 7 weeks learning how to move from linear to non-linear! Basic template:\n\n\\[\n\\text{Nonlinear model} = \\text{Linear model} \\underbrace{- \\; \\text{linearity restriction}}_{\\text{Enables overfitting...}} \\underbrace{+ \\text{ Complexity penalty}}_{\\text{Prevent overfitting}}\n\\]\n\nSo‚Ä¶ We should be able to find reasonable non-linear boundaries!\n\n\nFrom MathWorks"
  },
  {
    "objectID": "w08/slides.html#linear-rightarrow-linear",
    "href": "w08/slides.html#linear-rightarrow-linear",
    "title": "Week 8: Support Vector Machines",
    "section": "Linear \\(\\rightarrow\\) Linear",
    "text": "Linear \\(\\rightarrow\\) Linear\n\n\n\nOld feature space: \\(\\mathcal{S} = \\{X_1, \\ldots, X_J\\}\\)\nNew feature space: \\(\\mathcal{S}^2 = \\{X_1, X_1^2, \\ldots, X_J, X_J^2\\}\\)\nLinear boundary in \\(\\mathcal{S}^2\\) \\(\\implies\\) Quadratic boundary in \\(\\mathcal{S}\\)\nHelpful example: Non-linearly separable in 1D \\(\\leadsto\\) Linearly separable in 2D\n\n\n1D Hyperplane = point! But no separating point here üò∞\n\n\nCode\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nx_vals &lt;- runif(50, min=-9, max=9)\ndata_df &lt;- tibble(x=x_vals) |&gt; mutate(\n  label = factor(ifelse(abs(x) &gt;= 6, 1, -1)),\n  x2 = x^2\n)\ndata_df |&gt; ggplot(aes(x=x, y=0, color=label)) +\n  geom_point(\n    aes(color=label, shape=label), size=g_pointsize,\n    stroke=6\n  ) +\n  geom_point(\n    aes(fill=label), color='black', shape=21, size=6, stroke=0.75, alpha=0.333\n  ) +\n  # xlim(6,25) + ylim(2,22) +\n  # coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=28) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank()\n  ) +\n  xlim(-10, 10) +\n  ylim(-0.1, 0.1) +\n  labs(title=\"Non-Linearly-Separable Data\") +\n  theme(\n    plot.margin = unit(c(0,0,0,20), \"mm\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_df |&gt; ggplot(aes(x=x, y=x2, color=label)) +\n  geom_point(\n    aes(color=label, shape=label), size=g_pointsize,\n    stroke=6\n  ) +\n  geom_point(\n    aes(fill=label), color='black', shape=21, size=6, stroke=0.75, alpha=0.333\n  ) +\n  geom_hline(yintercept=36, linetype=\"dashed\") +\n  # xlim(6,25) + ylim(2,22) +\n  # coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=28) +\n  xlim(-10, 10) +\n  labs(\n    title = TeX(\"...Linearly Separable in $R^2$!\"),\n    y = TeX(\"$x^2$\")\n  )"
  },
  {
    "objectID": "w08/slides.html#and-in-3d",
    "href": "w08/slides.html#and-in-3d",
    "title": "Week 8: Support Vector Machines",
    "section": "And in 3D‚Ä¶",
    "text": "And in 3D‚Ä¶\n¬†\n\nImage Source"
  },
  {
    "objectID": "w08/slides.html#taking-this-idea-and-running-with-it",
    "href": "w08/slides.html#taking-this-idea-and-running-with-it",
    "title": "Week 8: Support Vector Machines",
    "section": "Taking This Idea and Running With It",
    "text": "Taking This Idea and Running With It\n\nThe goal: find a transformation of the original space that the (non-linearly-separable) data lives in, such that data is linearly separable in new space\nIn theory, could just compute tons and tons of new features \\(\\{X_1, X_1^2, X_1^3, \\ldots, X_1X_2, X_1X_3, \\ldots, X_1^2X_2, X_1^2X_3, \\ldots\\}\\)\nIn practice this ‚Äúblows up‚Äù the dimensionality of the feature space, making SVM slower and slower\nWeird trick (literally): turns out we only need inner products \\(\\langle x_i, x_j \\rangle\\) between datapoints‚Ä¶ ‚ÄúKernel Trick‚Äù"
  },
  {
    "objectID": "w08/slides.html#the-magic-of-kernels",
    "href": "w08/slides.html#the-magic-of-kernels",
    "title": "Week 8: Support Vector Machines",
    "section": "The Magic of Kernels",
    "text": "The Magic of Kernels\nRadial Basis Function (RBF) Kernel:\n\\[\nK(x_i, x_{i'}) = \\exp\\left[ -\\gamma \\sum_{j=1}^{J} (x_{ij} - x_{i'j})^2 \\right]\n\\]\n\n\n\n\nCode\nlibrary(kernlab) |&gt; suppressPackageStartupMessages()\nlibrary(mlbench) |&gt; suppressPackageStartupMessages()\nif (!file.exists(\"assets/linspike_df.rds\")) {\n  set.seed(5300)\n  N &lt;- 120\n  x1_vals &lt;- runif(N, min=-5, max=5)\n  x2_raw &lt;- x1_vals\n  x2_noise &lt;- rnorm(N, mean=0, sd=1.25)\n  x2_vals &lt;- x2_raw + x2_noise\n  linspike_df &lt;- tibble(x1=x1_vals, x2=x2_vals) |&gt;\n    mutate(\n      label = factor(ifelse(x1^2 + x2^2 &lt;= 2.75, 1, -1))\n    )\n  linspike_svm &lt;- ksvm(\n    label ~ x1 + x2,\n    data = linspike_df,\n    kernel = \"rbfdot\",\n    C = 500,\n    prob.model = TRUE\n  )\n  # Grid over which to evaluate decision boundaries\n  npts &lt;- 500\n  lsgrid &lt;- expand.grid(\n    x1 = seq(from = -5, 5, length = npts),\n    x2 = seq(from = -5, 5, length = npts)\n  )\n  # Predicted probabilities (as a two-column matrix)\n  prob_svm &lt;- predict(\n    linspike_svm,\n    newdata = lsgrid,\n    type = \"probabilities\"\n  )\n  # Add predicted class probabilities\n  lsgrid2 &lt;- lsgrid |&gt;\n    cbind(\"SVM\" = prob_svm[, 1L]) |&gt;\n    tidyr::gather(Model, Prob, -x1, -x2)\n\n  # Serialize for quicker rendering\n  saveRDS(linspike_df, \"assets/linspike_df.rds\")\n  saveRDS(lsgrid2, \"assets/lsgrid2.rds\")\n} else {\n  linspike_df &lt;- readRDS(\"assets/linspike_df.rds\")\n  lsgrid2 &lt;- readRDS(\"assets/lsgrid2.rds\")\n}\nlinspike_df &lt;- linspike_df |&gt; mutate(\n  Label = label\n)\nlinspike_df |&gt; ggplot(aes(x = x1, y = x2)) +\n  # geom_point(aes(shape = label, color = label), size = 3, alpha = 0.75) +\n  geom_point(aes(shape = Label, color = Label), size = 3, stroke=4) +\n  geom_point(aes(fill=Label), color='black', shape=21, size=4, stroke=0.75, alpha=0.4) +\n  xlab(expression(X[1])) +\n  ylab(expression(X[2])) +\n  coord_fixed() +\n  theme(legend.position = \"none\") +\n  theme_dsan(base_size=28) +\n  xlim(-5, 5) + ylim(-5, 5) +\n  stat_contour(\n    data = lsgrid2,\n    aes(x = x1, y = x2, z = Prob),\n    breaks = 0.5,\n    color = \"black\"\n  ) +\n  scale_shape_manual(values=c(95, 43))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nif (!file.exists(\"assets/spiral_df.rds\")) {\n  spiral_df &lt;- as.data.frame(\n    mlbench.spirals(300, cycles = 2, sd = 0.09)\n  )\n  names(spiral_df) &lt;- c(\"x1\", \"x2\", \"label\")\n\n  # Fit SVM using a RBF kernel\n  spirals_svm &lt;- ksvm(\n    label ~ x1 + x2,\n    data = spiral_df,\n    kernel = \"rbfdot\",\n    C = 500,\n    prob.model = TRUE\n  )\n\n  # Grid over which to evaluate decision boundaries\n  npts &lt;- 500\n  xgrid &lt;- expand.grid(\n    x1 = seq(from = -2, 2, length = npts),\n    x2 = seq(from = -2, 2, length = npts)\n  )\n\n  # Predicted probabilities (as a two-column matrix)\n  prob_svm &lt;- predict(\n    spirals_svm,\n    newdata = xgrid,\n    type = \"probabilities\"\n  )\n\n  # Add predicted class probabilities\n  xgrid2 &lt;- xgrid |&gt;\n    cbind(\"SVM\" = prob_svm[, 1L]) |&gt;\n    tidyr::gather(Model, Prob, -x1, -x2)\n\n  # Serialize for quicker rendering\n  saveRDS(spiral_df, \"assets/spiral_df.rds\")\n  saveRDS(xgrid2, \"assets/xgrid2.rds\")\n} else {\n  spiral_df &lt;- readRDS(\"assets/spiral_df.rds\")\n  xgrid2 &lt;- readRDS(\"assets/xgrid2.rds\")\n}\n# And plot\nspiral_df &lt;- spiral_df |&gt; mutate(\n  Label = factor(ifelse(label == 2, 1, -1))\n)\nspiral_df |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(shape = Label, color = Label), size = 3, stroke=4) +\n  geom_point(aes(fill=Label), color='black', shape=21, size=4, stroke=0.75, alpha=0.4) +\n  xlab(expression(X[1])) +\n  ylab(expression(X[2])) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  coord_fixed() +\n  theme(legend.position = \"none\") +\n  theme_dsan(base_size=28) +\n  stat_contour(\n    data = xgrid2,\n    aes(x = x1, y = x2, z = Prob),\n    breaks = 0.5,\n    color = \"black\"\n  ) +\n  scale_shape_manual(\"Label\", values=c(95, 43))\n\n\n\n\n\nBased on code from Boehmke and Greenwell (2019), Ch 14"
  },
  {
    "objectID": "w08/slides.html#how-is-this-possible",
    "href": "w08/slides.html#how-is-this-possible",
    "title": "Week 8: Support Vector Machines",
    "section": "How Is This Possible?",
    "text": "How Is This Possible?\n\nSVM prediction for an observation \\(\\mathbf{x}\\) can be rewritten as\n\n\\[\nf(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i \\langle \\mathbf{x}, \\mathbf{x}_i \\rangle\n\\]\n\nRather than one parameter \\(\\beta_j\\) per feature, we have one parameter \\(\\alpha_i\\) per observation\nAt first this seems worse, since usually \\(n &gt; p\\)‚Ä¶ But we‚Äôre saved by support vectors!\nSince the margin itself is only a function of the support vectors (not every vector‚Äîremember that margin \\(\\neq\\) average distance!), \\(\\alpha_i &gt; 0\\) only when \\(\\mathbf{x}_i\\) a support vector!\nThus the equation becomes \\(f(\\mathbf{x}) = \\beta_0 + \\sum_{i \\in \\mathcal{SV}} \\alpha_i \\langle \\mathbf{x}, \\mathbf{x}_i \\rangle\\), and there‚Äôs one step left‚Ä¶\nKernel \\(K\\) = similarity function (generalization of inner product!)\n\n\\[\nf(\\mathbf{x}) = \\beta_0 + \\sum_{i \\in \\mathcal{SV}} \\alpha_i K(\\mathbf{x}, \\mathbf{x}_i)\n\\]"
  },
  {
    "objectID": "w08/slides.html#inner-products-are-already-similarities",
    "href": "w08/slides.html#inner-products-are-already-similarities",
    "title": "Week 8: Support Vector Machines",
    "section": "Inner Products are Already Similarities!",
    "text": "Inner Products are Already Similarities!\n\nWhen linearly separable, this is exactly the property we use (by computing inner product of new vector \\(\\mathbf{x}\\) with support vectors \\(\\mathbf{x}_i\\)) to classify!\n\n\nInteractive demo\nWhy restrict ourselves to this similarity function? Choosing \\(K\\) converts the problem:\n\nFrom finding transformation of features allowing linear separability\nTo finding similarity function for observations allowing linear separability"
  },
  {
    "objectID": "w08/slides.html#kernel-trick-as-two-sided-swiss-army-knife",
    "href": "w08/slides.html#kernel-trick-as-two-sided-swiss-army-knife",
    "title": "Week 8: Support Vector Machines",
    "section": "Kernel Trick as Two-Sided Swiss Army Knife",
    "text": "Kernel Trick as Two-Sided Swiss Army Knife\n\n If we have a linearly-separating transformation (like \\(f(x) = x^2\\)), can ‚Äúencode‚Äù as kernel, saving computation\nExample: Transformation of features \\(f(x) = x^2\\) equivalent to quadratic kernel:\n\\[\nK(x_i, x_{i'}) = (1 + \\sum_{j = 1}^{p}x_{ij}x_{i'j})^2\n\\]\n If we don‚Äôt have a transformation (and having trouble figuring it out), can change the problem into one of finding a ‚Äúgood‚Äù similarity function\nExample: Look again at RBF Kernel:\n\\[\nK(x_i, x_{i'}) = \\exp\\left[ -\\gamma \\sum_{j=1}^{J} (x_{ij} - x_{i'j})^2 \\right]\n\\]\nIt turns out: no finite collection of transformed features is equivalent to this kernel! (Roughly: can keep adding transformed features to asymptotically approach it‚Äîspace of SVMs w/kernel thus \\(&gt;\\) space of SVMs w/transformed features)"
  },
  {
    "objectID": "w08/slides.html#references",
    "href": "w08/slides.html#references",
    "title": "Week 8: Support Vector Machines",
    "section": "References",
    "text": "References\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-On Machine Learning with R. CRC Press."
  },
  {
    "objectID": "w08/index.html",
    "href": "w08/index.html",
    "title": "Week 8: Support Vector Machines",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#quick-roadmap",
    "href": "w08/index.html#quick-roadmap",
    "title": "Week 8: Support Vector Machines",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nWeeks 1-7: Regression was central motivating problem\n\n(Even when we looked at classification via Logistic Regression, it was regression and then a final thresholding step on the regression result)\n\nThis Week:\n\nOn the one hand, we shift focus to binary classification through learning decision boundaries\nOn the other hand, everything we‚Äôve discussed about handling non-linearity will come into play!",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#decision-boundaries-leadsto-max-margin-classifiers",
    "href": "w08/index.html#decision-boundaries-leadsto-max-margin-classifiers",
    "title": "Week 8: Support Vector Machines",
    "section": "Decision Boundaries \\(\\leadsto\\) Max-Margin Classifiers",
    "text": "Decision Boundaries \\(\\leadsto\\) Max-Margin Classifiers\n\nMost important building block for SVMs‚Ä¶\nBut should not be used on its own! For reasons we‚Äôll see shortly\n\n(Like ‚ÄúLinear Probability Models‚Äù or ‚ÄúQuadratic Splines‚Äù, MMCs highlight problem that [usable method] solves!)\n\nMMC \\(\\leadsto\\) Support Vector Classifiers (can be used)\nMMC \\(\\leadsto\\) Support Vector Machines (can be used)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#separating-hyperplanes",
    "href": "w08/index.html#separating-hyperplanes",
    "title": "Week 8: Support Vector Machines",
    "section": "Separating Hyperplanes",
    "text": "Separating Hyperplanes\n\nAny line which perfectly separates positive from negative cases is a separating hyperplane\n\n\n\nCode\nbase_plot +\n  geom_abline(\n    data=rand_lines_df, aes(slope=slope, intercept=intercept, linetype=is_sep)\n  ) +\n  geom_abline(\n    data=rand_lines_df |&gt; filter(is_sep),\n    aes(slope=slope, intercept=intercept),\n    linewidth=3, color=cb_palette[4], alpha=0.333\n  ) +\n  scale_linetype_manual(\"Separating?\", values=c(\"dotted\", \"dashed\")) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  labs(\n    title = paste0(\"The Like vs. Dislike Boundary: 10 Guesses\"),\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )\n\n\n\n\n\n\n\n\n\n\nWhy might the left line be better? (Think of DGP!)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#max-margin-hyperplanes",
    "href": "w08/index.html#max-margin-hyperplanes",
    "title": "Week 8: Support Vector Machines",
    "section": "Max-Margin Hyperplanes",
    "text": "Max-Margin Hyperplanes\n\nAmong the two, left hyperplane establishes a larger margin between the two classes!\nMargin (of hyperplane) = Smallest distance to datapoint (Think of hyperplane as middle of separating slab: how wide is the slab?)\nImportant: Note how margin is not the same as average distance!\n\n\n\n\n\nCode\nsep_lines_df &lt;- rand_lines_df |&gt; filter(is_sep) |&gt; mutate(\n  norm_slope = (-1)/slope\n)\ncur_line_df &lt;- sep_lines_df |&gt; filter(slope &gt; 0)\n# left_line_df\n# And make one copy per point\ncur_sup_df &lt;- uncount(cur_line_df, nrow(house_df))\ncur_sup_df &lt;- bind_cols(cur_sup_df, house_df)\ncur_sup_df &lt;- cur_sup_df |&gt; mutate(\n  norm_intercept = yrs - norm_slope * sqm,\n  margin_intercept = yrs - slope * sqm,\n  margin_intercept_gap = intercept - margin_intercept,\n  margin_intercept_inv = intercept + margin_intercept_gap,\n  norm_cross_x = -(norm_intercept - intercept) / (norm_slope - slope),\n  x_gap = norm_cross_x - sqm,\n  norm_cross_y = yrs + x_gap * norm_slope,\n  vec_margin = label * (b0 + b1 * sqm + b2 * yrs),\n  is_sv = vec_margin &lt;= 240\n)\nbase_plot +\n  geom_abline(\n    data=cur_line_df, aes(slope=slope, intercept=intercept), linetype=\"solid\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept\n    ),\n    linetype=\"dashed\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept_inv\n    ),\n    linetype=\"dashed\"\n  ) +\n  geom_segment(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y),\n    color=cb_palette[4], linewidth=3\n  ) +\n  geom_segment(\n    data=cur_sup_df,\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y, linetype=is_sv)\n  ) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  scale_linetype_manual(\"Support\\nVector?\", values=c(\"dotted\", \"solid\")) +\n  labs(\n    title = paste0(\"Left Hyperplane Distances\"),\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# New calculation: line with same slope but that hits the SV\n# y - y1 = m(x - x1), so...\n# y - yrs = m(x - sqm) &lt;=&gt; y = m(x-sqm) + yrs &lt;=&gt; y = mx  - m*sqm + yrs\n# &lt;=&gt; b = yrs - -m*sqm\ncur_line_df &lt;- sep_lines_df |&gt; filter(slope &lt; 0)\n# left_line_df\n# And make one copy per point\ncur_sup_df &lt;- uncount(cur_line_df, nrow(house_df))\ncur_sup_df &lt;- bind_cols(cur_sup_df, house_df)\ncur_sup_df &lt;- cur_sup_df |&gt; mutate(\n  norm_intercept = yrs - norm_slope * sqm,\n  margin_intercept = yrs - slope * sqm,\n  margin_intercept_gap = intercept - margin_intercept,\n  margin_intercept_inv = intercept + margin_intercept_gap,\n  norm_cross_x = -(norm_intercept - intercept) / (norm_slope - slope),\n  x_gap = norm_cross_x - sqm,\n  norm_cross_y = yrs + x_gap * norm_slope,\n  vec_margin = abs(label * (b0 + b1 * sqm + b2 * yrs)),\n  is_sv = vec_margin &lt;= 25\n)\nbase_plot +\n  geom_abline(\n    data=cur_line_df, aes(slope=slope, intercept=intercept), linetype=\"solid\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept\n    ),\n    linetype=\"dashed\"\n  ) +\n  geom_abline(\n    data=cur_sup_df |&gt; filter(is_sv),\n    aes(\n      slope=slope,\n      intercept=margin_intercept_inv\n    ),\n    linetype=\"dashed\"\n  ) +\n  # geom_abline(\n  #   data=cur_line_df,\n  #   aes(slope=slope, intercept=intercept),\n  #   linewidth=3, color=cb_palette[4], alpha=0.333\n  # ) +\n  geom_segment(\n    data=cur_sup_df |&gt; filter(vec_margin &lt;= 18),\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y),\n    color=cb_palette[4], linewidth=3\n  ) +\n  geom_segment(\n    data=cur_sup_df,\n    aes(x=sqm, y=yrs, xend=norm_cross_x, yend=norm_cross_y, linetype=is_sv)\n  ) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  scale_linetype_manual(\"Support\\nVector?\", values=c(\"dotted\", \"solid\")) +\n  labs(\n    title = paste0(\"Right Hyperplane Margin\"),\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#optimal-max-margin-hyperplane",
    "href": "w08/index.html#optimal-max-margin-hyperplane",
    "title": "Week 8: Support Vector Machines",
    "section": "Optimal Max-Margin Hyperplane",
    "text": "Optimal Max-Margin Hyperplane\n\nOn previous slides we chose from a small set of randomly-generated lines‚Ä¶\nNow that we have max-margin objective, we can optimize over all possible lines!\nAny hyperplane can be written as \\(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} = 0\\)\n\n\n\nLet \\(y_i = \\begin{cases} +1 &\\text{if house }i\\text{ Liked} \\\\ -1 &\\text{if house }i\\text{ Disliked}\\end{cases}\\)\n\\[\n\\begin{align*}\n\\underset{\\beta_0, \\beta_1, \\beta_2, M}{\\text{maximize}}\\text{  } & M \\\\\n\\text{s.t.  } & y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}) \\geq M, \\\\\n~ & \\beta_0^2 + \\beta_1^2 + \\beta_2^2 = 1\n\\end{align*}\n\\]\n\n\\(\\beta_0^2 + \\beta_1^2 + \\beta_2^2 = 1\\) just ensures that \\(y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2})\\) gives distance from hyperplane to \\(x_i\\)\n\n\n\n\nCode\nlibrary(e1071)\nliked &lt;- as.factor(house_df$Rating == \"Liked\")\ncent_df &lt;- house_df\ncent_df$sqm &lt;- scale(cent_df$sqm)\ncent_df$yrs &lt;- scale(cent_df$yrs)\nsvm_model &lt;- svm(liked ~ sqm + yrs, data=cent_df, kernel=\"linear\")\ncf &lt;- coef(svm_model)\nsep_intercept &lt;- -cf[1] / cf[3]\nsep_slope &lt;- -cf[2] / cf[3]\n# Invert Z-scores\nsd_ratio &lt;- sd(house_df$yrs) / sd(house_df$sqm)\ninv_slope &lt;- sd_ratio * sep_slope\ninv_intercept &lt;- mean(house_df$yrs) - inv_slope * mean(house_df$sqm) + sd(house_df$yrs)*sep_intercept\n# And the margin boundary\nsv_index &lt;- svm_model$index[1]\nsv_sqm &lt;- house_df$sqm[sv_index]\nsv_yrs &lt;- house_df$yrs[sv_index]\nmargin_intercept &lt;- sv_yrs - inv_slope * sv_sqm\nmargin_diff &lt;- inv_intercept - margin_intercept\nmargin_intercept_inv &lt;- inv_intercept + margin_diff\nbase_plot +\n  coord_equal() +\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=14) +\n  geom_abline(\n    intercept=inv_intercept, slope=inv_slope, linetype=\"solid\"\n  ) +\n  geom_abline(\n    intercept=margin_intercept, slope=inv_slope, linetype=\"dashed\"\n  ) +\n  geom_abline(\n    intercept=margin_intercept_inv, slope=inv_slope, linetype=\"dashed\"\n  ) +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.9,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=6, stroke=0.75, alpha=0.333) +\n  scale_linetype_manual(\"Support\\nVector?\", values=c(\"dotted\", \"solid\")) +\n  labs(\n    title = \"Optimal Max-Margin Hyperplane\",\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  )\n\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\nScale for shape is already present.\nAdding another scale for shape, which will replace the existing scale.",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#when-does-this-fail",
    "href": "w08/index.html#when-does-this-fail",
    "title": "Week 8: Support Vector Machines",
    "section": "When Does This Fail?",
    "text": "When Does This Fail?\n\nWhat do we do in this case? (No separating hyperplane!)\n\n\n\nCode\n# Generate gaussian blob of disliked + gaussian\n# blob of liked :3\nlibrary(mvtnorm) |&gt; suppressPackageStartupMessages()\nset.seed(5304)\nnum_houses &lt;- 100\n# Shared covariance matrix\nSigma_all &lt;- matrix(c(12,0,0,20), nrow=2, ncol=2, byrow=TRUE)\n# Negative datapoints\nmu_neg &lt;- c(10, 12.5)\nneg_matrix &lt;- rmvnorm(num_houses/2, mean=mu_neg, sigma=Sigma_all)\ncolnames(neg_matrix) &lt;- c(\"sqm\", \"yrs\")\nneg_df &lt;- as_tibble(neg_matrix) |&gt; mutate(Rating=\"Disliked\")\n# Positive datapoints\nmu_pos &lt;- c(21, 12.5)\npos_matrix &lt;- rmvnorm(num_houses/2, mean=mu_pos, sigma=Sigma_all)\ncolnames(pos_matrix) &lt;- c(\"sqm\", \"yrs\")\npos_df &lt;- as_tibble(pos_matrix) |&gt; mutate(Rating=\"Liked\")\n# And combine\nnonsep_df &lt;- bind_rows(neg_df, pos_df)\nnonsep_df &lt;- nonsep_df |&gt; filter(yrs &gt;= 5 & sqm &lt;= 24 & sqm &gt;= 7)\n# Plot\nnonsep_plot &lt;- nonsep_df |&gt; ggplot(aes(x=sqm, y=yrs)) +\n  labs(\n    title = \"Jeff's House Search\",\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  ) +\n  # xlim(6,25) + ylim(2,22) +\n  coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=22)\nnonsep_plot +\n  geom_point(\n    aes(color=Rating, shape=Rating), size=g_pointsize * 0.25,\n    stroke=6\n  ) +\n  geom_point(aes(fill=Rating), color='black', shape=21, size=4, stroke=0.75, alpha=0.333)\n\n\n\n\n\n\n\n\n\n\nHere we need to keep in mind how our goal is relative to the Data-Generating Process!",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#no-separating-hyperplane",
    "href": "w08/index.html#no-separating-hyperplane",
    "title": "Week 8: Support Vector Machines",
    "section": "No Separating Hyperplane? üòß",
    "text": "No Separating Hyperplane? üòß\n\n\n\n\n\n\nDon‚Äôt let the perfect be the enemy of the good!",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#perfect-vs.-good",
    "href": "w08/index.html#perfect-vs.-good",
    "title": "Week 8: Support Vector Machines",
    "section": "Perfect vs.¬†Good",
    "text": "Perfect vs.¬†Good\n\nIn fact, ‚Äúsub-optimal‚Äù may be better than optimal, in terms of overfitting!\n\n\n\n\nISLR Figure 9.5: Notice how the dashed line on the right side may be better in terms of generalization, even though the solid line is the optimal Max-Margin Hyperplane!",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#handling-non-linearly-separable-data",
    "href": "w08/index.html#handling-non-linearly-separable-data",
    "title": "Week 8: Support Vector Machines",
    "section": "Handling Non-Linearly-Separable Data",
    "text": "Handling Non-Linearly-Separable Data\n\\[\n\\begin{align*}\n\\underset{\\beta_0, \\beta_1, \\beta_2, \\varepsilon_1, \\ldots, \\varepsilon_n, M}{\\text{maximize}}\\text{ } \\; & M \\\\\n\\text{s.t. } \\; & y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}) \\geq M(1 - \\varepsilon_i), \\\\\n~ & \\varepsilon_i \\geq 0, \\sum_{i=1}^{n}\\varepsilon_i \\leq C, \\\\\n~ & \\beta_0^2 + \\beta_1^2 + \\beta_2^2 = 1\n\\end{align*}\n\\]\n\n\\(C = 0\\) \\(\\implies\\) Max-Margin Classifier\nWe want to choose \\(C\\) to optimize generalizability to unseen data‚Ä¶ So, choose via Cross-Validation",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#slack-variables",
    "href": "w08/index.html#slack-variables",
    "title": "Week 8: Support Vector Machines",
    "section": "‚ÄúSlack‚Äù Variables",
    "text": "‚ÄúSlack‚Äù Variables\n\nThe \\(\\varepsilon_i\\) values also give us valuable information!\n\\(\\varepsilon_i = 0 \\implies\\) observation \\(i\\) on correct side of margin\n\\(0 &lt; \\varepsilon_i \\leq 1 \\implies\\) observation \\(i\\) is within margin but on correct side of hyperplane\n\\(\\varepsilon_i &gt; 1 \\implies\\) observation \\(i\\) on wrong side of hyperplane\n\n\n\n\nImage source",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#our-non-linearly-separable-example",
    "href": "w08/index.html#our-non-linearly-separable-example",
    "title": "Week 8: Support Vector Machines",
    "section": "Our Non-Linearly-Separable Example",
    "text": "Our Non-Linearly-Separable Example\n¬†\n\n\nCode\nlibrary(e1071)\nliked &lt;- as.factor(nonsep_df$Rating == \"Liked\")\nnonsep_cent_df &lt;- nonsep_df\nnonsep_cent_df$sqm &lt;- scale(nonsep_cent_df$sqm)\nnonsep_cent_df$yrs &lt;- scale(nonsep_cent_df$yrs)\n# Compute boundary for different cost budgets\nbudget_vals &lt;- c(0.01, 1, 5)\nsvm_df &lt;- tibble(\n  sep_slope=numeric(), sep_intercept=numeric(),\n  inv_slope=numeric(), inv_intercept=numeric(),\n  margin_intercept=numeric(), margin_intercept_inv=numeric(),\n  budget=numeric(), budget_label=character()\n)\nfor (cur_c in budget_vals) {\n  # print(cur_c)\n  svm_model &lt;- svm(liked ~ sqm + yrs, data=nonsep_cent_df, kernel=\"linear\", cost=cur_c)\n  cf &lt;- coef(svm_model)\n  sep_intercept &lt;- -cf[1] / cf[3]\n  sep_slope &lt;- -cf[2] / cf[3]\n  # Invert Z-scores\n  sd_ratio &lt;- sd(nonsep_df$yrs) / sd(nonsep_df$sqm)\n  inv_slope &lt;- sd_ratio * sep_slope\n  inv_intercept &lt;- mean(nonsep_df$yrs) - inv_slope * mean(nonsep_df$sqm) + sd(nonsep_df$yrs)*sep_intercept\n  # And the margin boundary\n  sv_index &lt;- svm_model$index[1]\n  sv_sqm &lt;- nonsep_df$sqm[sv_index]\n  sv_yrs &lt;- nonsep_df$yrs[sv_index]\n  margin_intercept &lt;- sv_yrs - inv_slope * sv_sqm\n  margin_diff &lt;- inv_intercept - margin_intercept\n  margin_intercept_inv &lt;- inv_intercept + margin_diff\n  cur_svm_row &lt;- tibble_row(\n    budget = cur_c,\n    budget_label = paste0(\"Penalty = \",cur_c),\n    sep_slope = sep_slope,\n    sep_intercept = sep_intercept,\n    inv_slope = inv_slope,\n    inv_intercept = inv_intercept,\n    margin_intercept = margin_intercept,\n    margin_intercept_inv = margin_intercept_inv\n  )\n  svm_df &lt;- bind_rows(svm_df, cur_svm_row)\n}\nggplot() +\n  # xlim(6,25) + ylim(2,22) +\n  coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=14) +\n  geom_abline(\n    data=svm_df,\n    aes(intercept=inv_intercept, slope=inv_slope),\n    linetype=\"solid\"\n  ) +\n  geom_abline(\n    data=svm_df,\n    aes(intercept=margin_intercept, slope=inv_slope),\n    linetype=\"dashed\"\n  ) +\n  geom_abline(\n    data=svm_df,\n    aes(intercept=margin_intercept_inv, slope=inv_slope),\n    linetype=\"dashed\"\n  ) +\n  geom_point(\n    data=nonsep_df,\n    aes(x=sqm, y=yrs, color=Rating, shape=Rating), size=g_pointsize * 0.1,\n    stroke=5\n  ) +\n  geom_point(\n    data=nonsep_df,\n    aes(x=sqm, y=yrs, fill=Rating), color='black', shape=21, size=3, stroke=0.75, alpha=0.333\n  ) +\n  labs(\n    title = \"Support Vector Classifier\",\n    x = \"Square Meters\",\n    y = \"Years Old\"\n  ) +\n  facet_wrap(vars(budget_label), nrow=1) +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, size = 0.4)\n  )\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#but-why-restrict-ourselves-to-lines",
    "href": "w08/index.html#but-why-restrict-ourselves-to-lines",
    "title": "Week 8: Support Vector Machines",
    "section": "But‚Ä¶ Why Restrict Ourselves to Lines?",
    "text": "But‚Ä¶ Why Restrict Ourselves to Lines?\n\nWe just spent 7 weeks learning how to move from linear to non-linear! Basic template:\n\n\\[\n\\text{Nonlinear model} = \\text{Linear model} \\underbrace{- \\; \\text{linearity restriction}}_{\\text{Enables overfitting...}} \\underbrace{+ \\text{ Complexity penalty}}_{\\text{Prevent overfitting}}\n\\]\n\nSo‚Ä¶ We should be able to find reasonable non-linear boundaries!\n\n\n\n\nFrom MathWorks",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#linear-rightarrow-linear",
    "href": "w08/index.html#linear-rightarrow-linear",
    "title": "Week 8: Support Vector Machines",
    "section": "Linear \\(\\rightarrow\\) Linear",
    "text": "Linear \\(\\rightarrow\\) Linear\n\n\n\nOld feature space: \\(\\mathcal{S} = \\{X_1, \\ldots, X_J\\}\\)\nNew feature space: \\(\\mathcal{S}^2 = \\{X_1, X_1^2, \\ldots, X_J, X_J^2\\}\\)\nLinear boundary in \\(\\mathcal{S}^2\\) \\(\\implies\\) Quadratic boundary in \\(\\mathcal{S}\\)\nHelpful example: Non-linearly separable in 1D \\(\\leadsto\\) Linearly separable in 2D\n\n\n1D Hyperplane = point! But no separating point here üò∞\n\n\nCode\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nx_vals &lt;- runif(50, min=-9, max=9)\ndata_df &lt;- tibble(x=x_vals) |&gt; mutate(\n  label = factor(ifelse(abs(x) &gt;= 6, 1, -1)),\n  x2 = x^2\n)\ndata_df |&gt; ggplot(aes(x=x, y=0, color=label)) +\n  geom_point(\n    aes(color=label, shape=label), size=g_pointsize,\n    stroke=6\n  ) +\n  geom_point(\n    aes(fill=label), color='black', shape=21, size=6, stroke=0.75, alpha=0.333\n  ) +\n  # xlim(6,25) + ylim(2,22) +\n  # coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=28) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank()\n  ) +\n  xlim(-10, 10) +\n  ylim(-0.1, 0.1) +\n  labs(title=\"Non-Linearly-Separable Data\") +\n  theme(\n    plot.margin = unit(c(0,0,0,20), \"mm\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_df |&gt; ggplot(aes(x=x, y=x2, color=label)) +\n  geom_point(\n    aes(color=label, shape=label), size=g_pointsize,\n    stroke=6\n  ) +\n  geom_point(\n    aes(fill=label), color='black', shape=21, size=6, stroke=0.75, alpha=0.333\n  ) +\n  geom_hline(yintercept=36, linetype=\"dashed\") +\n  # xlim(6,25) + ylim(2,22) +\n  # coord_equal() +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=28) +\n  xlim(-10, 10) +\n  labs(\n    title = TeX(\"...Linearly Separable in $R^2$!\"),\n    y = TeX(\"$x^2$\")\n  )",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#and-in-3d",
    "href": "w08/index.html#and-in-3d",
    "title": "Week 8: Support Vector Machines",
    "section": "And in 3D‚Ä¶",
    "text": "And in 3D‚Ä¶\n¬†\n\n\n\nImage Source",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#taking-this-idea-and-running-with-it",
    "href": "w08/index.html#taking-this-idea-and-running-with-it",
    "title": "Week 8: Support Vector Machines",
    "section": "Taking This Idea and Running With It",
    "text": "Taking This Idea and Running With It\n\nThe goal: find a transformation of the original space that the (non-linearly-separable) data lives in, such that data is linearly separable in new space\nIn theory, could just compute tons and tons of new features \\(\\{X_1, X_1^2, X_1^3, \\ldots, X_1X_2, X_1X_3, \\ldots, X_1^2X_2, X_1^2X_3, \\ldots\\}\\)\nIn practice this ‚Äúblows up‚Äù the dimensionality of the feature space, making SVM slower and slower\nWeird trick (literally): turns out we only need inner products \\(\\langle x_i, x_j \\rangle\\) between datapoints‚Ä¶ ‚ÄúKernel Trick‚Äù",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#the-magic-of-kernels",
    "href": "w08/index.html#the-magic-of-kernels",
    "title": "Week 8: Support Vector Machines",
    "section": "The Magic of Kernels",
    "text": "The Magic of Kernels\nRadial Basis Function (RBF) Kernel:\n\\[\nK(x_i, x_{i'}) = \\exp\\left[ -\\gamma \\sum_{j=1}^{J} (x_{ij} - x_{i'j})^2 \\right]\n\\]\n\n\n\n\nCode\nlibrary(kernlab) |&gt; suppressPackageStartupMessages()\nlibrary(mlbench) |&gt; suppressPackageStartupMessages()\nif (!file.exists(\"assets/linspike_df.rds\")) {\n  set.seed(5300)\n  N &lt;- 120\n  x1_vals &lt;- runif(N, min=-5, max=5)\n  x2_raw &lt;- x1_vals\n  x2_noise &lt;- rnorm(N, mean=0, sd=1.25)\n  x2_vals &lt;- x2_raw + x2_noise\n  linspike_df &lt;- tibble(x1=x1_vals, x2=x2_vals) |&gt;\n    mutate(\n      label = factor(ifelse(x1^2 + x2^2 &lt;= 2.75, 1, -1))\n    )\n  linspike_svm &lt;- ksvm(\n    label ~ x1 + x2,\n    data = linspike_df,\n    kernel = \"rbfdot\",\n    C = 500,\n    prob.model = TRUE\n  )\n  # Grid over which to evaluate decision boundaries\n  npts &lt;- 500\n  lsgrid &lt;- expand.grid(\n    x1 = seq(from = -5, 5, length = npts),\n    x2 = seq(from = -5, 5, length = npts)\n  )\n  # Predicted probabilities (as a two-column matrix)\n  prob_svm &lt;- predict(\n    linspike_svm,\n    newdata = lsgrid,\n    type = \"probabilities\"\n  )\n  # Add predicted class probabilities\n  lsgrid2 &lt;- lsgrid |&gt;\n    cbind(\"SVM\" = prob_svm[, 1L]) |&gt;\n    tidyr::gather(Model, Prob, -x1, -x2)\n\n  # Serialize for quicker rendering\n  saveRDS(linspike_df, \"assets/linspike_df.rds\")\n  saveRDS(lsgrid2, \"assets/lsgrid2.rds\")\n} else {\n  linspike_df &lt;- readRDS(\"assets/linspike_df.rds\")\n  lsgrid2 &lt;- readRDS(\"assets/lsgrid2.rds\")\n}\nlinspike_df &lt;- linspike_df |&gt; mutate(\n  Label = label\n)\nlinspike_df |&gt; ggplot(aes(x = x1, y = x2)) +\n  # geom_point(aes(shape = label, color = label), size = 3, alpha = 0.75) +\n  geom_point(aes(shape = Label, color = Label), size = 3, stroke=4) +\n  geom_point(aes(fill=Label), color='black', shape=21, size=4, stroke=0.75, alpha=0.4) +\n  xlab(expression(X[1])) +\n  ylab(expression(X[2])) +\n  coord_fixed() +\n  theme(legend.position = \"none\") +\n  theme_dsan(base_size=28) +\n  xlim(-5, 5) + ylim(-5, 5) +\n  stat_contour(\n    data = lsgrid2,\n    aes(x = x1, y = x2, z = Prob),\n    breaks = 0.5,\n    color = \"black\"\n  ) +\n  scale_shape_manual(values=c(95, 43))\n\n\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nif (!file.exists(\"assets/spiral_df.rds\")) {\n  spiral_df &lt;- as.data.frame(\n    mlbench.spirals(300, cycles = 2, sd = 0.09)\n  )\n  names(spiral_df) &lt;- c(\"x1\", \"x2\", \"label\")\n\n  # Fit SVM using a RBF kernel\n  spirals_svm &lt;- ksvm(\n    label ~ x1 + x2,\n    data = spiral_df,\n    kernel = \"rbfdot\",\n    C = 500,\n    prob.model = TRUE\n  )\n\n  # Grid over which to evaluate decision boundaries\n  npts &lt;- 500\n  xgrid &lt;- expand.grid(\n    x1 = seq(from = -2, 2, length = npts),\n    x2 = seq(from = -2, 2, length = npts)\n  )\n\n  # Predicted probabilities (as a two-column matrix)\n  prob_svm &lt;- predict(\n    spirals_svm,\n    newdata = xgrid,\n    type = \"probabilities\"\n  )\n\n  # Add predicted class probabilities\n  xgrid2 &lt;- xgrid |&gt;\n    cbind(\"SVM\" = prob_svm[, 1L]) |&gt;\n    tidyr::gather(Model, Prob, -x1, -x2)\n\n  # Serialize for quicker rendering\n  saveRDS(spiral_df, \"assets/spiral_df.rds\")\n  saveRDS(xgrid2, \"assets/xgrid2.rds\")\n} else {\n  spiral_df &lt;- readRDS(\"assets/spiral_df.rds\")\n  xgrid2 &lt;- readRDS(\"assets/xgrid2.rds\")\n}\n# And plot\nspiral_df &lt;- spiral_df |&gt; mutate(\n  Label = factor(ifelse(label == 2, 1, -1))\n)\nspiral_df |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(shape = Label, color = Label), size = 3, stroke=4) +\n  geom_point(aes(fill=Label), color='black', shape=21, size=4, stroke=0.75, alpha=0.4) +\n  xlab(expression(X[1])) +\n  ylab(expression(X[2])) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  coord_fixed() +\n  theme(legend.position = \"none\") +\n  theme_dsan(base_size=28) +\n  stat_contour(\n    data = xgrid2,\n    aes(x = x1, y = x2, z = Prob),\n    breaks = 0.5,\n    color = \"black\"\n  ) +\n  scale_shape_manual(\"Label\", values=c(95, 43))\n\n\n\n\n\nBased on code from Boehmke and Greenwell (2019), Ch 14",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#how-is-this-possible",
    "href": "w08/index.html#how-is-this-possible",
    "title": "Week 8: Support Vector Machines",
    "section": "How Is This Possible?",
    "text": "How Is This Possible?\n\nSVM prediction for an observation \\(\\mathbf{x}\\) can be rewritten as\n\n\\[\nf(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i \\langle \\mathbf{x}, \\mathbf{x}_i \\rangle\n\\]\n\nRather than one parameter \\(\\beta_j\\) per feature, we have one parameter \\(\\alpha_i\\) per observation\nAt first this seems worse, since usually \\(n &gt; p\\)‚Ä¶ But we‚Äôre saved by support vectors!\nSince the margin itself is only a function of the support vectors (not every vector‚Äîremember that margin \\(\\neq\\) average distance!), \\(\\alpha_i &gt; 0\\) only when \\(\\mathbf{x}_i\\) a support vector!\nThus the equation becomes \\(f(\\mathbf{x}) = \\beta_0 + \\sum_{i \\in \\mathcal{SV}} \\alpha_i \\langle \\mathbf{x}, \\mathbf{x}_i \\rangle\\), and there‚Äôs one step left‚Ä¶\nKernel \\(K\\) = similarity function (generalization of inner product!)\n\n\\[\nf(\\mathbf{x}) = \\beta_0 + \\sum_{i \\in \\mathcal{SV}} \\alpha_i K(\\mathbf{x}, \\mathbf{x}_i)\n\\]",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#inner-products-are-already-similarities",
    "href": "w08/index.html#inner-products-are-already-similarities",
    "title": "Week 8: Support Vector Machines",
    "section": "Inner Products are Already Similarities!",
    "text": "Inner Products are Already Similarities!\n\nWhen linearly separable, this is exactly the property we use (by computing inner product of new vector \\(\\mathbf{x}\\) with support vectors \\(\\mathbf{x}_i\\)) to classify!\n\n\n\n\nInteractive demo\n\n\n\nWhy restrict ourselves to this similarity function? Choosing \\(K\\) converts the problem:\n\nFrom finding transformation of features allowing linear separability\nTo finding similarity function for observations allowing linear separability",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#kernel-trick-as-two-sided-swiss-army-knife",
    "href": "w08/index.html#kernel-trick-as-two-sided-swiss-army-knife",
    "title": "Week 8: Support Vector Machines",
    "section": "Kernel Trick as Two-Sided Swiss Army Knife",
    "text": "Kernel Trick as Two-Sided Swiss Army Knife\n\n If we have a linearly-separating transformation (like \\(f(x) = x^2\\)), can ‚Äúencode‚Äù as kernel, saving computation\nExample: Transformation of features \\(f(x) = x^2\\) equivalent to quadratic kernel:\n\\[\nK(x_i, x_{i'}) = (1 + \\sum_{j = 1}^{p}x_{ij}x_{i'j})^2\n\\]\n If we don‚Äôt have a transformation (and having trouble figuring it out), can change the problem into one of finding a ‚Äúgood‚Äù similarity function\nExample: Look again at RBF Kernel:\n\\[\nK(x_i, x_{i'}) = \\exp\\left[ -\\gamma \\sum_{j=1}^{J} (x_{ij} - x_{i'j})^2 \\right]\n\\]\nIt turns out: no finite collection of transformed features is equivalent to this kernel! (Roughly: can keep adding transformed features to asymptotically approach it‚Äîspace of SVMs w/kernel thus \\(&gt;\\) space of SVMs w/transformed features)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#references",
    "href": "w08/index.html#references",
    "title": "Week 8: Support Vector Machines",
    "section": "References",
    "text": "References\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-On Machine Learning with R. CRC Press.",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w07/slides.html#what-problem-are-we-solving",
    "href": "w07/slides.html#what-problem-are-we-solving",
    "title": "Week 7: Basis Functions and Splines",
    "section": "What Problem Are We Solving?",
    "text": "What Problem Are We Solving?\n\nFrom W04-06, you are now really good at thinking through issues of non-linearity in terms of polynomial regression\n\n(Hence, why I kept ‚Äúplugging in‚Äù degree of polynomial as our measure of complexity)\n\nNow we want to move towards handling any kind of non-linearity\n\n(Hence, why I kept reminding you how degree of polynomial is only one possible measure of complexity)"
  },
  {
    "objectID": "w07/slides.html#whats-so-bad-about-polynomial-regression",
    "href": "w07/slides.html#whats-so-bad-about-polynomial-regression",
    "title": "Week 7: Basis Functions and Splines",
    "section": "What‚Äôs So Bad About Polynomial Regression?",
    "text": "What‚Äôs So Bad About Polynomial Regression?\n\nMathematically, Weierstrass Approximation Theorem says we can model any function as (possibly infinite) sum of polynomials\nIn practice, this can be a horrifically bad way to actually model things:\n\n\n\n Raw data: \\(y = \\sin(x) + \\varepsilon\\)\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nN &lt;- 200\nx_vals &lt;- seq(from=-10, to=10, length.out=N)\ny_raw &lt;- sin(x_vals)\ny_noise &lt;- rnorm(length(y_raw), mean=0, sd=0.075)\ny_vals &lt;- y_raw + y_noise\ndgp_label &lt;- TeX(\"Raw Data\")\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nbase_plot &lt;- data_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  theme_dsan(base_size=28)\nbase_plot + labs(title=dgp_label)\n\n\n\n\n\n\n\n\n\n Bad (quadratic) model\n\n\nCode\nquad_model &lt;- lm(y ~ poly(x,2), data=data_df)\nquad_rss &lt;- round(get_rss(quad_model), 3)\npoly_label_2 &lt;- TeX(paste0(\"2 $\\\\beta$ Coefficients: RSS = \",quad_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ poly(x,2)) +\n  labs(title=poly_label_2)\n\n\n\n\n\n\n\n\n\n\n¬†\n\n Making it ‚Äúbetter‚Äù with more complex polynomials\n\n\nCode\npoly5_model &lt;- lm(y ~ poly(x,5), data=data_df)\npoly5_rss &lt;- round(get_rss(poly5_model), 3)\npoly5_label &lt;- TeX(paste0(\"5 $\\\\beta$ Coefficients: RSS = \",poly5_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ poly(x,5)) +\n  labs(title=poly5_label)\n\n\n\n\n\n\n\n\n\n\n\nCode\npoly8_model &lt;- lm(y ~ poly(x,8), data=data_df)\npoly8_rss &lt;- round(get_rss(poly8_model), 3)\npoly8_label &lt;- TeX(paste0(\"8 $\\\\beta$ Coefficients: RSS = \",poly8_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ poly(x,8)) +\n  labs(title=poly8_label)\n\n\n\n\n\n\n\n\n\n\n¬†\n\n Using all data to estimate single parameter, by using the ‚Äúcorrect‚Äù basis function!\n\\[\nY = \\beta_0 + \\beta_1 \\sin(x)\n\\]\n\n\nCode\nsine_model &lt;- lm(y ~ sin(x), data=data_df)\nsine_rss &lt;- round(get_rss(sine_model), 3)\nsine_label &lt;- TeX(paste0(\"Single sin(x) Coefficient: RSS = \",sine_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ sin(x)) +\n  labs(title=sine_label)"
  },
  {
    "objectID": "w07/slides.html#the-general-form",
    "href": "w07/slides.html#the-general-form",
    "title": "Week 7: Basis Functions and Splines",
    "section": "The General Form",
    "text": "The General Form\n(Decomposing Fancy Regressions into Core ‚ÄúPieces‚Äù)\n\nQ: What do all these types of regression have in common?\nA: They can all be written in the form\n\\[\nY = \\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\cdots + \\beta_d b_d(X)\n\\]\nWhere \\(b(\\cdot)\\) is called a basis function\n\nPolynomial: \\(b_j(X) = X^j\\)\nPiecewise: \\(b_j(X) = \\mathbb{1}[c_{j-1} \\leq X &lt; c_j]\\)\nEarlier Example: \\(b_1(X) = \\sin(X)\\)"
  },
  {
    "objectID": "w07/slides.html#discontinuous-segmented-regression",
    "href": "w07/slides.html#discontinuous-segmented-regression",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Discontinuous Segmented Regression",
    "text": "Discontinuous Segmented Regression\n\nHere we see why our first two basis functions were polynomial and piecewise: most rudimentary spline fits a polynomial to each piece\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nset.seed(5300)\ncompute_y &lt;- function(x) {\n  return(x * cos(x^2))\n}\nN &lt;- 500\nxmin &lt;- -1.9\nxmax &lt;- 2.7\nx_vals &lt;- runif(N, min=xmin, max=xmax)\ny_raw = compute_y(x_vals)\ny_noise = rnorm(N, mean=0, sd=0.5)\ny_vals &lt;- y_raw + y_noise\nprod_df &lt;- tibble(x=x_vals, y=y_vals)\nknot &lt;- (xmin + xmax) / 2\nprod_df &lt;- prod_df |&gt; mutate(segment = x &lt;= knot)\n# First segment model\n#left_df &lt;- prod_df |&gt; filter(x &lt;= knot_point)\n#left_model &lt;- lm(y ~ poly(x, 2), data=left_df)\nprod_df |&gt; ggplot(aes(x=x, y=y, group=segment)) +\n  geom_point(size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  geom_smooth(method='lm', formula=y ~ poly(x, 2), se=TRUE) +\n  theme_classic(base_size=22)\n\n\n\n\nWhat‚Äôs the issue with this?"
  },
  {
    "objectID": "w07/slides.html#forcing-continuity-at-knot-points",
    "href": "w07/slides.html#forcing-continuity-at-knot-points",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Forcing Continuity at Knot Points",
    "text": "Forcing Continuity at Knot Points\n\nStarting slow: let‚Äôs come back down to line world‚Ä¶\nWhy are these two lines ‚Äúallowed‚Äù to be non-continuous, under our current approach?\n\n\n\n\\[\n\\hspace{6.5cm} Y^{\\phantom{üßê}}_L = \\beta_0 + \\beta_1 X_L\n\\]\n\n\\[\nY_R = \\beta_0^{üßê} + \\beta_1 X_R \\hspace{5cm}\n\\]\n\n\n\nCode\nset.seed(5300)\nxmin_sub &lt;- -1\nxmax_sub &lt;- 1.9\nsub_df &lt;- prod_df |&gt; filter(x &gt;= xmin_sub & x &lt;= xmax_sub)\nsub_knot &lt;- (xmin_sub + xmax_sub) / 2\nsub_df &lt;- sub_df |&gt; mutate(segment = x &lt;= sub_knot)\n# First segment model\nsub_df |&gt; ggplot(aes(x=x, y=y, group=segment)) +\n  geom_point(size=0.5) +\n  geom_vline(xintercept=sub_knot, linetype=\"dashed\") +\n  geom_smooth(method='lm', formula=y ~ x, se=TRUE, linewidth=g_linewidth) +\n  theme_classic(base_size=28)\n\n\n\n\n‚Ä¶We need \\(X_R\\) to have its own slope but not its own intercept! Something like:\n\n\\[\nY = \\beta_0 + \\beta_1 (X \\text{ before } \\xi) + \\beta_2 (X \\text{ after } \\xi)\n\\]"
  },
  {
    "objectID": "w07/slides.html#truncated-power-basis-relu-basis",
    "href": "w07/slides.html#truncated-power-basis-relu-basis",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Truncated Power Basis (ReLU Basis)",
    "text": "Truncated Power Basis (ReLU Basis)\n\\[\n\\text{ReLU}(x) \\definedas (x)_+ \\definedas \\begin{cases}\n0 &\\text{if }x \\leq 0 \\\\\nx &\\text{if }x &gt; 0\n\\end{cases} \\implies (x - \\xi)_+ = \\begin{cases}\n0 &\\text{if }x \\leq \\xi \\\\\nx - \\xi &\\text{if }x &gt; 0\n\\end{cases}\n\\]\n\n\nCode\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\ntrunc_x &lt;- function(x, xi) {\n  return(ifelse(x &lt;= xi, 0, x - xi))\n}\ntrunc_x_05 &lt;- function(x) trunc_x(x, 1/2)\ntrunc_title &lt;- TeX(\"$(x - \\\\xi)_+$ with $\\\\xi = 0.5$\")\ntrunc_label &lt;- TeX(\"$y = (x - \\\\xi)_+$\")\nggplot() +\n  stat_function(\n    data=data.frame(x=c(-3,3)),\n    fun=trunc_x_05,\n    linewidth=g_linewidth\n  ) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  labs(\n    title = trunc_title,\n    x = \"x\",\n    y = trunc_label\n  )\n\n\n\n\n\\(\\Rightarrow\\) we can include a ‚Äúslope modifier‚Äù \\(\\beta_m (X - \\xi)_+\\) that only ‚Äúkicks in‚Äù once \\(x\\) goes past \\(\\xi\\)! (Changing slope by \\(\\beta_m\\))"
  },
  {
    "objectID": "w07/slides.html#linear-segments-with-continuity-at-knot",
    "href": "w07/slides.html#linear-segments-with-continuity-at-knot",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Linear Segments with Continuity at Knot",
    "text": "Linear Segments with Continuity at Knot\n\n\nOur new (non-na√Øve) model:\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 X + \\beta_2 (X - \\xi)_+ \\\\\n&= \\beta_0 + \\begin{cases}\n\\beta_1 X &\\text{if }X \\leq \\xi \\\\\n(\\beta_1 + \\beta_2)X &\\text{if }X &gt; \\xi\n\\end{cases}\n\\end{align*}\n\\]\n\n\n\nCode\nsub_df &lt;- sub_df |&gt; mutate(x_tr = ifelse(x &lt; sub_knot, 0, x - sub_knot))\nlinseg_model &lt;- lm(y ~ x + x_tr, data=sub_df)\nbroom::tidy(linseg_model) |&gt; mutate_if(is.numeric, round, 3) |&gt; select(-p.value)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\n(Intercept)\n0.187\n0.044\n4.255\n\n\nx\n1.340\n0.093\n14.342\n\n\nx_tr\n-2.868\n0.168\n-17.095\n\n\n\n\n\n\n\n\n\nCode\nsub_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=0.5) +\n  geom_vline(xintercept=sub_knot, linetype=\"dashed\") +\n  geom_smooth(method='lm', formula=y ~ x + ifelse(x &gt; sub_knot, x-sub_knot, 0), se=TRUE, linewidth=g_linewidth) +\n  theme_classic(base_size=28)"
  },
  {
    "objectID": "w07/slides.html#continuous-segmented-regression",
    "href": "w07/slides.html#continuous-segmented-regression",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Continuous Segmented Regression",
    "text": "Continuous Segmented Regression\n\n\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 (X - \\xi)_+ + \\beta_4 (X - \\xi)_+^2 \\\\[0.8em]\n&= \\beta_0 + \\begin{cases}\n\\beta_1 X + \\beta_2 X^2 &\\text{if }X \\leq \\xi \\\\\n(\\beta_1 + \\beta_3) X + (\\beta_2 + \\beta_4) X^2 &\\text{if }X &gt; \\xi\n\\end{cases}\n\\end{align*}\n\\]\n\n\n\nCode\nprod_df &lt;- prod_df |&gt; mutate(x_tr = ifelse(x &gt; knot, x - knot, 0))\nseg_model &lt;- lm(\n  y ~ poly(x, 2) + poly(x_tr, 2),\n  data=prod_df\n)\n\n\n\n\n\n\n\nCode\nseg_model |&gt; broom::tidy() |&gt; mutate_if(is.numeric, round, 3) |&gt; select(-p.value)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\n(Intercept)\n0.104\n0.035\n2.961\n\n\npoly(x, 2)1\n112.929\n6.824\n16.548\n\n\npoly(x, 2)2\n64.558\n3.692\n17.484\n\n\npoly(x_tr, 2)1\n-125.195\n7.690\n-16.281\n\n\npoly(x_tr, 2)2\n1.526\n1.036\n1.473\n\n\n\n\n\n\n\n\n\nCode\ncont_seg_plot &lt;- ggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,2) + poly(ifelse(x &gt; knot, (x - knot), 0), 2),\n    n = 300\n  ) +\n  #geom_smooth(data=prod_df |&gt; filter(segment == FALSE), aes(x=x, y=y), method='lm', formula=y ~ poly(x,2)) +\n  # geom_smooth(method=segreg, formula=y ~ seg(x, npsi=1, fixed.psi=0.5)) + # + seg(I(x^2), npsi=1)) +\n  theme_classic(base_size=22)\ncont_seg_plot\n\n\n\n\n\n\n\n\n\n\n\nThere‚Äôs still a problem here‚Ä¶ can you see what it is? (Hint: things that break calculus)"
  },
  {
    "objectID": "w07/slides.html#constrained-derivatives-quadratic-splines",
    "href": "w07/slides.html#constrained-derivatives-quadratic-splines",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Constrained Derivatives: ‚ÄúQuadratic Splines‚Äù (‚ö†Ô∏è)",
    "text": "Constrained Derivatives: ‚ÄúQuadratic Splines‚Äù (‚ö†Ô∏è)\n\nLike ‚ÄúLinear Probability Models‚Äù, ‚ÄúQuadratic Splines‚Äù are a red flag\nWhy? If we have leftmost plot below, but want it differentiable everywhere‚Ä¶ only option is to fit a single quadratic, defeating the whole point of the ‚Äúchopping‚Äù!\n\n\\[\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + 2\\beta_2 X + \\beta_3 + 2\\beta_4 X\n\\]\n\n\nContinuous Segmented:\n\n\nCode\ncont_seg_plot\n\n\n\n\n\n\n\n\n\n\n¬†\n\n‚ÄúQuadratic Spline‚Äù:\n\n\nCode\nggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,2) + ifelse(x &gt; knot, (x - knot)^2, 0),\n    n = 300\n  ) +\n  theme_dsan(base_size=22)\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n\\]\n\n\nCode\nggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,2),\n    n = 300\n  ) +\n  theme_dsan(base_size=22)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Least-complex function that allows smooth ‚Äújoining‚Äù is cubic function"
  },
  {
    "objectID": "w07/slides.html#cubic-splines-aka-splines",
    "href": "w07/slides.html#cubic-splines-aka-splines",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Cubic Splines (aka, Splines)",
    "text": "Cubic Splines (aka, Splines)\n(We did it, we finally did it)\n\n\nCode\nggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,3) + ifelse(x &gt; knot, (x - knot)^3, 0),\n    n = 300\n  ) +\n  theme_dsan(base_size=22)"
  },
  {
    "objectID": "w07/slides.html#natural-splines-why-not-stop-there",
    "href": "w07/slides.html#natural-splines-why-not-stop-there",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Natural Splines: Why Not Stop There?",
    "text": "Natural Splines: Why Not Stop There?\n\n\nPolynomials start to BEHAVE BADLY as they go to \\(-\\infty\\) and \\(\\infty\\)\n\n\nCode\nlibrary(splines) |&gt; suppressPackageStartupMessages()\nset.seed(5300)\nN_sparse &lt;- 400\nx_vals &lt;- runif(N_sparse, min=xmin, max=xmax)\ny_raw = compute_y(x_vals)\ny_noise = rnorm(N_sparse, mean=0, sd=1.0)\ny_vals &lt;- y_raw + y_noise\nsparse_df &lt;- tibble(x=x_vals, y=y_vals)\nknot_sparse &lt;- (xmin + xmax) / 2\nknot_vec &lt;- c(-1.8,-1.7,-1.6,-1.5,-1.4,-1.2,-1.0,-0.8,-0.6,-0.4,-0.2,0,knot_sparse,1,1.5,2)\nknot_df &lt;- tibble(knot=knot_vec)\n# Boundary lines\nleft_bound_line &lt;- geom_vline(\n  xintercept = xmin + 0.1, linewidth=1,\n  color=\"red\", alpha=0.8\n)\nright_bound_line &lt;- geom_vline(\n  xintercept = xmax - 0.1,\n  linewidth=1, color=\"red\", alpha=0.8\n)\nggplot() +\n  geom_point(data=sparse_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(\n    data=knot_df, aes(xintercept=knot),\n    linetype=\"dashed\"\n  ) +\n  stat_smooth(\n    data=sparse_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ bs(x, knots=c(knot_sparse), degree=25),\n    n=300\n  ) +\n  left_bound_line +\n  right_bound_line +\n  theme_dsan(base_size=22)\n\n\n\n\n\n\n\n\n\n\nNatural Splines: Force leftmost and rightmost pieces to be linear\n\n\nCode\nlibrary(splines) |&gt; suppressPackageStartupMessages()\nset.seed(5300)\nN_sparse &lt;- 400\nx_vals &lt;- runif(N_sparse, min=xmin, max=xmax)\ny_raw = compute_y(x_vals)\ny_noise = rnorm(N_sparse, mean=0, sd=1.0)\ny_vals &lt;- y_raw + y_noise\nsparse_df &lt;- tibble(x=x_vals, y=y_vals)\nknot_sparse &lt;- (xmin + xmax) / 2\nggplot() +\n  geom_point(data=sparse_df, aes(x=x, y=y), size=0.5) +\n  stat_smooth(\n    data=sparse_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ ns(x, knots=knot_vec, Boundary.knots=c(xmin + 0.1, xmax - 0.1)),\n    n=300\n  ) +\n  geom_vline(\n    data=knot_df, aes(xintercept=knot),\n    linetype=\"dashed\"\n  ) +\n  left_bound_line +\n  right_bound_line +\n  theme_dsan(base_size=22)"
  },
  {
    "objectID": "w07/slides.html#how-do-we-find-the-magical-knot-points",
    "href": "w07/slides.html#how-do-we-find-the-magical-knot-points",
    "title": "Week 7: Basis Functions and Splines",
    "section": "How Do We Find The Magical Knot Points?",
    "text": "How Do We Find The Magical Knot Points?\n\nHyperparameter 0: Number of knots \\(K\\)\nHyperparameters 1 to \\(K\\): Location \\(\\xi_k\\) for each knot\nFear: Seems like we‚Äôre going to need a bunch of slides/lectures talking about this, right?\n\n(e.g., if you really do have to manually choose them, heuristic is to place them in more ‚Äúvolatile‚Äù parts of \\(D_X\\))\n\nReality: Nope, just use CV! üòâüòé"
  },
  {
    "objectID": "w07/slides.html#reminder-w04-penalizing-complexity",
    "href": "w07/slides.html#reminder-w04-penalizing-complexity",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Reminder (W04): Penalizing Complexity",
    "text": "Reminder (W04): Penalizing Complexity\n\n\nWeek 4 (General Form):\n\\[\n\\boldsymbol\\theta^* = \\underset{\\boldsymbol\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y, \\widehat{y}; \\boldsymbol\\theta) + \\lambda \\cdot \\mathsf{Complexity}(\\boldsymbol\\theta) \\right]\n\\]\n\n\n\n\n\n\n\n\n\nLast Week (Penalizing Polynomials):\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\underset{\\boldsymbol\\beta, \\lambda}{\\operatorname{argmin}} \\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\|\\boldsymbol\\beta\\|_1 \\right]\n\\]\n\n\n\n\n\n\n\n\n\nNow (Penalizing Sharp Change / Wigglyness):\n\\[\ng^* = \\underset{g, \\lambda}{\\operatorname{argmin}} \\left[ \\sum_{i=1}^{N}(g(x_i) - y_i)^2 + \\lambda \\overbrace{\\int}^{\\mathclap{\\text{Sum of}}} [\\underbrace{g''(t)}_{\\mathclap{\\text{Change in }g'(t)}}]^2 \\mathrm{d}t \\right]\n\\]\n\n\n\n\n\n\n\n\nRestricting the first derivative would optimize for a function that doesn‚Äôt change much at all. Restricting the second derivative just constrains sharpness of the changes.\nThink of an Uber driver: penalizing speed would just make them go slowly (ensuring slow ride). Penalizing acceleration ensures that they can travel whatever speed they want, as long as they smoothly accelerate and decelerate between those speeds (ensuring non-bumpy ride!)."
  },
  {
    "objectID": "w07/slides.html#penalizing-wiggles",
    "href": "w07/slides.html#penalizing-wiggles",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Penalizing Wiggles",
    "text": "Penalizing Wiggles\n\\[\ng^* = \\underset{g, \\lambda}{\\operatorname{argmin}} \\left[ \\sum_{i=1}^{N}(g(x_i) - y_i)^2 + \\lambda \\overbrace{\\int}^{\\mathclap{\\text{Sum of}}} [\\underbrace{g''(t)}_{\\mathclap{\\text{Change in }g'(t)}}]^2 \\mathrm{d}t \\right]\n\\]\n\n(Note that we do not assume it‚Äôs a spline! We optimize over \\(g\\) itself, not \\(K\\), \\(\\xi_k\\))\nPenalizing \\([g'(t)]^2\\) would mean: penalize function for changing over its domain\nPenalizing \\([g''(t)]^2\\) means: it‚Äôs ok to change, but do so smoothly\nUber ride metaphor: remember how \\(g'(t)\\) gives speed while \\(g''(t)\\) gives acceleration\nPenalizing speed‚Ä¶ would just make driver go slowly (ensuring slow ride)\nPenalizing acceleration‚Ä¶\n\nEnsures that they can fit the traffic pattern (can drive whatever speed they want)\nAs long as they smoothly accelerate and decelerate between those speeds (ensuring non-bumpy ride!)"
  },
  {
    "objectID": "w07/slides.html#three-mind-blowing-math-facts",
    "href": "w07/slides.html#three-mind-blowing-math-facts",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Three Mind-Blowing Math Facts",
    "text": "Three Mind-Blowing Math Facts\n As \\(\\lambda \\rightarrow 0\\), \\(g^*\\) will grow as wiggly as necessary to achieve zero RSS / MSE / \\(R^2\\)\n As \\(\\lambda \\rightarrow \\infty\\), \\(g^*\\) converges to OLS linear regression line\n We didn‚Äôt optimize over splines, yet optimal \\(g\\) is a spline!\nSpecifically, a Natural Cubic Spline with‚Ä¶\n\nKnots at every data point (\\(K = N\\), \\(\\xi_i = x_i\\)), but\nPenalized via (optimal) \\(\\lambda^*\\), so not as wiggly as ‚Äúfree to wiggle‚Äù splines from previous approach: shrunken natural cubic spline"
  },
  {
    "objectID": "w07/slides.html#related-important-local-regression-at-x-x_0",
    "href": "w07/slides.html#related-important-local-regression-at-x-x_0",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Related (Important!) Local Regression at \\(X = x_0\\)",
    "text": "Related (Important!) Local Regression at \\(X = x_0\\)\n Identify \\(k\\) points closest to \\(x_0\\)\n Compute weights \\(K_i = K(x_0, x_i)\\), such that \\(K_i = 0\\) for furthest \\(i\\), and \\(K_i\\) increases as \\(x_i\\) gets closer to \\(x_0\\)\n Compute Weighted Least Squares Estimate\n\\[\n\\boldsymbol\\beta^*_{\\text{WLS}} = \\underset{\\boldsymbol\\beta}{\\operatorname{argmin}} \\left[ \\frac{1}{N}\\sum_{i=1}^{N}K_i(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right]\n\\]\n Estimate is ready! \\(\\widehat{f}(x_0) = \\beta^{\\text{WLS}}_0 + \\beta^{\\text{WLS}}_1 x_0\\)."
  },
  {
    "objectID": "w07/slides.html#references",
    "href": "w07/slides.html#references",
    "title": "Week 7: Basis Functions and Splines",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w07/index.html",
    "href": "w07/index.html",
    "title": "Week 7: Basis Functions and Splines",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#what-problem-are-we-solving",
    "href": "w07/index.html#what-problem-are-we-solving",
    "title": "Week 7: Basis Functions and Splines",
    "section": "What Problem Are We Solving?",
    "text": "What Problem Are We Solving?\n\nFrom W04-06, you are now really good at thinking through issues of non-linearity in terms of polynomial regression\n\n(Hence, why I kept ‚Äúplugging in‚Äù degree of polynomial as our measure of complexity)\n\nNow we want to move towards handling any kind of non-linearity\n\n(Hence, why I kept reminding you how degree of polynomial is only one possible measure of complexity)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#whats-so-bad-about-polynomial-regression",
    "href": "w07/index.html#whats-so-bad-about-polynomial-regression",
    "title": "Week 7: Basis Functions and Splines",
    "section": "What‚Äôs So Bad About Polynomial Regression?",
    "text": "What‚Äôs So Bad About Polynomial Regression?\n\nMathematically, Weierstrass Approximation Theorem says we can model any function as (possibly infinite) sum of polynomials\nIn practice, this can be a horrifically bad way to actually model things:\n\n\n\n Raw data: \\(y = \\sin(x) + \\varepsilon\\)\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nN &lt;- 200\nx_vals &lt;- seq(from=-10, to=10, length.out=N)\ny_raw &lt;- sin(x_vals)\ny_noise &lt;- rnorm(length(y_raw), mean=0, sd=0.075)\ny_vals &lt;- y_raw + y_noise\ndgp_label &lt;- TeX(\"Raw Data\")\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nbase_plot &lt;- data_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  theme_dsan(base_size=28)\nbase_plot + labs(title=dgp_label)\n\n\n\n\n\n\n\n\n\n Bad (quadratic) model\n\n\nCode\nquad_model &lt;- lm(y ~ poly(x,2), data=data_df)\nquad_rss &lt;- round(get_rss(quad_model), 3)\npoly_label_2 &lt;- TeX(paste0(\"2 $\\\\beta$ Coefficients: RSS = \",quad_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ poly(x,2)) +\n  labs(title=poly_label_2)\n\n\n\n\n\n\n\n\n\n\n¬†\n\n Making it ‚Äúbetter‚Äù with more complex polynomials\n\n\nCode\npoly5_model &lt;- lm(y ~ poly(x,5), data=data_df)\npoly5_rss &lt;- round(get_rss(poly5_model), 3)\npoly5_label &lt;- TeX(paste0(\"5 $\\\\beta$ Coefficients: RSS = \",poly5_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ poly(x,5)) +\n  labs(title=poly5_label)\n\n\n\n\n\n\n\n\n\n\n\nCode\npoly8_model &lt;- lm(y ~ poly(x,8), data=data_df)\npoly8_rss &lt;- round(get_rss(poly8_model), 3)\npoly8_label &lt;- TeX(paste0(\"8 $\\\\beta$ Coefficients: RSS = \",poly8_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ poly(x,8)) +\n  labs(title=poly8_label)\n\n\n\n\n\n\n\n\n\n\n¬†\n\n Using all data to estimate single parameter, by using the ‚Äúcorrect‚Äù basis function!\n\\[\nY = \\beta_0 + \\beta_1 \\sin(x)\n\\]\n\n\nCode\nsine_model &lt;- lm(y ~ sin(x), data=data_df)\nsine_rss &lt;- round(get_rss(sine_model), 3)\nsine_label &lt;- TeX(paste0(\"Single sin(x) Coefficient: RSS = \",sine_rss))\nbase_plot +\n  geom_smooth(method='lm', formula=y ~ sin(x)) +\n  labs(title=sine_label)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#the-general-form",
    "href": "w07/index.html#the-general-form",
    "title": "Week 7: Basis Functions and Splines",
    "section": "The General Form",
    "text": "The General Form\n(Decomposing Fancy Regressions into Core ‚ÄúPieces‚Äù)\n\nQ: What do all these types of regression have in common?\nA: They can all be written in the form\n\\[\nY = \\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\cdots + \\beta_d b_d(X)\n\\]\nWhere \\(b(\\cdot)\\) is called a basis function\n\nPolynomial: \\(b_j(X) = X^j\\)\nPiecewise: \\(b_j(X) = \\mathbb{1}[c_{j-1} \\leq X &lt; c_j]\\)\nEarlier Example: \\(b_1(X) = \\sin(X)\\)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#discontinuous-segmented-regression",
    "href": "w07/index.html#discontinuous-segmented-regression",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Discontinuous Segmented Regression",
    "text": "Discontinuous Segmented Regression\n\nHere we see why our first two basis functions were polynomial and piecewise: most rudimentary spline fits a polynomial to each piece\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nset.seed(5300)\ncompute_y &lt;- function(x) {\n  return(x * cos(x^2))\n}\nN &lt;- 500\nxmin &lt;- -1.9\nxmax &lt;- 2.7\nx_vals &lt;- runif(N, min=xmin, max=xmax)\ny_raw = compute_y(x_vals)\ny_noise = rnorm(N, mean=0, sd=0.5)\ny_vals &lt;- y_raw + y_noise\nprod_df &lt;- tibble(x=x_vals, y=y_vals)\nknot &lt;- (xmin + xmax) / 2\nprod_df &lt;- prod_df |&gt; mutate(segment = x &lt;= knot)\n# First segment model\n#left_df &lt;- prod_df |&gt; filter(x &lt;= knot_point)\n#left_model &lt;- lm(y ~ poly(x, 2), data=left_df)\nprod_df |&gt; ggplot(aes(x=x, y=y, group=segment)) +\n  geom_point(size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  geom_smooth(method='lm', formula=y ~ poly(x, 2), se=TRUE) +\n  theme_classic(base_size=22)\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs the issue with this?",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#forcing-continuity-at-knot-points",
    "href": "w07/index.html#forcing-continuity-at-knot-points",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Forcing Continuity at Knot Points",
    "text": "Forcing Continuity at Knot Points\n\nStarting slow: let‚Äôs come back down to line world‚Ä¶\nWhy are these two lines ‚Äúallowed‚Äù to be non-continuous, under our current approach?\n\n\n\n\\[\n\\hspace{6.5cm} Y^{\\phantom{üßê}}_L = \\beta_0 + \\beta_1 X_L\n\\]\n\n\\[\nY_R = \\beta_0^{üßê} + \\beta_1 X_R \\hspace{5cm}\n\\]\n\n\n\n\nCode\nset.seed(5300)\nxmin_sub &lt;- -1\nxmax_sub &lt;- 1.9\nsub_df &lt;- prod_df |&gt; filter(x &gt;= xmin_sub & x &lt;= xmax_sub)\nsub_knot &lt;- (xmin_sub + xmax_sub) / 2\nsub_df &lt;- sub_df |&gt; mutate(segment = x &lt;= sub_knot)\n# First segment model\nsub_df |&gt; ggplot(aes(x=x, y=y, group=segment)) +\n  geom_point(size=0.5) +\n  geom_vline(xintercept=sub_knot, linetype=\"dashed\") +\n  geom_smooth(method='lm', formula=y ~ x, se=TRUE, linewidth=g_linewidth) +\n  theme_classic(base_size=28)\n\n\n\n\n\n\n\n\n\n\n‚Ä¶We need \\(X_R\\) to have its own slope but not its own intercept! Something like:\n\n\\[\nY = \\beta_0 + \\beta_1 (X \\text{ before } \\xi) + \\beta_2 (X \\text{ after } \\xi)\n\\]",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#truncated-power-basis-relu-basis",
    "href": "w07/index.html#truncated-power-basis-relu-basis",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Truncated Power Basis (ReLU Basis)",
    "text": "Truncated Power Basis (ReLU Basis)\n\\[\n\\text{ReLU}(x) \\definedas (x)_+ \\definedas \\begin{cases}\n0 &\\text{if }x \\leq 0 \\\\\nx &\\text{if }x &gt; 0\n\\end{cases} \\implies (x - \\xi)_+ = \\begin{cases}\n0 &\\text{if }x \\leq \\xi \\\\\nx - \\xi &\\text{if }x &gt; 0\n\\end{cases}\n\\]\n\n\nCode\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\ntrunc_x &lt;- function(x, xi) {\n  return(ifelse(x &lt;= xi, 0, x - xi))\n}\ntrunc_x_05 &lt;- function(x) trunc_x(x, 1/2)\ntrunc_title &lt;- TeX(\"$(x - \\\\xi)_+$ with $\\\\xi = 0.5$\")\ntrunc_label &lt;- TeX(\"$y = (x - \\\\xi)_+$\")\nggplot() +\n  stat_function(\n    data=data.frame(x=c(-3,3)),\n    fun=trunc_x_05,\n    linewidth=g_linewidth\n  ) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  labs(\n    title = trunc_title,\n    x = \"x\",\n    y = trunc_label\n  )\n\n\n\n\n\n\n\n\n\n\n\\(\\Rightarrow\\) we can include a ‚Äúslope modifier‚Äù \\(\\beta_m (X - \\xi)_+\\) that only ‚Äúkicks in‚Äù once \\(x\\) goes past \\(\\xi\\)! (Changing slope by \\(\\beta_m\\))",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#linear-segments-with-continuity-at-knot",
    "href": "w07/index.html#linear-segments-with-continuity-at-knot",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Linear Segments with Continuity at Knot",
    "text": "Linear Segments with Continuity at Knot\n\n\nOur new (non-na√Øve) model:\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 X + \\beta_2 (X - \\xi)_+ \\\\\n&= \\beta_0 + \\begin{cases}\n\\beta_1 X &\\text{if }X \\leq \\xi \\\\\n(\\beta_1 + \\beta_2)X &\\text{if }X &gt; \\xi\n\\end{cases}\n\\end{align*}\n\\]\n\n\n\nCode\nsub_df &lt;- sub_df |&gt; mutate(x_tr = ifelse(x &lt; sub_knot, 0, x - sub_knot))\nlinseg_model &lt;- lm(y ~ x + x_tr, data=sub_df)\nbroom::tidy(linseg_model) |&gt; mutate_if(is.numeric, round, 3) |&gt; select(-p.value)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\n(Intercept)\n0.187\n0.044\n4.255\n\n\nx\n1.340\n0.093\n14.342\n\n\nx_tr\n-2.868\n0.168\n-17.095\n\n\n\n\n\n\n\n\n\n\nCode\nsub_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=0.5) +\n  geom_vline(xintercept=sub_knot, linetype=\"dashed\") +\n  geom_smooth(method='lm', formula=y ~ x + ifelse(x &gt; sub_knot, x-sub_knot, 0), se=TRUE, linewidth=g_linewidth) +\n  theme_classic(base_size=28)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#continuous-segmented-regression",
    "href": "w07/index.html#continuous-segmented-regression",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Continuous Segmented Regression",
    "text": "Continuous Segmented Regression\n\n\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 (X - \\xi)_+ + \\beta_4 (X - \\xi)_+^2 \\\\[0.8em]\n&= \\beta_0 + \\begin{cases}\n\\beta_1 X + \\beta_2 X^2 &\\text{if }X \\leq \\xi \\\\\n(\\beta_1 + \\beta_3) X + (\\beta_2 + \\beta_4) X^2 &\\text{if }X &gt; \\xi\n\\end{cases}\n\\end{align*}\n\\]\n\n\n\nCode\nprod_df &lt;- prod_df |&gt; mutate(x_tr = ifelse(x &gt; knot, x - knot, 0))\nseg_model &lt;- lm(\n  y ~ poly(x, 2) + poly(x_tr, 2),\n  data=prod_df\n)\n\n\n\n\n\n\n\n\nCode\nseg_model |&gt; broom::tidy() |&gt; mutate_if(is.numeric, round, 3) |&gt; select(-p.value)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\n(Intercept)\n0.104\n0.035\n2.961\n\n\npoly(x, 2)1\n112.929\n6.824\n16.548\n\n\npoly(x, 2)2\n64.558\n3.692\n17.484\n\n\npoly(x_tr, 2)1\n-125.195\n7.690\n-16.281\n\n\npoly(x_tr, 2)2\n1.526\n1.036\n1.473\n\n\n\n\n\n\n\n\n\nCode\ncont_seg_plot &lt;- ggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,2) + poly(ifelse(x &gt; knot, (x - knot), 0), 2),\n    n = 300\n  ) +\n  #geom_smooth(data=prod_df |&gt; filter(segment == FALSE), aes(x=x, y=y), method='lm', formula=y ~ poly(x,2)) +\n  # geom_smooth(method=segreg, formula=y ~ seg(x, npsi=1, fixed.psi=0.5)) + # + seg(I(x^2), npsi=1)) +\n  theme_classic(base_size=22)\ncont_seg_plot\n\n\n\n\n\n\n\n\n\n\n\n\nThere‚Äôs still a problem here‚Ä¶ can you see what it is? (Hint: things that break calculus)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#constrained-derivatives-quadratic-splines",
    "href": "w07/index.html#constrained-derivatives-quadratic-splines",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Constrained Derivatives: ‚ÄúQuadratic Splines‚Äù (‚ö†Ô∏è)",
    "text": "Constrained Derivatives: ‚ÄúQuadratic Splines‚Äù (‚ö†Ô∏è)\n\nLike ‚ÄúLinear Probability Models‚Äù, ‚ÄúQuadratic Splines‚Äù are a red flag\nWhy? If we have leftmost plot below, but want it differentiable everywhere‚Ä¶ only option is to fit a single quadratic, defeating the whole point of the ‚Äúchopping‚Äù!\n\n\\[\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + 2\\beta_2 X + \\beta_3 + 2\\beta_4 X\n\\]\n\n\nContinuous Segmented:\n\n\nCode\ncont_seg_plot\n\n\n\n\n\n\n\n\n\n\n¬†\n\n‚ÄúQuadratic Spline‚Äù:\n\n\nCode\nggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,2) + ifelse(x &gt; knot, (x - knot)^2, 0),\n    n = 300\n  ) +\n  theme_dsan(base_size=22)\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n\\]\n\n\nCode\nggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,2),\n    n = 300\n  ) +\n  theme_dsan(base_size=22)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Least-complex function that allows smooth ‚Äújoining‚Äù is cubic function",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#cubic-splines-aka-splines",
    "href": "w07/index.html#cubic-splines-aka-splines",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Cubic Splines (aka, Splines)",
    "text": "Cubic Splines (aka, Splines)\n(We did it, we finally did it)\n\n\nCode\nggplot() +\n  geom_point(data=prod_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(xintercept=knot, linetype=\"dashed\") +\n  stat_smooth(\n    data=prod_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ poly(x,3) + ifelse(x &gt; knot, (x - knot)^3, 0),\n    n = 300\n  ) +\n  theme_dsan(base_size=22)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#natural-splines-why-not-stop-there",
    "href": "w07/index.html#natural-splines-why-not-stop-there",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Natural Splines: Why Not Stop There?",
    "text": "Natural Splines: Why Not Stop There?\n\n\nPolynomials start to BEHAVE BADLY as they go to \\(-\\infty\\) and \\(\\infty\\)\n\n\nCode\nlibrary(splines) |&gt; suppressPackageStartupMessages()\nset.seed(5300)\nN_sparse &lt;- 400\nx_vals &lt;- runif(N_sparse, min=xmin, max=xmax)\ny_raw = compute_y(x_vals)\ny_noise = rnorm(N_sparse, mean=0, sd=1.0)\ny_vals &lt;- y_raw + y_noise\nsparse_df &lt;- tibble(x=x_vals, y=y_vals)\nknot_sparse &lt;- (xmin + xmax) / 2\nknot_vec &lt;- c(-1.8,-1.7,-1.6,-1.5,-1.4,-1.2,-1.0,-0.8,-0.6,-0.4,-0.2,0,knot_sparse,1,1.5,2)\nknot_df &lt;- tibble(knot=knot_vec)\n# Boundary lines\nleft_bound_line &lt;- geom_vline(\n  xintercept = xmin + 0.1, linewidth=1,\n  color=\"red\", alpha=0.8\n)\nright_bound_line &lt;- geom_vline(\n  xintercept = xmax - 0.1,\n  linewidth=1, color=\"red\", alpha=0.8\n)\nggplot() +\n  geom_point(data=sparse_df, aes(x=x, y=y), size=0.5) +\n  geom_vline(\n    data=knot_df, aes(xintercept=knot),\n    linetype=\"dashed\"\n  ) +\n  stat_smooth(\n    data=sparse_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ bs(x, knots=c(knot_sparse), degree=25),\n    n=300\n  ) +\n  left_bound_line +\n  right_bound_line +\n  theme_dsan(base_size=22)\n\n\n\n\n\n\n\n\n\n\nNatural Splines: Force leftmost and rightmost pieces to be linear\n\n\nCode\nlibrary(splines) |&gt; suppressPackageStartupMessages()\nset.seed(5300)\nN_sparse &lt;- 400\nx_vals &lt;- runif(N_sparse, min=xmin, max=xmax)\ny_raw = compute_y(x_vals)\ny_noise = rnorm(N_sparse, mean=0, sd=1.0)\ny_vals &lt;- y_raw + y_noise\nsparse_df &lt;- tibble(x=x_vals, y=y_vals)\nknot_sparse &lt;- (xmin + xmax) / 2\nggplot() +\n  geom_point(data=sparse_df, aes(x=x, y=y), size=0.5) +\n  stat_smooth(\n    data=sparse_df, aes(x=x, y=y),\n    method='lm',\n    formula=y ~ ns(x, knots=knot_vec, Boundary.knots=c(xmin + 0.1, xmax - 0.1)),\n    n=300\n  ) +\n  geom_vline(\n    data=knot_df, aes(xintercept=knot),\n    linetype=\"dashed\"\n  ) +\n  left_bound_line +\n  right_bound_line +\n  theme_dsan(base_size=22)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#how-do-we-find-the-magical-knot-points",
    "href": "w07/index.html#how-do-we-find-the-magical-knot-points",
    "title": "Week 7: Basis Functions and Splines",
    "section": "How Do We Find The Magical Knot Points?",
    "text": "How Do We Find The Magical Knot Points?\n\nHyperparameter 0: Number of knots \\(K\\)\nHyperparameters 1 to \\(K\\): Location \\(\\xi_k\\) for each knot\nFear: Seems like we‚Äôre going to need a bunch of slides/lectures talking about this, right?\n\n(e.g., if you really do have to manually choose them, heuristic is to place them in more ‚Äúvolatile‚Äù parts of \\(D_X\\))\n\nReality: Nope, just use CV! üòâüòé",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#reminder-w04-penalizing-complexity",
    "href": "w07/index.html#reminder-w04-penalizing-complexity",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Reminder (W04): Penalizing Complexity",
    "text": "Reminder (W04): Penalizing Complexity\n\n\nWeek 4 (General Form):\n\\[\n\\boldsymbol\\theta^* = \\underset{\\boldsymbol\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y, \\widehat{y}; \\boldsymbol\\theta) + \\lambda \\cdot \\mathsf{Complexity}(\\boldsymbol\\theta) \\right]\n\\]\n\n\n\n\n\n\n\n\n\n\nLast Week (Penalizing Polynomials):\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\underset{\\boldsymbol\\beta, \\lambda}{\\operatorname{argmin}} \\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\|\\boldsymbol\\beta\\|_1 \\right]\n\\]\n\n\n\n\n\n\n\n\n\n\nNow (Penalizing Sharp Change / Wigglyness):\n\\[\ng^* = \\underset{g, \\lambda}{\\operatorname{argmin}} \\left[ \\sum_{i=1}^{N}(g(x_i) - y_i)^2 + \\lambda \\overbrace{\\int}^{\\mathclap{\\text{Sum of}}} [\\underbrace{g''(t)}_{\\mathclap{\\text{Change in }g'(t)}}]^2 \\mathrm{d}t \\right]\n\\]\n\n\n\n\n\n\n\n\n\nRestricting the first derivative would optimize for a function that doesn‚Äôt change much at all. Restricting the second derivative just constrains sharpness of the changes.\nThink of an Uber driver: penalizing speed would just make them go slowly (ensuring slow ride). Penalizing acceleration ensures that they can travel whatever speed they want, as long as they smoothly accelerate and decelerate between those speeds (ensuring non-bumpy ride!).",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#penalizing-wiggles",
    "href": "w07/index.html#penalizing-wiggles",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Penalizing Wiggles",
    "text": "Penalizing Wiggles\n\\[\ng^* = \\underset{g, \\lambda}{\\operatorname{argmin}} \\left[ \\sum_{i=1}^{N}(g(x_i) - y_i)^2 + \\lambda \\overbrace{\\int}^{\\mathclap{\\text{Sum of}}} [\\underbrace{g''(t)}_{\\mathclap{\\text{Change in }g'(t)}}]^2 \\mathrm{d}t \\right]\n\\]\n\n(Note that we do not assume it‚Äôs a spline! We optimize over \\(g\\) itself, not \\(K\\), \\(\\xi_k\\))\nPenalizing \\([g'(t)]^2\\) would mean: penalize function for changing over its domain\nPenalizing \\([g''(t)]^2\\) means: it‚Äôs ok to change, but do so smoothly\nUber ride metaphor: remember how \\(g'(t)\\) gives speed while \\(g''(t)\\) gives acceleration\nPenalizing speed‚Ä¶ would just make driver go slowly (ensuring slow ride)\nPenalizing acceleration‚Ä¶\n\nEnsures that they can fit the traffic pattern (can drive whatever speed they want)\nAs long as they smoothly accelerate and decelerate between those speeds (ensuring non-bumpy ride!)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#three-mind-blowing-math-facts",
    "href": "w07/index.html#three-mind-blowing-math-facts",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Three Mind-Blowing Math Facts",
    "text": "Three Mind-Blowing Math Facts\n As \\(\\lambda \\rightarrow 0\\), \\(g^*\\) will grow as wiggly as necessary to achieve zero RSS / MSE / \\(R^2\\)\n As \\(\\lambda \\rightarrow \\infty\\), \\(g^*\\) converges to OLS linear regression line\n We didn‚Äôt optimize over splines, yet optimal \\(g\\) is a spline!\nSpecifically, a Natural Cubic Spline with‚Ä¶\n\nKnots at every data point (\\(K = N\\), \\(\\xi_i = x_i\\)), but\nPenalized via (optimal) \\(\\lambda^*\\), so not as wiggly as ‚Äúfree to wiggle‚Äù splines from previous approach: shrunken natural cubic spline",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#related-important-local-regression-at-x-x_0",
    "href": "w07/index.html#related-important-local-regression-at-x-x_0",
    "title": "Week 7: Basis Functions and Splines",
    "section": "Related (Important!) Local Regression at \\(X = x_0\\)",
    "text": "Related (Important!) Local Regression at \\(X = x_0\\)\n Identify \\(k\\) points closest to \\(x_0\\)\n Compute weights \\(K_i = K(x_0, x_i)\\), such that \\(K_i = 0\\) for furthest \\(i\\), and \\(K_i\\) increases as \\(x_i\\) gets closer to \\(x_0\\)\n Compute Weighted Least Squares Estimate\n\\[\n\\boldsymbol\\beta^*_{\\text{WLS}} = \\underset{\\boldsymbol\\beta}{\\operatorname{argmin}} \\left[ \\frac{1}{N}\\sum_{i=1}^{N}K_i(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 \\right]\n\\]\n Estimate is ready! \\(\\widehat{f}(x_0) = \\beta^{\\text{WLS}}_0 + \\beta^{\\text{WLS}}_1 x_0\\).",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#references",
    "href": "w07/index.html#references",
    "title": "Week 7: Basis Functions and Splines",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w10/slides.html#quick-roadmap",
    "href": "w10/slides.html#quick-roadmap",
    "title": "Week 10: Deep Learning",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nWe made it! Cutting-edge method for statistical neural learning"
  },
  {
    "objectID": "w10/slides.html#diagram-leftrightarrow-math",
    "href": "w10/slides.html#diagram-leftrightarrow-math",
    "title": "Week 10: Deep Learning",
    "section": "Diagram \\(\\leftrightarrow\\) Math",
    "text": "Diagram \\(\\leftrightarrow\\) Math\n\n\n\n\\(p = 4\\) features in Input Layer\n\\(K = 5\\) Hidden Units\nOutput Layer: Regression on activations \\(a_k\\) (Hidden Unit outputs)\n\n\\[\n\\begin{align*}\n{\\color{#976464} y} &= { \\color{#976464} \\beta_0 } + {\\color{#666693} \\sum_{k=1}^{5} } {\\color{#976464} \\beta_k } { \\color{#666693} \\overbrace{\\boxed{a_k} }^{\\mathclap{k^\\text{th}\\text{ activation}}} } \\\\\n{\\color{#976464} y} &= { \\color{#976464} \\beta_0 } + {\\color{#666693} \\sum_{k=1}^{5} } {\\color{#976464} \\beta_k } { \\color{#666693} \\underbrace{ g \\mkern-4mu \\left( w_{k0} + {\\color{#679d67} \\sum_{j=1}^{4} } w_{kj} {\\color{#679d67} x_j} \\right) }_{k^\\text{th}\\text{ activation}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w10/slides.html#matrix-form-only-if-sanity-helping",
    "href": "w10/slides.html#matrix-form-only-if-sanity-helping",
    "title": "Week 10: Deep Learning",
    "section": "Matrix Form (Only if Sanity-Helping)",
    "text": "Matrix Form (Only if Sanity-Helping)"
  },
  {
    "objectID": "w10/slides.html#example",
    "href": "w10/slides.html#example",
    "title": "Week 10: Deep Learning",
    "section": "Example",
    "text": "Example\n\nRather than pondering over what that diagram can/can‚Äôt do, consider two ‚Äútrue‚Äù DGPs:\n\n\\[\n\\begin{align*}\nY &= {\\color{#e69f00} X_1 X_2 } \\\\\nY &= {\\color{#56b4e9} X_1^2 + X_2^2 } \\\\\nY &= {\\color{#009E73} X_1 \\underset{\\mathclap{\\small \\text{XOR}}}{\\oplus} X_2}\n\\end{align*}\n\\]\n\nHow exactly is a neural net able to learn these relationships?"
  },
  {
    "objectID": "w10/slides.html#sum-of-squares",
    "href": "w10/slides.html#sum-of-squares",
    "title": "Week 10: Deep Learning",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\n\n\nCan we learn \\(y = {\\color{#56b4e9} x_1^2 + x_2^2 }\\)?\nLet‚Äôs use \\(g(x) = x^2\\).\nLet \\(\\mathbf{w}_1 = (0, 1, 0)\\), \\(\\mathbf{w}_2 = (0, 0, 1)\\).\nOur two activations are:\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n{\\color{#666693} a_1 } &= g(0 + (1)(x_1) + (0)(x_2)) = x_1^2 \\\\\n{\\color{#666693} a_2 } &= g(0 + (0)(x_1) + (1)(x_2)) = x_2^2\n\\end{align*}\n\\]\n\nSo, if \\(\\boldsymbol\\beta = (0, 1, 1)\\), then\n\n\\[\n{\\color{#976464} y } = 0 + (1)(x_1^2) + (1)(x_2^2) = {\\color{#56b4e9} x_1^2 + x_2^2} \\; ‚úÖ\n\\]"
  },
  {
    "objectID": "w10/slides.html#interaction-term",
    "href": "w10/slides.html#interaction-term",
    "title": "Week 10: Deep Learning",
    "section": "Interaction Term",
    "text": "Interaction Term\n\n\n\nCan we learn \\(Y = {\\color{#e69f00} x_1x_2}\\)?\nLet‚Äôs use \\(g(x) = x^2\\) again.\nLet \\(\\mathbf{w}_1 = (0, 1, 1)\\), \\(\\mathbf{w}_2 = (0, 1, -1)\\).\nOur two activations are:\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n{\\color{#666693} a_1 } &= g(0 + (1)(x_1) + (1)(x_2)) = (x_1 + x_2)^2 = x_1^2 + x_2^2 +2x_1x_2 \\\\\n{\\color{#666693} a_2 } &= g(0 + (1)(x_1) + (-1)(x_2)) = (x_1 - x_2)^2 = x_1^2 + x_2^2 - 2x_1x_2\n\\end{align*}\n\\]\n\nSo, if we let \\(\\boldsymbol\\beta = \\left( 0, \\frac{1}{4}, -\\frac{1}{4} \\right)\\), then\n\n\\[\n{\\color{#976464} y } = 0 + \\left(\\frac{1}{4}\\right)(x_1^2 + x_2^2 + 2x_1x_2) + \\left(-\\frac{1}{4}\\right)(x_1^2 + x_2^2 - 2x_1x_2) = {\\color{#e69f00} x_1x_2} \\; ‚úÖ\n\\]"
  },
  {
    "objectID": "w10/slides.html#the-xor-problem",
    "href": "w10/slides.html#the-xor-problem",
    "title": "Week 10: Deep Learning",
    "section": "The XOR Problem",
    "text": "The XOR Problem\n\n\n\nCan we learn \\(Y = {\\color{#009E73} x_1 \\underset{\\mathclap{\\small \\text{XOR}}}{\\oplus} x_2}\\)?\nLet‚Äôs use \\(g(x) = x^2\\) once more.\nLet \\(\\mathbf{w}_1 = (0, 1, 1)\\), \\(\\mathbf{w}_2 = (0, 1, -1)\\).\nOur two activations are:\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n{\\color{#666693} a_1 } &= g(0 + (1)(x_1) + (1)(x_2)) = (x_1 + x_2)^2 = x_1^2 + x_2^2 +2x_1x_2 \\\\\n{\\color{#666693} a_2 } &= g(0 + (1)(x_1) + (-1)(x_2)) = (x_1 - x_2)^2 = x_1^2 + x_2^2 - 2x_1x_2\n\\end{align*}\n\\]\n\nSo, if we let \\(\\boldsymbol\\beta = (0, 0, 1)\\), then\n\n\\[\n\\begin{align*}\n{\\color{#976464} y }(0,0) &= 0 + (0)(0^2 + 0^2 + 2(0)(0)) + (1)(0^2 + 0^2 - 2(0)(0)) = {\\color{#009e73} 0} \\; ‚úÖ \\\\\n{\\color{#976464} y }(0,1) &= 0 + (0)(0^2 + 1^2 + 2(0)(1)) + (1)(0^2 + 1^2 - 2(0)(1)) = {\\color{#009e73} 1} \\; ‚úÖ \\\\\n{\\color{#976464} y }(1,0) &= 0 + (0)(1^2 + 0^2 + 2(1)(0)) + (1)(1^2 + 0^2 - 2(1)(0)) = {\\color{#009e73} 1} \\; ‚úÖ \\\\\n{\\color{#976464} y }(1,1) &= 0 + (0)(1^2 + 1^2 + 2(1)(1)) + (1)(1^2 + 1^2 - 2(1)(1)) = {\\color{#009e73} 0} \\; ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w10/slides.html#but-how",
    "href": "w10/slides.html#but-how",
    "title": "Week 10: Deep Learning",
    "section": "But How?",
    "text": "But How?\n\nOutput Layer is just linear regression on activations (Hidden Layer outputs)\nWe saw in Week 7 how good basis function allows regression to learn any function\nNeural Networks: GOAT non-linear basis function learners!\n\n\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nxor_df &lt;- tribble(\n    ~x1, ~x2, ~label,\n    0, 0, 0,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 0\n) |&gt;\nmutate(\n    h1 = (x1 - x2)^2,\n    label = factor(label)\n)\nxor_df |&gt; ggplot(aes(x=x1, y=x2, label=label)) +\n  geom_point(\n    aes(color=label, shape=label),\n    size=g_pointsize * 2,\n    stroke=6\n  ) +\n  geom_point(aes(fill=label), color='black', shape=21, size=g_pointsize * 2.5, stroke=0.75, alpha=0.4) +\n  scale_x_continuous(breaks=c(0, 1)) +\n  scale_y_continuous(breaks=c(0, 1)) +\n  expand_limits(y=c(-0.1,1.1)) +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=32) +\n  remove_legend_title() +\n  labs(\n    x=TeX(\"$x_1$\"),\n    y=TeX(\"$x_2$\"),\n    title=\"XOR Problem: Original Features\"\n  )\n\n\n\n\n\n\n\n\nFigure¬†1: The DGP \\(Y = x_1 \\oplus x_2\\) produces points in \\([0,1]^2\\) which are not linearly separable\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nxor_df &lt;- tribble(\n    ~x1, ~x2, ~label,\n    0, 0, 0,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 0\n) |&gt;\nmutate(\n    h1 = (x1 - x2)^2,\n    h2 = (x1 + x2)^2,\n    h2 = ifelse(h1 &gt; 0.5 & x2==0, h2 + 0.5, h2),\n    label = factor(label)\n)\nxor_df |&gt; ggplot(aes(x=h1, y=h2, label=label)) +\n  geom_vline(xintercept=0.5, linetype=\"dashed\", linewidth=1) +\n  # Negative space\n  geom_rect(xmin=-Inf, xmax=0.5, ymin=-Inf, ymax=Inf, fill=cb_palette[1], alpha=0.15) +\n  # Positive space\n  geom_rect(xmin=0.5, xmax=Inf, ymin=-Inf, ymax=Inf, fill=cb_palette[2], alpha=0.15) +\n  geom_point(\n    aes(color=label, shape=label),\n    size=g_pointsize * 2,\n    stroke=6\n  ) +\n  geom_point(aes(fill=label), color='black', shape=21, size=g_pointsize*2.5, stroke=0.75, alpha=0.4) +\n  expand_limits(y=c(-0.2,4.2)) +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=32) +\n  remove_legend_title() +\n  labs(\n    title=\"NN-Learned Feature Space\",\n    x=TeX(\"$h_1(x_1, x_2)$\"),\n    y=TeX(\"$h_2(x_1, x_2)$\")\n  )\n\n\n\n\n\n\n\n\nFigure¬†2: Learned bases \\(h_1 = (x_1 - x_2)^2\\) and \\(h_2 = (x_1 + x_2)^2\\) enable separating hyperplane \\(h_1 = 0.5\\)\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nx1_vals &lt;- seq(from=0, to=1, by=0.0075)\nx2_vals &lt;- seq(from=0, to=1, by=0.0075)\ngrid_df &lt;- expand.grid(x1=x1_vals, x2=x2_vals) |&gt;\n  as_tibble() |&gt;\n  mutate(\n    label=factor(as.numeric((x1-x2)^2 &gt; 0.5))\n  )\nggplot() +\n  geom_point(\n    data=grid_df,\n    aes(x=x1, y=x2, color=label),\n    alpha=0.4\n  ) +\n  geom_point(\n    data=xor_df,\n    aes(x=x1, y=x2, color=label, shape=label),\n    size=g_pointsize * 2,\n    stroke=6\n  ) +\n  geom_point(\n    data=xor_df,\n    aes(x=x1, y=x2, fill=label),\n    color='black', shape=21, size=g_pointsize*2.5, stroke=0.75, alpha=0.4\n  ) +\n  geom_abline(slope=1, intercept=0.7, linetype=\"dashed\", linewidth=1) +\n  geom_abline(slope=1, intercept=-0.7, linetype=\"dashed\", linewidth=1) +\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=32) +\n  remove_legend_title() +\n  labs(\n    title=\"XOR Problem: Inverted NN Features\",\n    x=TeX(\"$X_1$\"), y=TeX(\"$X_2$\")\n  )\n\n\n\n\n\n\n\n\nFigure¬†3: Here, the blue area represents points where \\(h_1 = (x_1 - x_2)^2 &gt; 0.5\\)"
  },
  {
    "objectID": "w10/slides.html#input-representation",
    "href": "w10/slides.html#input-representation",
    "title": "Week 10: Deep Learning",
    "section": "Input Representation",
    "text": "Input Representation\n\n\n\n\n\nFrom But what is a neural network?, 3Blue1Brown"
  },
  {
    "objectID": "w10/slides.html#but-wait-ten-outputs",
    "href": "w10/slides.html#but-wait-ten-outputs",
    "title": "Week 10: Deep Learning",
    "section": "But Wait‚Ä¶ Ten Outputs?",
    "text": "But Wait‚Ä¶ Ten Outputs?\n\n\n\n\n\n\n\n\n\nThe (magical) softmax function!\n\n\\[\nz_d = \\Pr(Y = d \\mid X) = \\frac{e^{y_d}}{\\sum_{i=0}^{9}e^{y_i}}\n\\]\n\nEnsures that each \\(Z_d\\) is a probability!\n\n\\[\n\\begin{align}\n0 \\leq z_d &\\leq 1 \\; \\; \\forall ~ d \\in \\{0,\\ldots,9\\} \\\\\n\\sum_{d=0}^{9}z_d &= 1\n\\end{align}\n\\]"
  },
  {
    "objectID": "w10/slides.html#visualizing-softmax-results",
    "href": "w10/slides.html#visualizing-softmax-results",
    "title": "Week 10: Deep Learning",
    "section": "Visualizing Softmax Results",
    "text": "Visualizing Softmax Results\n\nInteractive Visualization: Handwritten-Digit Space"
  },
  {
    "objectID": "w10/slides.html#cnns",
    "href": "w10/slides.html#cnns",
    "title": "Week 10: Deep Learning",
    "section": "CNNs",
    "text": "CNNs\n\nKey point: Convolutional layers are not fully connected!\nEach layer ‚Äúpools‚Äù info from two units in previous layer"
  },
  {
    "objectID": "w10/slides.html#decoding-the-thought-vector",
    "href": "w10/slides.html#decoding-the-thought-vector",
    "title": "Week 10: Deep Learning",
    "section": "Decoding the Thought Vector",
    "text": "Decoding the Thought Vector\n\nHidden layers closer to input layer detect low-level ‚Äúfine-grained‚Äù features\nHidden layers closer to output layer detect high-level ‚Äúcoarse-grained‚Äù features\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding the Thought Vector"
  },
  {
    "objectID": "w10/slides.html#variational-autoencoders",
    "href": "w10/slides.html#variational-autoencoders",
    "title": "Week 10: Deep Learning",
    "section": "Variational Autoencoders",
    "text": "Variational Autoencoders"
  },
  {
    "objectID": "w10/slides.html#rnns",
    "href": "w10/slides.html#rnns",
    "title": "Week 10: Deep Learning",
    "section": "RNNs",
    "text": "RNNs\n‚Ä¶More next week, tbh\n\nISLR Figure 10.12"
  },
  {
    "objectID": "w10/slides.html#ok-but-how-do-we-learn-the-weights",
    "href": "w10/slides.html#ok-but-how-do-we-learn-the-weights",
    "title": "Week 10: Deep Learning",
    "section": "Ok But How Do We Learn The Weights?",
    "text": "Ok But How Do We Learn The Weights?\n\nBackpropagation! (3Blue1Brown Again!)(Full NN playlist here)"
  },
  {
    "objectID": "w10/slides.html#references",
    "href": "w10/slides.html#references",
    "title": "Week 10: Deep Learning",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w10/index.html",
    "href": "w10/index.html",
    "title": "Week 10: Deep Learning",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#quick-roadmap",
    "href": "w10/index.html#quick-roadmap",
    "title": "Week 10: Deep Learning",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nWe made it! Cutting-edge method for statistical neural learning",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#diagram-leftrightarrow-math",
    "href": "w10/index.html#diagram-leftrightarrow-math",
    "title": "Week 10: Deep Learning",
    "section": "Diagram \\(\\leftrightarrow\\) Math",
    "text": "Diagram \\(\\leftrightarrow\\) Math\n\n\n\n\\(p = 4\\) features in Input Layer\n\\(K = 5\\) Hidden Units\nOutput Layer: Regression on activations \\(a_k\\) (Hidden Unit outputs)\n\n\\[\n\\begin{align*}\n{\\color{#976464} y} &= { \\color{#976464} \\beta_0 } + {\\color{#666693} \\sum_{k=1}^{5} } {\\color{#976464} \\beta_k } { \\color{#666693} \\overbrace{\\boxed{a_k} }^{\\mathclap{k^\\text{th}\\text{ activation}}} } \\\\\n{\\color{#976464} y} &= { \\color{#976464} \\beta_0 } + {\\color{#666693} \\sum_{k=1}^{5} } {\\color{#976464} \\beta_k } { \\color{#666693} \\underbrace{ g \\mkern-4mu \\left( w_{k0} + {\\color{#679d67} \\sum_{j=1}^{4} } w_{kj} {\\color{#679d67} x_j} \\right) }_{k^\\text{th}\\text{ activation}}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#matrix-form-only-if-sanity-helping",
    "href": "w10/index.html#matrix-form-only-if-sanity-helping",
    "title": "Week 10: Deep Learning",
    "section": "Matrix Form (Only if Sanity-Helping)",
    "text": "Matrix Form (Only if Sanity-Helping)",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#example",
    "href": "w10/index.html#example",
    "title": "Week 10: Deep Learning",
    "section": "Example",
    "text": "Example\n\nRather than pondering over what that diagram can/can‚Äôt do, consider two ‚Äútrue‚Äù DGPs:\n\n\\[\n\\begin{align*}\nY &= {\\color{#e69f00} X_1 X_2 } \\\\\nY &= {\\color{#56b4e9} X_1^2 + X_2^2 } \\\\\nY &= {\\color{#009E73} X_1 \\underset{\\mathclap{\\small \\text{XOR}}}{\\oplus} X_2}\n\\end{align*}\n\\]\n\nHow exactly is a neural net able to learn these relationships?",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#sum-of-squares",
    "href": "w10/index.html#sum-of-squares",
    "title": "Week 10: Deep Learning",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\n\n\nCan we learn \\(y = {\\color{#56b4e9} x_1^2 + x_2^2 }\\)?\nLet‚Äôs use \\(g(x) = x^2\\).\nLet \\(\\mathbf{w}_1 = (0, 1, 0)\\), \\(\\mathbf{w}_2 = (0, 0, 1)\\).\nOur two activations are:\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n{\\color{#666693} a_1 } &= g(0 + (1)(x_1) + (0)(x_2)) = x_1^2 \\\\\n{\\color{#666693} a_2 } &= g(0 + (0)(x_1) + (1)(x_2)) = x_2^2\n\\end{align*}\n\\]\n\nSo, if \\(\\boldsymbol\\beta = (0, 1, 1)\\), then\n\n\\[\n{\\color{#976464} y } = 0 + (1)(x_1^2) + (1)(x_2^2) = {\\color{#56b4e9} x_1^2 + x_2^2} \\; ‚úÖ\n\\]",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#interaction-term",
    "href": "w10/index.html#interaction-term",
    "title": "Week 10: Deep Learning",
    "section": "Interaction Term",
    "text": "Interaction Term\n\n\n\nCan we learn \\(Y = {\\color{#e69f00} x_1x_2}\\)?\nLet‚Äôs use \\(g(x) = x^2\\) again.\nLet \\(\\mathbf{w}_1 = (0, 1, 1)\\), \\(\\mathbf{w}_2 = (0, 1, -1)\\).\nOur two activations are:\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n{\\color{#666693} a_1 } &= g(0 + (1)(x_1) + (1)(x_2)) = (x_1 + x_2)^2 = x_1^2 + x_2^2 +2x_1x_2 \\\\\n{\\color{#666693} a_2 } &= g(0 + (1)(x_1) + (-1)(x_2)) = (x_1 - x_2)^2 = x_1^2 + x_2^2 - 2x_1x_2\n\\end{align*}\n\\]\n\nSo, if we let \\(\\boldsymbol\\beta = \\left( 0, \\frac{1}{4}, -\\frac{1}{4} \\right)\\), then\n\n\\[\n{\\color{#976464} y } = 0 + \\left(\\frac{1}{4}\\right)(x_1^2 + x_2^2 + 2x_1x_2) + \\left(-\\frac{1}{4}\\right)(x_1^2 + x_2^2 - 2x_1x_2) = {\\color{#e69f00} x_1x_2} \\; ‚úÖ\n\\]",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#the-xor-problem",
    "href": "w10/index.html#the-xor-problem",
    "title": "Week 10: Deep Learning",
    "section": "The XOR Problem",
    "text": "The XOR Problem\n\n\n\nCan we learn \\(Y = {\\color{#009E73} x_1 \\underset{\\mathclap{\\small \\text{XOR}}}{\\oplus} x_2}\\)?\nLet‚Äôs use \\(g(x) = x^2\\) once more.\nLet \\(\\mathbf{w}_1 = (0, 1, 1)\\), \\(\\mathbf{w}_2 = (0, 1, -1)\\).\nOur two activations are:\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n{\\color{#666693} a_1 } &= g(0 + (1)(x_1) + (1)(x_2)) = (x_1 + x_2)^2 = x_1^2 + x_2^2 +2x_1x_2 \\\\\n{\\color{#666693} a_2 } &= g(0 + (1)(x_1) + (-1)(x_2)) = (x_1 - x_2)^2 = x_1^2 + x_2^2 - 2x_1x_2\n\\end{align*}\n\\]\n\nSo, if we let \\(\\boldsymbol\\beta = (0, 0, 1)\\), then\n\n\\[\n\\begin{align*}\n{\\color{#976464} y }(0,0) &= 0 + (0)(0^2 + 0^2 + 2(0)(0)) + (1)(0^2 + 0^2 - 2(0)(0)) = {\\color{#009e73} 0} \\; ‚úÖ \\\\\n{\\color{#976464} y }(0,1) &= 0 + (0)(0^2 + 1^2 + 2(0)(1)) + (1)(0^2 + 1^2 - 2(0)(1)) = {\\color{#009e73} 1} \\; ‚úÖ \\\\\n{\\color{#976464} y }(1,0) &= 0 + (0)(1^2 + 0^2 + 2(1)(0)) + (1)(1^2 + 0^2 - 2(1)(0)) = {\\color{#009e73} 1} \\; ‚úÖ \\\\\n{\\color{#976464} y }(1,1) &= 0 + (0)(1^2 + 1^2 + 2(1)(1)) + (1)(1^2 + 1^2 - 2(1)(1)) = {\\color{#009e73} 0} \\; ‚úÖ\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#but-how",
    "href": "w10/index.html#but-how",
    "title": "Week 10: Deep Learning",
    "section": "But How?",
    "text": "But How?\n\nOutput Layer is just linear regression on activations (Hidden Layer outputs)\nWe saw in Week 7 how good basis function allows regression to learn any function\nNeural Networks: GOAT non-linear basis function learners!\n\n\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nxor_df &lt;- tribble(\n    ~x1, ~x2, ~label,\n    0, 0, 0,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 0\n) |&gt;\nmutate(\n    h1 = (x1 - x2)^2,\n    label = factor(label)\n)\nxor_df |&gt; ggplot(aes(x=x1, y=x2, label=label)) +\n  geom_point(\n    aes(color=label, shape=label),\n    size=g_pointsize * 2,\n    stroke=6\n  ) +\n  geom_point(aes(fill=label), color='black', shape=21, size=g_pointsize * 2.5, stroke=0.75, alpha=0.4) +\n  scale_x_continuous(breaks=c(0, 1)) +\n  scale_y_continuous(breaks=c(0, 1)) +\n  expand_limits(y=c(-0.1,1.1)) +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=32) +\n  remove_legend_title() +\n  labs(\n    x=TeX(\"$x_1$\"),\n    y=TeX(\"$x_2$\"),\n    title=\"XOR Problem: Original Features\"\n  )\n\n\n\n\n\n\n\n\nFigure¬†1: The DGP \\(Y = x_1 \\oplus x_2\\) produces points in \\([0,1]^2\\) which are not linearly separable\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nxor_df &lt;- tribble(\n    ~x1, ~x2, ~label,\n    0, 0, 0,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 0\n) |&gt;\nmutate(\n    h1 = (x1 - x2)^2,\n    h2 = (x1 + x2)^2,\n    h2 = ifelse(h1 &gt; 0.5 & x2==0, h2 + 0.5, h2),\n    label = factor(label)\n)\nxor_df |&gt; ggplot(aes(x=h1, y=h2, label=label)) +\n  geom_vline(xintercept=0.5, linetype=\"dashed\", linewidth=1) +\n  # Negative space\n  geom_rect(xmin=-Inf, xmax=0.5, ymin=-Inf, ymax=Inf, fill=cb_palette[1], alpha=0.15) +\n  # Positive space\n  geom_rect(xmin=0.5, xmax=Inf, ymin=-Inf, ymax=Inf, fill=cb_palette[2], alpha=0.15) +\n  geom_point(\n    aes(color=label, shape=label),\n    size=g_pointsize * 2,\n    stroke=6\n  ) +\n  geom_point(aes(fill=label), color='black', shape=21, size=g_pointsize*2.5, stroke=0.75, alpha=0.4) +\n  expand_limits(y=c(-0.2,4.2)) +\n  # 45 is minus sign, 95 is em-dash\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=32) +\n  remove_legend_title() +\n  labs(\n    title=\"NN-Learned Feature Space\",\n    x=TeX(\"$h_1(x_1, x_2)$\"),\n    y=TeX(\"$h_2(x_1, x_2)$\")\n  )\n\n\n\n\n\n\n\n\nFigure¬†2: Learned bases \\(h_1 = (x_1 - x_2)^2\\) and \\(h_2 = (x_1 + x_2)^2\\) enable separating hyperplane \\(h_1 = 0.5\\)\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nx1_vals &lt;- seq(from=0, to=1, by=0.0075)\nx2_vals &lt;- seq(from=0, to=1, by=0.0075)\ngrid_df &lt;- expand.grid(x1=x1_vals, x2=x2_vals) |&gt;\n  as_tibble() |&gt;\n  mutate(\n    label=factor(as.numeric((x1-x2)^2 &gt; 0.5))\n  )\nggplot() +\n  geom_point(\n    data=grid_df,\n    aes(x=x1, y=x2, color=label),\n    alpha=0.4\n  ) +\n  geom_point(\n    data=xor_df,\n    aes(x=x1, y=x2, color=label, shape=label),\n    size=g_pointsize * 2,\n    stroke=6\n  ) +\n  geom_point(\n    data=xor_df,\n    aes(x=x1, y=x2, fill=label),\n    color='black', shape=21, size=g_pointsize*2.5, stroke=0.75, alpha=0.4\n  ) +\n  geom_abline(slope=1, intercept=0.7, linetype=\"dashed\", linewidth=1) +\n  geom_abline(slope=1, intercept=-0.7, linetype=\"dashed\", linewidth=1) +\n  scale_shape_manual(values=c(95, 43)) +\n  theme_dsan(base_size=32) +\n  remove_legend_title() +\n  labs(\n    title=\"XOR Problem: Inverted NN Features\",\n    x=TeX(\"$X_1$\"), y=TeX(\"$X_2$\")\n  )\n\n\n\n\n\n\n\n\nFigure¬†3: Here, the blue area represents points where \\(h_1 = (x_1 - x_2)^2 &gt; 0.5\\)",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#input-representation",
    "href": "w10/index.html#input-representation",
    "title": "Week 10: Deep Learning",
    "section": "Input Representation",
    "text": "Input Representation\n\n\n\n\n\nFrom But what is a neural network?, 3Blue1Brown",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#but-wait-ten-outputs",
    "href": "w10/index.html#but-wait-ten-outputs",
    "title": "Week 10: Deep Learning",
    "section": "But Wait‚Ä¶ Ten Outputs?",
    "text": "But Wait‚Ä¶ Ten Outputs?\n\n\n\n\n\n\n\n\n\nThe (magical) softmax function!\n\n\\[\nz_d = \\Pr(Y = d \\mid X) = \\frac{e^{y_d}}{\\sum_{i=0}^{9}e^{y_i}}\n\\]\n\nEnsures that each \\(Z_d\\) is a probability!\n\n\\[\n\\begin{align}\n0 \\leq z_d &\\leq 1 \\; \\; \\forall ~ d \\in \\{0,\\ldots,9\\} \\\\\n\\sum_{d=0}^{9}z_d &= 1\n\\end{align}\n\\]",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#visualizing-softmax-results",
    "href": "w10/index.html#visualizing-softmax-results",
    "title": "Week 10: Deep Learning",
    "section": "Visualizing Softmax Results",
    "text": "Visualizing Softmax Results\n\n\n\nInteractive Visualization: Handwritten-Digit Space",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#cnns",
    "href": "w10/index.html#cnns",
    "title": "Week 10: Deep Learning",
    "section": "CNNs",
    "text": "CNNs\n\nKey point: Convolutional layers are not fully connected!\nEach layer ‚Äúpools‚Äù info from two units in previous layer",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#decoding-the-thought-vector",
    "href": "w10/index.html#decoding-the-thought-vector",
    "title": "Week 10: Deep Learning",
    "section": "Decoding the Thought Vector",
    "text": "Decoding the Thought Vector\n\nHidden layers closer to input layer detect low-level ‚Äúfine-grained‚Äù features\nHidden layers closer to output layer detect high-level ‚Äúcoarse-grained‚Äù features\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding the Thought Vector",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#variational-autoencoders",
    "href": "w10/index.html#variational-autoencoders",
    "title": "Week 10: Deep Learning",
    "section": "Variational Autoencoders",
    "text": "Variational Autoencoders",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#rnns",
    "href": "w10/index.html#rnns",
    "title": "Week 10: Deep Learning",
    "section": "RNNs",
    "text": "RNNs\n‚Ä¶More next week, tbh\n\n\n\nISLR Figure 10.12",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#ok-but-how-do-we-learn-the-weights",
    "href": "w10/index.html#ok-but-how-do-we-learn-the-weights",
    "title": "Week 10: Deep Learning",
    "section": "Ok But How Do We Learn The Weights?",
    "text": "Ok But How Do We Learn The Weights?\n\n\n\nBackpropagation! (3Blue1Brown Again!)\n\n\n(Full NN playlist here)",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#references",
    "href": "w10/index.html#references",
    "title": "Week 10: Deep Learning",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-central-tool-of-data-science",
    "href": "writeups/regression-vs-pca/slides.html#the-central-tool-of-data-science",
    "title": "Regression vs.¬†PCA",
    "section": "The Central Tool of Data Science",
    "text": "The Central Tool of Data Science\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\small\\text{def}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-goal",
    "href": "writeups/regression-vs-pca/slides.html#the-goal",
    "title": "Regression vs.¬†PCA",
    "section": "The Goal",
    "text": "The Goal\n\nWhenever you carry out a regression, keep the goal in the front of your mind:\n\n\n\n\n\n\n\n\nThe Goal of Regression\n\n\nFind a line \\(\\widehat{y} = mx + b\\) that best predicts \\(Y\\) for given values of \\(X\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#how-do-we-define-best",
    "href": "writeups/regression-vs-pca/slides.html#how-do-we-define-best",
    "title": "Regression vs.¬†PCA",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#principal-component-analysis",
    "href": "writeups/regression-vs-pca/slides.html#principal-component-analysis",
    "title": "Regression vs.¬†PCA",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPrincipal Component Line can be used to project the data onto its dimension of highest variance\nMore simply: PCA can discover meaningful axes in data (unsupervised learning / exploratory data analysis settings)\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://juliasilge.com/blog/un-voting/ for an amazing blog post using PCA, with 2 dimensions, to explore UN voting patterns!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#create-your-own-dimension",
    "href": "writeups/regression-vs-pca/slides.html#create-your-own-dimension",
    "title": "Regression vs.¬†PCA",
    "section": "Create Your Own Dimension!",
    "text": "Create Your Own Dimension!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#and-use-it-for-eda",
    "href": "writeups/regression-vs-pca/slides.html#and-use-it-for-eda",
    "title": "Regression vs.¬†PCA",
    "section": "And Use It for EDA",
    "text": "And Use It for EDA"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#but-in-our-case",
    "href": "writeups/regression-vs-pca/slides.html#but-in-our-case",
    "title": "Regression vs.¬†PCA",
    "section": "But in Our Case‚Ä¶",
    "text": "But in Our Case‚Ä¶\n\n\\(x\\) and \\(y\\) dimensions already have meaning, and we have a hypothesis about \\(x \\rightarrow y\\)!\n\n\n\n\n\n\n\n\nThe Regression Hypothesis \\(\\mathcal{H}_{\\text{reg}}\\)\n\n\nGiven data \\((X, Y)\\), we estimate \\(\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1}x\\), hypothesizing that:\n\nStarting from \\(y = \\widehat{\\beta_0}\\) when \\(x = 0\\) (the intercept),\nAn increase of \\(x\\) by 1 unit is associated with an increase of \\(y\\) by \\(\\widehat{\\beta_1}\\) units (the coefficient)\n\n\n\n\n\n\nWe want to measure how well our line predicts \\(y\\) for any given \\(x\\) value \\(\\implies\\) vertical distance from regression line"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#key-features-of-regression-line",
    "href": "writeups/regression-vs-pca/slides.html#key-features-of-regression-line",
    "title": "Regression vs.¬†PCA",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-5mm] \\text{intercept}\\end{array}} + \\underbrace{\\widehat{\\beta_1}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-4mm] \\text{slope}\\end{array}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\theta = \\left(\\widehat{\\beta_0}, \\widehat{\\beta_1}\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(\\overbrace{\\widehat{y}(x_i)}^{\\small\\text{Predicted }y} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^2 \\right]\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#regression-in-r",
    "href": "writeups/regression-vs-pca/slides.html#regression-in-r",
    "title": "Regression vs.¬†PCA",
    "section": "Regression in R",
    "text": "Regression in R\n\n\nCode\nlin_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#lm-syntax",
    "href": "writeups/regression-vs-pca/slides.html#lm-syntax",
    "title": "Regression vs.¬†PCA",
    "section": "lm Syntax",
    "text": "lm Syntax\nlm(\n  formula = dependent ~ independent + controls,\n  data = my_df\n)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#interpreting-output",
    "href": "writeups/regression-vs-pca/slides.html#interpreting-output",
    "title": "Regression vs.¬†PCA",
    "section": "Interpreting Output",
    "text": "Interpreting Output\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#zooming-in-coefficients",
    "href": "writeups/regression-vs-pca/slides.html#zooming-in-coefficients",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#zooming-in-significance",
    "href": "writeups/regression-vs-pca/slides.html#zooming-in-significance",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-residual-plot",
    "href": "writeups/regression-vs-pca/slides.html#the-residual-plot",
    "title": "Regression vs.¬†PCA",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#q-q-plot",
    "href": "writeups/regression-vs-pca/slides.html#q-q-plot",
    "title": "Regression vs.¬†PCA",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#multiple-linear-regression",
    "href": "writeups/regression-vs-pca/slides.html#multiple-linear-regression",
    "title": "Regression vs.¬†PCA",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#references",
    "href": "writeups/regression-vs-pca/slides.html#references",
    "title": "Regression vs.¬†PCA",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html",
    "href": "writeups/regression-vs-pca/index.html",
    "title": "Regression vs.¬†PCA",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\small\\text{def}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nx_data &lt;- seq(from=0, to=1, by=0.02)\nnum_x &lt;- length(x_data)\ny_data &lt;- x_data + runif(num_x, 0, 0.2)\nreg_df &lt;- tibble(x=x_data, y=y_data)\nggplot(reg_df, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat \n\n\n\n\n\nCode\nggplot(reg_df, aes(x=x, y=y)) + \n  geom_point(size=g_pointsize) +\n  geom_smooth(method = \"lm\", se = FALSE, color = cbPalette[1], formula = y ~ x, linewidth = g_linewidth*3) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-central-tool-of-data-science",
    "href": "writeups/regression-vs-pca/index.html#the-central-tool-of-data-science",
    "title": "Regression vs.¬†PCA",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\small\\text{def}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nx_data &lt;- seq(from=0, to=1, by=0.02)\nnum_x &lt;- length(x_data)\ny_data &lt;- x_data + runif(num_x, 0, 0.2)\nreg_df &lt;- tibble(x=x_data, y=y_data)\nggplot(reg_df, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat \n\n\n\n\n\nCode\nggplot(reg_df, aes(x=x, y=y)) + \n  geom_point(size=g_pointsize) +\n  geom_smooth(method = \"lm\", se = FALSE, color = cbPalette[1], formula = y ~ x, linewidth = g_linewidth*3) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-goal",
    "href": "writeups/regression-vs-pca/index.html#the-goal",
    "title": "Regression vs.¬†PCA",
    "section": "The Goal",
    "text": "The Goal\n\nWhenever you carry out a regression, keep the goal in the front of your mind:\n\n\n\n\n\n\n\nThe Goal of Regression\n\n\n\nFind a line \\(\\widehat{y} = mx + b\\) that best predicts \\(Y\\) for given values of \\(X\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#how-do-we-define-best",
    "href": "writeups/regression-vs-pca/index.html#how-do-we-define-best",
    "title": "Regression vs.¬†PCA",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#principal-component-analysis",
    "href": "writeups/regression-vs-pca/index.html#principal-component-analysis",
    "title": "Regression vs.¬†PCA",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPrincipal Component Line can be used to project the data onto its dimension of highest variance\nMore simply: PCA can discover meaningful axes in data (unsupervised learning / exploratory data analysis settings)\n\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\n\ndist_to_line &lt;- function(x0, y0, a, c) {\n    numer &lt;- abs(a * x0 - y0 + c)\n    denom &lt;- sqrt(a * a + 1)\n    return(numer / denom)\n}\n# Finding PCA line for industrial vs. exports\nx &lt;- gdp_df$industrial\ny &lt;- gdp_df$exports\nlossFn &lt;- function(lineParams, x0, y0) {\n    a &lt;- lineParams[1]\n    c &lt;- lineParams[2]\n    return(sum(dist_to_line(x0, y0, a, c)))\n}\no &lt;- optim(c(0, 0), lossFn, x0 = x, y0 = y)\nggplot(gdp_df, aes(x = industrial, y = exports)) +\n    geom_point(size=g_pointsize/2) +\n    geom_abline(aes(slope = o$par[1], intercept = o$par[2], color=\"pca\"), linewidth=g_linewidth, show.legend = TRUE) +\n    geom_smooth(aes(color=\"lm\"), method = \"lm\", se = FALSE, linewidth=g_linewidth, key_glyph = \"blank\") +\n    scale_color_manual(element_blank(), values=c(\"pca\"=cbPalette[2],\"lm\"=cbPalette[1]), labels=c(\"Regression\",\"PCA\")) +\n    dsan_theme(\"half\") +\n    remove_legend_title() +\n    labs(\n      title = \"PCA Line vs. Regression Line\",\n        x = \"Industrial Production (% of GDP)\",\n        y = \"Exports (% of GDP)\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nSee https://juliasilge.com/blog/un-voting/ for an amazing blog post using PCA, with 2 dimensions, to explore UN voting patterns!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#create-your-own-dimension",
    "href": "writeups/regression-vs-pca/index.html#create-your-own-dimension",
    "title": "Regression vs.¬†PCA",
    "section": "Create Your Own Dimension!",
    "text": "Create Your Own Dimension!\n\n\nCode\nggplot(gdp_df, aes(pc1, .fittedPC2)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(aes(yintercept=0, color='PCA Line'), linetype='solid', size=g_linesize) +\n    geom_rug(sides = \"b\", linewidth=g_linewidth/1.2, length = unit(0.1, \"npc\"), color=cbPalette[3]) +\n    expand_limits(y=-1.6) +\n    scale_color_manual(element_blank(), values=c(\"PCA Line\"=cbPalette[2])) +\n    dsan_theme(\"full\") +\n    remove_legend_title() +\n    labs(\n      title = \"Exports vs. Industrial Production in Principal Component Space\",\n      x = \"First Principal Component (Dimension of Greatest Variance)\",\n      y = \"Second Principal Component\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#and-use-it-for-eda",
    "href": "writeups/regression-vs-pca/index.html#and-use-it-for-eda",
    "title": "Regression vs.¬†PCA",
    "section": "And Use It for EDA",
    "text": "And Use It for EDA\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nplot_df &lt;- gdp_df %&gt;% select(c(country_code, pc1, agriculture, military))\nlong_df &lt;- plot_df %&gt;% pivot_longer(!c(country_code, pc1), names_to = \"var\", values_to = \"val\")\nlong_df &lt;- long_df |&gt; mutate(\n  var = case_match(\n    var,\n    \"agriculture\" ~ \"Agricultural Production\",\n    \"military\" ~ \"Military Spending\"\n  )\n)\nggplot(long_df, aes(x = pc1, y = val, facet = var)) +\n    geom_point() +\n    facet_wrap(vars(var), scales = \"free\") +\n    dsan_theme(\"full\") +\n    labs(\n        x = \"Industrial-Export Dimension\",\n        y = \"% of GDP\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#but-in-our-case",
    "href": "writeups/regression-vs-pca/index.html#but-in-our-case",
    "title": "Regression vs.¬†PCA",
    "section": "But in Our Case‚Ä¶",
    "text": "But in Our Case‚Ä¶\n\n\\(x\\) and \\(y\\) dimensions already have meaning, and we have a hypothesis about \\(x \\rightarrow y\\)!\n\n\n\n\n\n\n\nThe Regression Hypothesis \\(\\mathcal{H}_{\\text{reg}}\\)\n\n\n\nGiven data \\((X, Y)\\), we estimate \\(\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1}x\\), hypothesizing that:\n\nStarting from \\(y = \\widehat{\\beta_0}\\) when \\(x = 0\\) (the intercept),\nAn increase of \\(x\\) by 1 unit is associated with an increase of \\(y\\) by \\(\\widehat{\\beta_1}\\) units (the coefficient)\n\n\n\n\nWe want to measure how well our line predicts \\(y\\) for any given \\(x\\) value \\(\\implies\\) vertical distance from regression line"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#key-features-of-regression-line",
    "href": "writeups/regression-vs-pca/index.html#key-features-of-regression-line",
    "title": "Regression vs.¬†PCA",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-5mm] \\text{intercept}\\end{array}} + \\underbrace{\\widehat{\\beta_1}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-4mm] \\text{slope}\\end{array}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\theta = \\left(\\widehat{\\beta_0}, \\widehat{\\beta_1}\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(\\overbrace{\\widehat{y}(x_i)}^{\\small\\text{Predicted }y} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^2 \\right]\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#regression-in-r",
    "href": "writeups/regression-vs-pca/index.html#regression-in-r",
    "title": "Regression vs.¬†PCA",
    "section": "Regression in R",
    "text": "Regression in R\n\n\nCode\nlin_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#lm-syntax",
    "href": "writeups/regression-vs-pca/index.html#lm-syntax",
    "title": "Regression vs.¬†PCA",
    "section": "lm Syntax",
    "text": "lm Syntax\nlm(\n  formula = dependent ~ independent + controls,\n  data = my_df\n)"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#interpreting-output",
    "href": "writeups/regression-vs-pca/index.html#interpreting-output",
    "title": "Regression vs.¬†PCA",
    "section": "Interpreting Output",
    "text": "Interpreting Output\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#zooming-in-coefficients",
    "href": "writeups/regression-vs-pca/index.html#zooming-in-coefficients",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#zooming-in-significance",
    "href": "writeups/regression-vs-pca/index.html#zooming-in-significance",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth) +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"dashed\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"solid\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"dashed\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-residual-plot",
    "href": "writeups/regression-vs-pca/index.html#the-residual-plot",
    "title": "Regression vs.¬†PCA",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(lin_model)\nggplot(gdp_resid_df, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Industrial ~ Military\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#q-q-plot",
    "href": "writeups/regression-vs-pca/index.html#q-q-plot",
    "title": "Regression vs.¬†PCA",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#multiple-linear-regression",
    "href": "writeups/regression-vs-pca/index.html#multiple-linear-regression",
    "title": "Regression vs.¬†PCA",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#references",
    "href": "writeups/regression-vs-pca/index.html#references",
    "title": "Regression vs.¬†PCA",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "writeups/3d-plots/index.html",
    "href": "writeups/3d-plots/index.html",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "",
    "text": "The main reason why I figured a writeup on plotting with Python could be helpful for DSAN 5300 specifically is because, even though many of yall are in DSAN 5200 right now, there are visualization techniques that will be helpful to have early-on in 5300‚Äîprimarily the challenges of plotting in 3D rather than 2D‚Äîwhich may not be covered until later in 5200.\nSo, therefore, the motivating example in this writeup will be plotting a 3D surface within which we‚Äôll want to overlay the path taken by gradient descent to move from a randomly-selected point on the surface to a (possibly local) minimum of the surface."
  },
  {
    "objectID": "writeups/3d-plots/index.html#the-goal-3d-plots",
    "href": "writeups/3d-plots/index.html#the-goal-3d-plots",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "",
    "text": "The main reason why I figured a writeup on plotting with Python could be helpful for DSAN 5300 specifically is because, even though many of yall are in DSAN 5200 right now, there are visualization techniques that will be helpful to have early-on in 5300‚Äîprimarily the challenges of plotting in 3D rather than 2D‚Äîwhich may not be covered until later in 5200.\nSo, therefore, the motivating example in this writeup will be plotting a 3D surface within which we‚Äôll want to overlay the path taken by gradient descent to move from a randomly-selected point on the surface to a (possibly local) minimum of the surface."
  },
  {
    "objectID": "writeups/3d-plots/index.html#no-3d-plotting-in-seaborn-matplotlib-to-the-rescue",
    "href": "writeups/3d-plots/index.html#no-3d-plotting-in-seaborn-matplotlib-to-the-rescue",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "No 3D Plotting in Seaborn üò¢ Matplotlib to the Rescue!",
    "text": "No 3D Plotting in Seaborn üò¢ Matplotlib to the Rescue!\nSadly, even though Seaborn is typically my go-to for the types of visualizations I find myself needing to produce in Data Science contexts, it has little to no support for 3D plotting specifically. Which‚Ä¶ makes some sense, given that (as you‚Äôll see in 5200) in many visual-communication scenarios there is usually a way to use 2D plots to achieve a more easily-interpretable set of visualizations for your audience!\nBut, nonetheless, in this class we have run into one of the key cases (understanding gradient descent beyond single-variable loss functions) where I think we can truly benefit from having 3D plots. So, with that said, a link you can bookmark and explore to get a feel for 3D visualization in Python is Matplotlib‚Äôs plot-type: 3D tag, which gathers together all of the examples within Matplotlib‚Äôs documentation which involve generating 3D plots:\nhttps://matplotlib.org/stable/_tags/plot-type-3d.html"
  },
  {
    "objectID": "writeups/3d-plots/index.html#your-first-3d-plot-in-matplotlib",
    "href": "writeups/3d-plots/index.html#your-first-3d-plot-in-matplotlib",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "Your First 3D Plot in Matplotlib",
    "text": "Your First 3D Plot in Matplotlib\nTo see your first 3D plot created with matplotlib, let‚Äôs consider how gradient descent might allow us to choose a random point on the (visible) surface of this plot (the green dot), and eventually make our way to the optimal loss-minimizing value (the light blue dot at \\((x,y) = (0,0)\\)):\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nrng = np.random.default_rng(seed=5302)\nfrom matplotlib import cm\n\ncb_palette = ['#E69F00', '#56B4E9', '#009E73']\n\ndef loss_fn(x, y):\n  return x**2 + y**2\n\ndef objective_fn(x, y):\n  return -loss_fn(x, y)\n\nrand_v = np.array([\n  rng.uniform(low=-4, high=0),\n  rng.uniform(low=0, high=4),\n])\n\ndef gen_surface_plot(\n  points,\n  title=None,\n  minimization=True,\n  label_points=False,\n  arrows=False,\n  elevation=42,\n  azimuth=-60\n):\n  if title is None:\n    title = \"Minimization of a Loss Function\"\n    if not minimization:\n      title = \"Maximization of an Objective Function\"\n  opt_fn = loss_fn if minimization else objective_fn\n  fig, ax = plt.subplots(\n    subplot_kw={\n      \"projection\": \"3d\",\n      \"title\": title,\n      \"computed_zorder\": False,\n    }\n  )\n  # Generate the surface representing the value of\n  # the loss function (the z coordinate) for any\n  # pair of (x,y) values\n  x_range = np.arange(-5, 5, 0.25)\n  y_range = np.arange(-5, 5, 0.25)\n  x_vals, y_vals = np.meshgrid(x_range, y_range)\n  z_vals = opt_fn(x_vals, y_vals)\n\n  # Plot the points given by the points argument!\n  points_x = [p[0] for p in points]\n  points_y = [p[1] for p in points]\n  points_z = [opt_fn(p[0],p[1]) for p in points]\n  ax.scatter(points_x, points_y, points_z, color=cb_palette[2], s=80, zorder=10)\n\n  # Add labels to points if label_points is True\n  if label_points:\n    for point_index, point in enumerate(points):\n      ax.text(point[0], point[1], loss_fn(point[0], point[1]), str(point_index), zorder=20)\n\n  # Compute and plot the optimal value\n  opt_z = np.min(z_vals) if minimization else np.max(z_vals)\n  ax.scatter([0], [0], [opt_z], color=cb_palette[1], s=80, zorder=10)\n  surf = ax.plot_surface(\n      x_vals, y_vals, z_vals, cmap='magma', zorder=0, alpha=0.8\n  )\n  ax.set_xlabel(\"x\")\n  ax.set_ylabel(\"y\")\n  \n  ax.view_init(elev=elevation, azim=azimuth)\n\n  # Add arrows if requested\n  if arrows:\n    for i in range(len(points) - 1):\n      cur_point = points[i]\n      cur_z = loss_fn(cur_point[0], cur_point[1])\n      next_point = points[i+1]\n      next_z = loss_fn(next_point[0], next_point[1])\n      x_diff = next_point[0] - cur_point[0]\n      y_diff = next_point[1] - cur_point[1]\n      z_diff = next_z - cur_z\n      ax.quiver(\n        cur_point[0], cur_point[1], cur_z,\n        x_diff, y_diff, z_diff,\n        color = 'white', alpha = .8, lw = 1,\n        length=1\n      )\n\n  # Add a color bar which maps values to colors.\n  fig.colorbar(surf, shrink=0.5, aspect=5)\n  plt.show()\ngen_surface_plot(points=[rand_v])\n\n\n\n\n\n\n\n\n\nAlso note how I made the plot-generation code into a function, here called gen_surface_plot(), which can be immensely helpful for your own plotting adventures, since you can utilize arguments to the functions to quickly change different aspects of the plot. For example, I set this function up to allow ‚Äúinstant‚Äù switching from visualization of minimization to visualization of a maximization problem, which means we can just pass minimization=False to the function to generate a plot of what finding the maximum value of a function (the same function, just ‚Äúflipped over‚Äù via \\(z \\mapsto -z\\)):\n\ngen_surface_plot(points=[rand_v], minimization=False)"
  },
  {
    "objectID": "writeups/3d-plots/index.html#plotting-the-gradient-descent-path",
    "href": "writeups/3d-plots/index.html#plotting-the-gradient-descent-path",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "Plotting the Gradient Descent Path",
    "text": "Plotting the Gradient Descent Path\n\nThe Straightforward First Step: Plotting Points\nReturning to our minimization-in-3D case, how can we now plot the path/trajectory that we follow if we apply gradient descent starting from the randomly-chosen (green) point? One easy way is to just keep track of each new value in a list (here, we call it grad_path), and then plot each element in this list as a point along our gradient-descent pathway:\n\n\nCode\ndef grad_loss(x, y):\n  return np.array([2*x, 2*y])\n\ndef run_grad_descent(starting_v, num_steps, step_size=0.1):\n  cur_v = starting_v\n  path = [starting_v]\n  for i in range(num_steps):\n    grad_at_v = grad_loss(cur_v[0], cur_v[1])\n    new_v = cur_v - step_size * grad_at_v\n    path.append(new_v)\n    cur_v = new_v\n  return path\nnum_steps = 5\ngrad_path = run_grad_descent(rand_v, num_steps)\ngen_surface_plot(grad_path, title=f\"Gradient Descent, {num_steps} steps\")\n\n\n\n\n\n\n\n\n\nEven though it may be obvious in this case the order of the points (meaning, you can just ‚Äúeyeball‚Äù which was the starting point, the second point, the third, and so on), oftentimes it can help to label the order of the points. So, the function takes an optional label_points argument, which shows the order of the points along the gradient path if set to True (for more on how the text() function works in 3D world, see here):\n\n\nCode\ngen_surface_plot(grad_path, title=f\"Gradient Descent, {num_steps} steps\", label_points=True)\n\n\n\n\n\n\n\n\n\nAnd now we can get a sense for how many steps we‚Äôll need for convergence in this case (with a step size set to be \\(0.1\\)), by extending this plot to show the trajectory for 10 steps rather than 5:\n\n\nCode\nnum_steps = 10\ngrad_path_10 = run_grad_descent(rand_v, num_steps)\ngen_surface_plot(grad_path_10, title=f\"Gradient Descent, {num_steps} steps\", label_points=True)\n\n\n\n\n\n\n\n\n\n\n\nThe Less-Straightforward Second Step: Arrows\nI‚Äôm not gonna lie to you‚Ä¶ by the time you are trying to add arrows to your matplotlib plots, you have somewhat hit the limits of the (very basic!) functionality of matplotlib. It is possible, and supported by way of the quiver() function, but personally I‚Äôve never been able to get the arrows to actually look good, which is why at this point I might recommend switching over from matplotlib to something like Plotly for this task. But, since we‚Äôve already produced the above plot, we may as well see what it looks like to just add the arrows onto the plot using ax.quiver().\n\n\nCode\ngen_surface_plot(\n  grad_path_10,\n  title=\"Adding arrows with ax.quiver()\",\n  label_points=True,\n  arrows=True\n)"
  },
  {
    "objectID": "writeups/3d-plots/index.html#moving-the-camera",
    "href": "writeups/3d-plots/index.html#moving-the-camera",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "Moving the ‚ÄúCamera‚Äù",
    "text": "Moving the ‚ÄúCamera‚Äù\nLastly, finally, since this trajectory is somewhat difficult to see from the default angle, you can use the elevation and azimuth options to the ax.view_init() function to shift the ‚Äúcamera‚Äù around! Here we‚Äôll try to see the pathway a little better by lowering its elevation a bit and rotating it to look at this trajectory more ‚Äúfrom the side‚Äù than head-on:\n\n\nCode\ngen_surface_plot(\n  grad_path_10,\n  title=f\"Gradient Descent, {num_steps} steps\",\n  label_points=True,\n  arrows=True,\n  elevation=39,\n  azimuth=-98\n)\n\n\n\n\n\n\n\n\n\nSo, if you find yourself extremely stuck at some point where 3D visualization of what‚Äôs going on could help, I hope you are able to use this gen_surface_plot() function with the different options to maybe get a glimpse of e.g.¬†the trajectory that some numerical optimization algorithm is taking with respect to some loss surface!"
  },
  {
    "objectID": "writeups/hw2-guide/index.html",
    "href": "writeups/hw2-guide/index.html",
    "title": "Getting Started with HW 2",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nOriginal version posted 10 Feb 2025, 10:00pm"
  },
  {
    "objectID": "writeups/hw2-guide/index.html#full-text-for-hw-2.5-islr-6.8-8a-d",
    "href": "writeups/hw2-guide/index.html#full-text-for-hw-2.5-islr-6.8-8a-d",
    "title": "Getting Started with HW 2",
    "section": "Full Text for HW-2.5: ISLR-6.8 #8(a-d)",
    "text": "Full Text for HW-2.5: ISLR-6.8 #8(a-d)\nHere, for ease of access (since the problem in this case is from the previous edition of ISLR, pdf here‚Äîthank you Prof.¬†James for the PDF link!), is the full text of Section 6.8 Problem 8. Remember that you only need to do (a) through (d)! The full problem is here just for completeness (you can think about how you‚Äôd approach parts (e) and (f)).\n\n\n\n\n\n\nISLR Section 6.8, Exercise #8\n\n\n\nIn this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n\nUse the rnorm() function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\epsilon\\) of length \\(n = 100\\).\nGenerate a response vector \\(Y\\) of length \\(n = 100\\) according to the model\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon,\n\\]\nwhere \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) are constants of your choice.\nUse the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X, X^2, \\ldots, X^{10}\\). What is the best model obtained according to \\(C_p\\), BIC, and adjusted \\(R^2\\)? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\nRepeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\nNow fit a lasso model to the simulated data, again using \\(X, X^2, \\ldots, X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\nNow generate a response vector \\(Y\\) according to the model\n\\[\nY = \\beta_0 + \\beta_7 X^7 + \\epsilon,\n\\]\nand perform best subset selection and the lasso. Discuss the results obtained."
  },
  {
    "objectID": "writeups/optimization/index.html",
    "href": "writeups/optimization/index.html",
    "title": "Mathematical Optimization",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\small\\text{def}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nAs we entered the unit on Parameter Estimation, encompassing both Maximum Likelihood Estimation (MLE) and Generalized Method of Moments (GMM) Estimation, a bunch of students helpfully pointed out how the mathematical content of the course suddenly kind of ‚Äúshifted into high gear‚Äù.\nSo, although it won‚Äôt help in terms of the Quizzes and Labs you‚Äôve already completed on these topics, I think it is helpful to return to the mathematical aspects of those units, since they will become fairly central to the problems you‚Äôll encounter in DSAN 5300: Statistical Learning, in the Spring semester.\nMy goals here are:\n\nTo start specifically with the Maximum Likelihood Estimation approach, going more slowly through some example problems emphasizing how MLE is rooted in optimization of an objective function, and then\nTo introduce constrained optimization via the Lagrange Multiplier approach, as the ‚Äúnext step‚Äù once you feel comfortable with MLE, again going through example problems that can then also serve as preparation for DSAN 5300!\n\nThe second bullet point is why I also mentioned GMM Estimation above: it turns out that, whereas MLE is an optimization problem without constraints, GMM can essentially be written as an optimization problem with only constraints.\nAs a preview of what this means, first assume we have:\n\nA vector-valued Random Variable \\(\\mathfrak{X}\\),\nAn \\(N\\)-dimensional vector \\(\\mathbf{x} = (x_1, \\ldots, x_N)\\), our dataset (a realization of \\(\\mathfrak{X}\\)),\nA vector of \\(J\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\ldots, \\theta_J)\\),\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, it has \\(J = 2\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\)\n\nA likelihood function \\(\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta})\\).\n\nYou can compare MLE for this scenario, written in the form of an optimization problem:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta}) \\\\\n&& \\text{s.t.} \\quad &\\varnothing\n\\end{alignat}\n\\]\nwith GMM estimation for this scenario written in the same form:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\varnothing \\\\\n&& \\text{s.t.} \\quad & \\mu_1(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_1(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\mu_2(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_2(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\phantom{\\mu_1(\\param{\\boldsymbol\\theta})} ~ \\vdots \\\\\n&& \\quad & \\mu_J(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_J(\\mathbf{x}, \\param{\\boldsymbol\\theta}),\n\\end{alignat}\n\\]\nwhere:\n\n\\(\\mu_i(\\param{\\boldsymbol\\theta})\\) is the \\(i\\)th theoretical moment of \\(\\mathfrak{X}\\)\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, \\(\\mu_1(\\theta_1, \\theta_2) = \\mathbb{E}[\\mathfrak{X}]\\)\n\n\\(\\widehat{\\mu}_i(\\mathbf{x}, \\param{\\boldsymbol\\theta})\\) is the \\(i\\)th empirical moment of \\(\\mathfrak{X}\\)\n\nFor example, since \\(\\mu_1 = \\mathbb{E}[\\mathfrak{X}]\\), \\(\\widehat{\\mu_1}\\) is the sample ‚Äúversion‚Äù of \\(\\mathbb{E}[\\mathfrak{X}]\\), namely, \\(\\widehat{\\mu}_1 = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nIn both cases, the symbol \\(\\varnothing\\) literally just means ‚Äúnothing‚Äù: in the MLE case, we have no constraints, whereas in the GMM estimation case, we have no objective function.\nFor readability, \\(\\text{s.t.}\\) is just shorthand for ‚Äúsubject to‚Äù, or sometimes ‚Äúsuch that‚Äù. This means that the full expression in general can be read like ‚Äú\\(\\param{\\boldsymbol\\theta}^*\\) is the maximum of ____, subject to _____‚Äù"
  },
  {
    "objectID": "writeups/optimization/index.html#motivation-a-closer-look-at-parameter-estimation",
    "href": "writeups/optimization/index.html#motivation-a-closer-look-at-parameter-estimation",
    "title": "Mathematical Optimization",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\small\\text{def}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nAs we entered the unit on Parameter Estimation, encompassing both Maximum Likelihood Estimation (MLE) and Generalized Method of Moments (GMM) Estimation, a bunch of students helpfully pointed out how the mathematical content of the course suddenly kind of ‚Äúshifted into high gear‚Äù.\nSo, although it won‚Äôt help in terms of the Quizzes and Labs you‚Äôve already completed on these topics, I think it is helpful to return to the mathematical aspects of those units, since they will become fairly central to the problems you‚Äôll encounter in DSAN 5300: Statistical Learning, in the Spring semester.\nMy goals here are:\n\nTo start specifically with the Maximum Likelihood Estimation approach, going more slowly through some example problems emphasizing how MLE is rooted in optimization of an objective function, and then\nTo introduce constrained optimization via the Lagrange Multiplier approach, as the ‚Äúnext step‚Äù once you feel comfortable with MLE, again going through example problems that can then also serve as preparation for DSAN 5300!\n\nThe second bullet point is why I also mentioned GMM Estimation above: it turns out that, whereas MLE is an optimization problem without constraints, GMM can essentially be written as an optimization problem with only constraints.\nAs a preview of what this means, first assume we have:\n\nA vector-valued Random Variable \\(\\mathfrak{X}\\),\nAn \\(N\\)-dimensional vector \\(\\mathbf{x} = (x_1, \\ldots, x_N)\\), our dataset (a realization of \\(\\mathfrak{X}\\)),\nA vector of \\(J\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\ldots, \\theta_J)\\),\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, it has \\(J = 2\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\)\n\nA likelihood function \\(\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta})\\).\n\nYou can compare MLE for this scenario, written in the form of an optimization problem:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta}) \\\\\n&& \\text{s.t.} \\quad &\\varnothing\n\\end{alignat}\n\\]\nwith GMM estimation for this scenario written in the same form:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\varnothing \\\\\n&& \\text{s.t.} \\quad & \\mu_1(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_1(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\mu_2(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_2(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\phantom{\\mu_1(\\param{\\boldsymbol\\theta})} ~ \\vdots \\\\\n&& \\quad & \\mu_J(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_J(\\mathbf{x}, \\param{\\boldsymbol\\theta}),\n\\end{alignat}\n\\]\nwhere:\n\n\\(\\mu_i(\\param{\\boldsymbol\\theta})\\) is the \\(i\\)th theoretical moment of \\(\\mathfrak{X}\\)\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, \\(\\mu_1(\\theta_1, \\theta_2) = \\mathbb{E}[\\mathfrak{X}]\\)\n\n\\(\\widehat{\\mu}_i(\\mathbf{x}, \\param{\\boldsymbol\\theta})\\) is the \\(i\\)th empirical moment of \\(\\mathfrak{X}\\)\n\nFor example, since \\(\\mu_1 = \\mathbb{E}[\\mathfrak{X}]\\), \\(\\widehat{\\mu_1}\\) is the sample ‚Äúversion‚Äù of \\(\\mathbb{E}[\\mathfrak{X}]\\), namely, \\(\\widehat{\\mu}_1 = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nIn both cases, the symbol \\(\\varnothing\\) literally just means ‚Äúnothing‚Äù: in the MLE case, we have no constraints, whereas in the GMM estimation case, we have no objective function.\nFor readability, \\(\\text{s.t.}\\) is just shorthand for ‚Äúsubject to‚Äù, or sometimes ‚Äúsuch that‚Äù. This means that the full expression in general can be read like ‚Äú\\(\\param{\\boldsymbol\\theta}^*\\) is the maximum of ____, subject to _____‚Äù"
  },
  {
    "objectID": "writeups/optimization/index.html#visualizing-unconstrained-and-constrained-optimization",
    "href": "writeups/optimization/index.html#visualizing-unconstrained-and-constrained-optimization",
    "title": "Mathematical Optimization",
    "section": "Visualizing Unconstrained and Constrained Optimization",
    "text": "Visualizing Unconstrained and Constrained Optimization\nThe unconstrained optimizations we will carry out here can be visualized as ‚Äúhill climbing‚Äù: the optimization approach you probably learned in calculus class‚Äîcomputing the derivative and setting it equal to zero‚Äîworks precisely because the top of the ‚Äúhill‚Äù is the point at which the derivative becomes zero. For example, if we started climbing from the left side of the following plot, the derivative would get lower and lower as we moved right, hitting zero when we reach the top of the ‚Äúhill‚Äù:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nmy_hill &lt;- function(x) exp(-(1/2)*x^2)\nmy_slope &lt;- function(x) -x*exp(-(1/2)*x^2)\nx0_vals &lt;- c(-1.75, -0.333, 0)\ntangent_at_x0 &lt;- function(x,x0) my_slope(x0)*(x - x0) + my_hill(x0)\ntan_x1 &lt;- function(x) tangent_at_x0(x, x0_vals[1])\ntan_x2 &lt;- function(x) tangent_at_x0(x, x0_vals[2])\ntan_x3 &lt;- function(x) tangent_at_x0(x, x0_vals[3])\neval_df &lt;- tibble(x=x0_vals) |&gt; mutate(y=my_hill(x))\ntan_ext &lt;- 0.5\nslopes &lt;- round(c(\n  my_slope(x0_vals[1]),\n  my_slope(x0_vals[2]),\n  my_slope(x0_vals[3])\n), 3)\nopt_df &lt;- tibble(x=0, y=my_hill(0))\nx_df &lt;- tibble(x=c(-3, 3))\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=my_hill, linewidth=1) +\n  geom_function(\n    fun=tan_x1, aes(color=\"x1\"), linewidth=1,\n    xlim=c(\n      x0_vals[1]-tan_ext,\n      x0_vals[1]+tan_ext\n    )\n  ) +\n  geom_function(\n    fun=tan_x2, aes(color=\"x2\"), linewidth=1,\n    xlim=c(\n      x0_vals[2]-tan_ext,\n      x0_vals[2]+tan_ext\n    )\n  ) +\n  geom_function(\n    fun=tan_x3, aes(color=\"x3\"), linewidth=1,\n    xlim=c(\n      -1,1\n    )\n  ) +\n  geom_point(\n    data=eval_df,\n    aes(x=x, y=y, color=c(\"x1\",\"x2\",\"x3\")),\n    size=2\n  ) +\n  geom_point(\n    data=opt_df,\n    aes(x=x, y=y, shape=\"mle\"),\n    size=2\n  ) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"MLE Estimate\"\n  ) +\n  scale_color_manual(\n    \"Slope at x\",\n    values=c(cb_palette[1], cb_palette[2], cb_palette[3]),\n    labels=slopes\n  ) +\n  theme_classic(base_size=18) +\n  ylim(c(0, 1.15))\n\n\n\n\n\nThe derivatives ‚Äúpoint‚Äù in the direction you should go to get to the maximum value, at which it has value zero\n\n\n\n\nUsing this same metaphor of the top of a hill being the ‚Äúbest‚Äù point, constrained optimization is a bit harder to visualize, but you can think of it like:\n\nGetting (almost) crunched between two walls coming closer together, then\nGetting (almost) crunched between a floor and a ceiling coming closer together,\n\nwhere that coming-closer-together is set up so as to crunch the space, making it smaller and smaller until the only point(s) left are the point(s) representing the optimal solution:\n\n\nCode\nxl &lt;- -0.06\nxu &lt;- 0.06\nyl &lt;- 0.985\nyu &lt;- 1.015\nanno_x &lt;- -1.8\nanno_y &lt;- 0.2\nx_df &lt;- tibble(x=c(-3, 3))\nrib_df &lt;- x_df |&gt; mutate(\n  ymin = -0.1,\n  ymax = 0.1\n)\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=my_hill, linewidth=1) +\n  geom_rect(\n    aes(fill=\"x\"), xmin=xl, xmax=xu, ymin=-Inf, ymax=Inf, alpha=0.5, color='black'\n  ) +\n  geom_segment(x=xl, xend=xl, y=-Inf, yend=Inf) +\n  geom_segment(x=xu, xend=xu, y=-Inf, yend=Inf) +\n  geom_rect(\n    aes(fill=\"y\"), xmin=-Inf, xmax=Inf, ymin=yl, ymax=yu, alpha=0.5, color='black'\n  ) +\n  geom_segment(x=-Inf, xend=Inf, y=yl, yend=yl) +\n  geom_segment(x=-Inf, xend=Inf, y=yu, yend=yu) +\n  # Anno xmin\n  annotate(\n    \"segment\", y=anno_y, yend=anno_y, x = xl-0.5, xend = xl-0.04,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", y=anno_y, x=xl-1.2, label = \"x &gt; a\", color = \"black\",\n    angle = 0, hjust = 0, vjust=0.5, size = 5\n  ) +\n  # Anno xmax\n  annotate(\n    \"segment\", y=anno_y, yend=anno_y, x = xu+0.5, xend = xu+0.04,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", y=anno_y, x=xu+0.6, label = \"x &lt; b\", color = \"black\",\n    angle = 0, hjust = 0, vjust=0.5, size = 5\n  ) +\n  # Anno ymin\n  annotate(\n    \"segment\", x=anno_x, xend=anno_x, y = yl-0.2, yend = yl-0.015,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", x=anno_x, y=yl-0.25, label = \"y &gt; c\", color = \"black\",\n    angle = 0, hjust = 0.5, vjust=0.5, size = 5\n  ) +\n  # Anno ymax\n  annotate(\n    \"segment\", x=anno_x, xend=anno_x, y = yu+0.2, yend = yu+0.015,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", x=anno_x, y=yu+0.2, label = \"y &lt; d\", color = \"black\",\n    angle = 0, hjust = 0.5, vjust=-0.5, size = 5\n  ) +\n  scale_fill_manual(\n    \"Constraints:\",\n    values=c(cb_palette[1], cb_palette[2]),\n    labels=c(\"a &lt; x &lt; b\", \"c &lt; y &lt; d\")\n  ) +\n  geom_point(\n    data=opt_df,\n    aes(x=x, y=y, shape=\"gmm\"),\n    size=2\n  ) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"GMM Estimate\"\n  ) +\n  theme_classic(base_size=18) +\n  ylim(c(0, 1.5))\n\n\n\n\n\nThe constraints ‚Äúcrunch‚Äù the admissible values of \\(x\\) until only one point (the Method of Moments estimate) is left. The metaphor is a bit more contrived in this case, however, since we usually don‚Äôt manually compute these constraints in exactly this form (they are implicit in the GMM‚Äôs system of equations), though numerically (i.e., if we use R to compute the GMM estimate), this is exactly how the constraints would be applied!\n\n\n\n\nLet‚Äôs look at how we can work through both MLE and GMM estimation (which we previously learned separately!) as optimization problems, and how that will give us a unified optimization framework for estimating parameters in the fancier ML models you‚Äôll see in DSAN 5300!"
  },
  {
    "objectID": "writeups/optimization/index.html#general-unconstrained-optimization",
    "href": "writeups/optimization/index.html#general-unconstrained-optimization",
    "title": "Mathematical Optimization",
    "section": "General Unconstrained Optimization",
    "text": "General Unconstrained Optimization\nAs the name implies, Maximum likelihood estimation involves maximizing something:\n\nIn MLE, that thing is the Likelihood Function \\(\\mathcal{L}(X \\mid \\theta)\\).\nIn optimization, more generally, that thing is called the objective function \\(f(x, \\theta)\\).\n\nThis is the reasoning behind something I might have mentioned in class in earlier weeks, that ‚Äúthe objective function in MLE is the likelihood function‚Äù.\nIn other words, when you see the term ‚Äúobjective function‚Äù, just replace it in your brain with ‚Äúthing I‚Äôm trying to optimize (minimize or maximize) here‚Äù.1\nBefore we look at MLE specifically, therefore, let‚Äôs look at a general unconstrained optimization problem, something you almost surely have already solved tons of times (even if you didn‚Äôt have the vocabulary of optimization, objective functions, constraints, and so on):\n\n\n\n\n\n\nExample 1: General Unconstrained Optimization\n\n\n\nFind \\(x^*\\), the solution to\n\\[\n\\begin{align}\n    \\min_{x} ~ & f(x) = 3x^2 - x \\\\\n    \\text{s.t. } ~ & \\varnothing\n\\end{align}\n\\]\n\n\nFrom calculus, we know that finding the optimum value for a function like this (whether minimum or maximum) boils down to:\n\nComputing the derivative \\(f'(x) = \\frac{\\partial}{\\partial x}f(x)\\),\nSetting it equal to zero: \\(f'(x) = 0\\), and\nSolving this equal for \\(x\\), i.e., finding values \\(x^*\\) which satisfy \\(f'(x^*) = 0\\)\n\nHere, without any constraints, we can follow this exact procedure to find the minimum value. We start by computing the derivative:\n\\[\nf'(x) = \\frac{\\partial}{\\partial x}f(x) = \\frac{\\partial}{\\partial x}\\left[3x^2 - x\\right] = 6x - 1,\n\\]\nthen solve for \\(x^*\\) as the value(s) satisfying \\(\\frac{\\partial}{\\partial x}f'(x^*) = 0\\) for the just-derived \\(f'(x)\\):\n\\[\nf'(x^*) = 0 \\iff 6x^* - 1 = 0 \\iff x^* = \\frac{1}{6}.\n\\]\n\n\n\n\n\n\nDerivative Cheatsheet (Click to Collapse / Expand)\n\n\n\n\n\n(These green boxes are where I get to pop off a tiny bit, but in a way that I hope is helpful! üòú)\nPersonally, I absolutely hate, despise memorizing things. So much of school growing up felt like memorizing a ton of things for no reason, since they were things we could just Google 90% of the time‚Ä¶\nSo, even though people usually associate math with memorization of formulas, for whatever reason I have the opposite association: math was the one class where I didn‚Äôt have to memorize things, because (unlike‚Ä¶ ‚Äúthe mitochondria is the powerhouse of the cell‚Äù) I had really good teachers who always walked us through how to derive things from more basic principles.\nSo, I‚Äôm providing this here as a small set of ‚Äúshortcuts‚Äù, but long story short each of these can be derived from even simpler procedures (I don‚Äôt want to clutter this writeup even more by writing those out, but I‚Äôm happy to walk you through how you could derive these, in office hours for example!)\n\n\n\n\n\n\n\n\nType of Thing\nThing\nChange in Thing when \\(x\\) Changes by Tiny Amount\n\n\n\n\nPolynomial\n\\(f(x) = x^n\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x}f(x) = nx^{n-1}\\)\n\n\nFraction\n\\(f(x) = \\frac{1}{x}\\)\nUse Polynomial rule (since \\(\\frac{1}{x} = x^{-1}\\)) to get \\(f'(x) = -\\frac{1}{x^2}\\)\n\n\nLogarithm\n\\(f(x) = \\ln(x)\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x} = \\frac{1}{x}\\)\n\n\nExponential\n\\(f(x) = e^x\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x}e^x = e^x\\) (üßê‚ùóÔ∏è)\n\n\nMultiplication\n\\(f(x) = g(x)h(x)\\)\n\\(f'(x) = g'(x)h(x) + g(x)h'(x)\\)\n\n\nDivision\n\\(f(x) = \\frac{g(x)}{h(x)}\\)\nToo hard to memorize‚Ä¶ turn it into Multiplication, as \\(f(x) = g(x)(h(x))^{-1}\\)\n\n\nComposition (Chain Rule)\n\\(f(x) = g(h(x))\\)\n\\(f'(x) = g'(h(x))h'(x)\\)\n\n\nFancy Logarithm\n\\(f(x) = \\ln(g(x))\\)\n\\(f'(x) = \\frac{g'(x)}{g(x)}\\) by Chain Rule\n\n\nFancy Exponential\n\\(f(x) = e^{g(x)}\\)\n\\(f'(x) = g'(x)e^{g(x)}\\) by Chain Rule\n\n\n\n\n\n\nThe reason why derivatives are so important for optimization is that we‚Äôre trying to climb a hill (for maximization; if we‚Äôre minimizing then we‚Äôre trying to reach the bottom of a lake)"
  },
  {
    "objectID": "writeups/optimization/index.html#maximum-likelihood-estimation-as-unconstrained-optimization",
    "href": "writeups/optimization/index.html#maximum-likelihood-estimation-as-unconstrained-optimization",
    "title": "Mathematical Optimization",
    "section": "Maximum Likelihood Estimation as Unconstrained Optimization",
    "text": "Maximum Likelihood Estimation as Unconstrained Optimization\nNow that we have this general procedure for non-constrained optimization in general, let‚Äôs use it to obtain a maximum likelihood estimate for some probabilistic model.\n\n‚ÄúStandard‚Äù Example: Poisson Distribution\nWe‚Äôll start with an example more similar to the ones you did in DSAN 5100: estimating the rate parameter \\(\\param{\\lambda}\\) for a random variable \\(X\\) with a Poisson distribution.\n\n\n\n\n\n\nExample 2: MLE for Poisson-Distributed RV\n\n\n\nGiven a dataset consisting of \\(N\\) i.i.d. realizations \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) of Poisson-distributed random variables\n\\[\nX_1, \\ldots, X_n \\sim \\text{Pois}(\\param{\\lambda}),\n\\]\nfind the Maximum Likelihood Estimate for the parameter \\(\\param{\\lambda}\\).\n\n\nLike with other distributions we looked at in class, the way to approach problems like this is to write out the details step-by-step until you have enough information to start deriving the MLE. So, the first piece of information we have is:\n\nA random variable \\(X \\sim \\text{Pois}(\\param{\\lambda})\\)\n\nGiven how the Poisson distribution works, this means that2\n\\[\n\\Pr(X = k; \\param{\\lambda}) = \\frac{\\param{\\lambda}^ke^{-\\param{\\lambda}}}{k!},\n\\tag{1}\\]\nwhere we can use probability mass \\(\\Pr(X = k)\\) rather than probability density \\(f_X(k)\\) since the Poisson distribution is a discrete distribution (modeling integer counts rather than continuous values like the normal distribution).\nThe next piece of information we have, in a Maximum Likelihood setup, is an observed dataset containing \\(N\\) i.i.d. datapoints,\n\n\\(\\mathbf{x} = (x_1, \\ldots, x_n)\\),\n\nwhere each point is assumed to be drawn from this Poisson distribution with parameter \\(\\param{\\lambda}\\). This assumption means that we treat each of these observed points \\(x_i\\) as the realization of a Poisson-distributed Random Variable \\(X_i\\), so that (by Equation¬†1 above):\n\\[\n\\Pr(X_i = x_i; \\param{\\lambda}) =  \\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!},\n\\]\nOur goal in Maximum Likelihood Estimation is to take this observed dataset and figure out what value of the parameter \\(\\param{\\lambda}\\) is most likely to have produced \\(\\mathbf{x}\\).\nIn other words, we are trying to find the value of \\(\\param{\\lambda}\\) which maximizes the joint probability that \\(X_1\\) is realized as \\(x_1\\), \\(X_2\\), is realized as \\(x_2\\), and so on up to \\(X_n\\) being realized as \\(x_n\\), which we call the likelihood \\(\\mathcal{L}(\\mathbf{x}; \\param{\\lambda})\\) of the dataset:\n\\[\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n; \\param{\\lambda}).\n\\]\nThe key for being able to compute this giant joint probability is the independence assumption: since the values in \\(\\mathbf{x}\\) are assumed to be independent and identically distributed (i.i.d.), by the definition of independence, we can factor this full joint probability into the product of individual-variable probabilities:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) &= \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n; \\param{\\lambda}) \\\\\n&= \\Pr(X_1 = x_1; \\param{\\lambda}) \\times \\cdots \\times \\Pr(X_n = x_n; \\param{\\lambda}) \\\\\n&= \\prod_{i=1}^{N}\\Pr(X_i = x_i; \\param{\\lambda})\n\\end{align*}\n\\]\nThis factoring into individual terms is the key to solving this problem! Now that we‚Äôve done this, the rest of the problem boils down to a calculus problem. Let‚Äôs write out this product (which will look messy at first, but we‚Äôll make it simpler using \\(\\log()\\) below!):\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) &= \\prod_{i=1}^{N}\\Pr(X_i = x_i; \\param{\\lambda}) \\\\\n&= \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!}\n\\end{align*}\n\\]\nBecause \\(\\log()\\) is a monotonic function, the value of \\(x\\) which maximizes \\(\\log(f(x))\\) will be the same as the value which maximizes \\(f(x)\\). So, to make our lives easier since \\(\\log()\\) turns multiplications into additions, we compute the log-likelihood (the log of the likelihood function above), which we denote \\(\\ell(\\mathbf{x}; \\param{\\lambda})\\):\n\\[\n\\ell(\\mathbf{x}; \\param{\\lambda}) = \\log\\left[ \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!} \\right]\n\\]\nI recommend working through this application of \\(\\log()\\) yourself, if you can, then you can click the following button to show the worked-out solution:\n\n\nClick to Show Solution\n\n\\[\n\\begin{align*}\n\\ell(\\mathbf{x}; \\param{\\lambda}) &= \\log\\left[ \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!} \\right] \\\\\n&= \\sum_{i=1}^{N}\\log\\left[ \\frac{\\param{\\lambda}^{x_i}e^{-\\lambda}}{x_i!} \\right] \\\\\n&= \\sum_{i=1}^{N}\\log (\\param{\\lambda}^{x_i}e^{-\\lambda}) - \\sum_{i=1}^N\\log(x_i!) \\\\\n&= \\sum_{i=1}^{N}x_i\\log(\\param{\\lambda}) + \\sum_{i=1}^{N}\\log(e^{-\\param{\\lambda}}) - \\sum_{i=1}^{N}\\log(x_i!) \\\\\n&= \\log(\\param{\\lambda})\\sum_{i=1}^{N}x_i - N\\param{\\lambda} - \\sum_{i=1}^{N}\\log(x_i!).\n\\end{align*}\n\\]\n\nThis might look scary at first, for example, because of the term with the \\(x_i!\\). However, keep in mind that we won‚Äôt need to worry about this term, since it does not involve \\(\\param{\\lambda}\\), the parameter we are maximizing over!\nSo, following the same procedure as our previous example, we maximize this log-likelihood function with respect to \\(\\param{\\lambda}\\). We start by computing the derivative of \\(\\ell(\\mathbf{x}; \\param{\\lambda})\\) with respect to \\(\\param{\\lambda}\\):\n\\[\n\\frac{\\partial}{\\partial \\param{\\lambda}}\\ell(\\mathbf{x}; \\param{\\lambda}) = \\frac{\\partial}{\\partial \\param{\\lambda}}\\left[ \\log(\\param{\\lambda})\\sum_{i=1}^{N}x_i - N\\param{\\lambda} - \\sum_{i=1}^{N}\\log(x_i!) \\right]\n\\]\nLike before, I recommend working through this yourself on paper, and then you can click the following to show the worked-out solution:\n\n\nClick to Show Solution\n\nSince both \\(\\sum_{i=1}^{N}x_i\\) and \\(\\sum_{i=1}^{N}\\log(x_i!)\\) are constants with respect to \\(\\param{\\lambda}\\), and since the derivative operator \\(\\frac{\\partial}{\\partial\\param{\\lambda}}\\) is linear, this reduces to:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\param{\\lambda}}\\ell(\\mathbf{x}; \\param{\\lambda}) &= \\left( \\sum_{i=1}^{N}x_i\\right) \\frac{\\partial}{\\partial \\param{\\lambda}}[\\log(\\param{\\lambda})] - N\\frac{\\partial}{\\partial\\param{\\lambda}}[\\param{\\lambda}] \\\\\n&= \\frac{\\sum_{i=1}^{N}x_i}{\\param{\\lambda}} - N.\n\\end{align*}\n\\]\nAnd now we can set this equal to zero and solve to obtain the maximum-likelihood estimator \\(\\lambda^*\\):\n\\[\n\\begin{align*}\n&\\frac{\\sum_{i=1}^{N}x_i}{\\lambda^*} - N = 0 \\\\\n\\iff &\\frac{\\sum_{i=1}^{N}x_i}{\\lambda^*} = N \\\\\n\\iff &\\sum_{i=1}^{N}x_i = N\\lambda^* \\\\\n\\iff &\\lambda^* = \\frac{1}{N}\\sum_{i=1}^{N}x_i,\n\\end{align*}\n\\]\n\nmeaning that, after all this work, the maximum likelihood estimator for a dataset containing realizations of i.i.d. Poisson RVs is the sample mean of those points, \\(\\frac{1}{N}\\sum_{i=1}^{N}x_i\\).\nIn other words, if you are given a dataset \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\), and you think that the entries in this dataset were generated via the Poisson distribution, then the ‚Äúbest guess‚Äù (if we define ‚Äúbest guess‚Äù as ‚Äúguess with maximum likelihood‚Äù) for the parameter \\(\\param{\\lambda}\\) of this Poisson distribution is the sample mean of the observed points, \\(\\frac{1}{N}\\sum_{i=1}^{N}x_i\\).\n\n\nTrickier Example: Linear Regression\nSince I think linear regression is a really important model to have in the back of your mind as you move towards fancier Machine Learning models, but is a bit more complex than the Poisson case we just looked at, a good starting point is an over-simplified version of linear regression, where we don‚Äôt even have an intercept.\n\n\n\n\n\n\nExample 3: Zero-Intercept Linear Regression\n\n\n\nAssume we have a dataset \\(\\mathbf{d} = ((x_1,y_1),\\ldots,(x_N,y_N))\\) containing noisy observations from some underlying linear relationship \\(y = \\param{\\beta} x\\), so that we model what we observe in \\(\\mathbf{d}\\) as realizations of random variables \\(X\\), \\(Y\\), and \\(\\varepsilon_i\\):\n\\[\nY_i = \\param{\\beta} X_i + \\varepsilon_i,\n\\]\nwhere the variables \\(\\varepsilon_i\\) are i.i.d. normally-distributed variables with mean zero and a given variance \\(\\sigma^2\\): \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).\nFind the Maximum Likelihood Estimator for the single parameter in this case, \\(\\param{\\beta}\\).\n\n\nNote that in general, if a random variable \\(X\\) is distributed normally, then adding things to \\(X\\) just shifts the mean parameter \\(\\mu\\) (meaning, for example, if \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\), then \\(X + 3 \\sim \\mathcal{N}(\\mu + 3, \\sigma)\\)).\nHere, since \\(\\varepsilon_i\\) is a normally-distributed random variable with \\(\\mu = 0\\), the left-hand side of the above equation means that \\(Y_i \\sim \\mathcal{N}(\\param{\\beta} X_i, \\sigma)\\).\nJust as we used the Poisson PMF in the previous example, here you will use the Normal pdf, the probability density function for \\(Y_i\\) in this case, which is typically denoted using the Greek letter \\(\\varphi\\) (‚ÄúPhi‚Äù):\n\\[\n\\varphi(v; \\param{\\mu}, \\param{\\sigma}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left(\\frac{v - \\mu}{\\sigma}\\right)^2\\right].\n\\]\nAlthough in this ‚Äústandard form‚Äù for the pdf of the normal distribution the two parameters are \\(\\mu\\) and \\(\\sigma\\), in our case note that we are not estimating the parameters \\(\\mu\\) and \\(\\sigma\\) itself. Instead, we are estimating a single parameter \\(\\param{\\beta}\\), which is not the mean or standard deviation itself, though it ends up affecting the mean since \\(Y_i \\sim \\mathcal{N}(\\param{\\beta} X_i, \\sigma)\\).\nSo, the way we obtain the likelihood which we can then use to estimate \\(\\param{\\beta}\\) is by plugging \\(\\param{\\beta} X_i\\) into the pdf above, to obtain:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{d}; \\param{\\beta}) &= \\prod_{i=1}^{N}\\varphi(y_i; \\param{\\beta}x_i, \\sigma) \\\\\n&= \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right]\n\\end{align*}\n\\]\nLike in the Poisson case, this looks scary until you transform it into the log-likelihood function, at which point lots of things simplify and you can compute a closed-form solution!\n\n\nClick to Show Solution\n\n\\[\n\\begin{align*}\n\\ell(\\mathbf{d}; \\param{\\beta}) &= \\log\\left[ \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right] \\right] \\\\\n&= \\sum_{i=1}^{N}\\log\\left[ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right] \\right] \\\\\n&= \\sum_{i=1}^{N}-\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2}\\sum_{i=1}^{N}\\left(\\frac{y_i - \\param{\\beta} x_i}{\\sigma}\\right)^2 \\\\\n&= -N\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(y_i - \\param{\\beta} x_i\\right)^2.\n\\end{align*}\n\\]\nAnd we now have the log-likelihood in a form where we can compute a derivative straightforwardly (using the derivative ‚Äúrules‚Äù in the table presented earlier):\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\param{\\beta}}\\ell(\\mathbf{d}; \\param{\\beta}) &= \\frac{\\partial}{\\partial\\param{\\beta}}\\left[ -N\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(y_i - \\param{\\beta} x_i\\right)^2 \\right] \\\\\n&= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\param{\\beta}}\\left[ (y_i - \\param{\\beta}x_i)^2 \\right] \\\\\n&= -\\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(y_i-\\param{\\beta}x_i)x_i \\\\\n&= -\\frac{1}{\\sigma^2}\\left[\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2\\right].\n\\end{align*}\n\\]\nBy setting this equal to zero, we can now solve for the MLE estimator \\(\\beta^*\\):\n\\[\n\\begin{align*}\n&-\\frac{1}{\\sigma^2}\\left[\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2\\right] = 0 \\\\\n\\iff &\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2 = 0 \\\\\n\\iff &\\sum_{i=1}^{N}x_iy_i = \\param{\\beta}\\sum_{i=1}^{N}x_i^2,\n\\end{align*}\n\\]\nAnd thus the MLE for \\(\\param{\\beta}\\) in this case is\n\\[\n\\widehat{\\beta}_{MLE} = \\frac{\\sum_{i=1}^{N}x_iy_i}{\\sum_{i=1}^{N}x_i^2}.\n\\]\n\nThis means that, if we want the line of ‚Äúbest‚Äù fit for a dataset \\(\\mathbf{d} = ((x_1,y_1),\\ldots,(x_n,y_n))\\), where ‚Äúbest fit‚Äù is defined to be ‚Äúslope with maximum likelihood, with intercept 0‚Äù, this line is\n\\[\ny = \\widehat{\\beta}_{MLE}x = \\frac{\\sum_{i=1}^{N}x_iy_i}{\\sum_{i=1}^{N}x_i^2}x.\n\\]"
  },
  {
    "objectID": "writeups/optimization/index.html#general-constraints-only-optimization",
    "href": "writeups/optimization/index.html#general-constraints-only-optimization",
    "title": "Mathematical Optimization",
    "section": "General Constraints-Only Optimization",
    "text": "General Constraints-Only Optimization\nSeeing an optimization problem written out with only constraints looks a bit weird at first, but will be helpful to consider before we look at full-on constrained optimization.\nLike how we looked at a ‚Äúbasic‚Äù calculus problem before applying the same methodology to MLE before, here let‚Äôs look at a ‚Äúbasic‚Äù algebra problem with inequalities:\n\n\n\n\n\n\nExample 4: Inequality Constraints\n\n\n\nFind the value \\(x^*\\) which satisfies the system of inequalities\n\\[\n\\begin{alignat}{2}\nx^* = &&\\max_{x} \\quad &f(x,y) = 0 \\\\\n&&\\text{s.t.} \\quad & y \\geq x^2 + 1 \\\\\n&& \\quad & y \\leq \\sqrt{1 - x^2}\n\\end{alignat}\n\\]\n\n\nIt looks weird at first because, the \\(\\max_x\\) portion doesn‚Äôt give us anything to work with: we‚Äôre ‚Äúmaximizing‚Äù \\(f(x)\\) which is always just the number \\(0\\)! So, instead, we focus on just the constraints:\n\n\\(y \\geq x^2 + 1\\)\n\\(y \\leq \\sqrt{1 - x^2}\\)\n\nPlotting these two functions, we can see that they meet at exactly one point:\n\n\nCode\nlibrary(latex2exp)\nx_df &lt;- tibble(x=c(-2,2))\nf1 &lt;- function(x) x^2 + 1\nf1_lab &lt;- TeX(\"$y \\\\geq x^2 + 1$\")\nf2 &lt;- function(x) sqrt(1 - x^2)\nf2_lab &lt;- TeX(\"$y \\\\leq \\\\sqrt{1 - x^2}$\")\nsoln_df &lt;- tibble(x=0, y=1)\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=f1, color='black') +\n  stat_function(fun=f1, geom=\"ribbon\", mapping=aes(ymin=after_stat(y),ymax=4, fill=\"f1\"), alpha=0.5) +\n  stat_function(fun=f2, color='black') +\n  stat_function(fun=f2, geom=\"area\", aes(fill=\"f2\"), alpha=0.5) +\n  geom_point(data=soln_df, aes(x=x, y=y)) +\n  theme_classic() +\n  scale_fill_manual(\n    \"Constraints:\",\n    values=c(cb_palette[1], cb_palette[2]),\n    labels=c(f1_lab, f2_lab)\n  ) +\n  ylim(0,4)\n\n\n\n\n\n\n\n\n\nwhich means that we can solve for where they‚Äôre equal to derive the unique optimal solution \\(x^*\\)!\n\\[\n\\begin{align*}\n&x^2 + 1 = \\sqrt{1 - x^2} \\\\\n\\iff &(x^2 + 1)^2 = 1 - x^2 \\\\\n\\iff & x^4 + 2x^2 + 1 = 1 - x^2 \\\\\n\\iff & x^4 + 3x^2 = 0 \\\\\n\\iff & x^2(x^2 + 3) = 0,\n\\end{align*}\n\\]\nwhich means that the only possible solutions are the solutions to \\(x^2 = 0\\) and \\(x^2 + 3 = 0\\). The only \\(x\\) which satisfies \\(x^2 = 0\\) is \\(x^* = 0\\). The only \\(x\\) which satisfies \\(x^2 + 3 = 0\\) is \\(x^* = \\sqrt{-3}= \\pm 3i\\), meaning that our only real solution is \\(x^* = 0\\), forming the unique (real) solution to our optimization problem."
  },
  {
    "objectID": "writeups/optimization/index.html#gmm-estimation-as-constraints-only-optimization",
    "href": "writeups/optimization/index.html#gmm-estimation-as-constraints-only-optimization",
    "title": "Mathematical Optimization",
    "section": "GMM Estimation as Constraints-Only Optimization",
    "text": "GMM Estimation as Constraints-Only Optimization\nThe Generalized Method of Moments approach basically takes advantage of the type of ‚Äúcrunching‚Äù we saw in the previous example, setting up a system of equations which ‚Äúcrunches‚Äù the set of possible values for the desired parameter \\(\\param{\\theta}\\) down into just a single value.\nIt is able to accomplish this, basically, by saying:\nIf the entries in our dataset \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) are realizations of RVs \\(X_1, \\ldots, X_n\\) drawn i.i.d. from some distribution \\(\\mathcal{D}(\\param{\\boldsymbol\\theta})\\) with parameters \\(\\param{\\boldsymbol\\theta}\\), then the moments of the theoretical distribution \\(\\mathcal{D}(\\param{\\boldsymbol\\theta})\\) should match their observed counterparts.‚Äù\nSpecifically, \\(\\param{\\boldsymbol\\theta}\\) should be the value which makes the:\n\n\n\n\n\n\n\n\nExpected mean \\(\\mu = \\mathbb{E}[X_i]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved mean of the dataset, \\(\\widehat{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nExpected variance \\(\\sigma^2 = \\mathbb{E}[(X_i - \\mu)^2]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved variance of the dataset, \\(\\widehat{\\sigma^2} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^2\\)\n\n\nExpected skewness \\(\\gamma = \\mathbb{E}[(X_i - \\mu)^3]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved skewness of the dataset, \\(\\widehat{\\gamma} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^3\\)\n\n\nExpected kurtosis \\(\\kappa = \\mathbb{E}[(X_i - \\mu)^4]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved skewness of the dataset, \\(\\widehat{\\kappa} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^4\\)\n\n\n\nand so on‚Äîas many equations as we need to estimate the parameters \\(\\param{\\boldsymbol\\theta}\\) of the distribution \\(\\mathcal{D}\\)!\nSo, let‚Äôs repeat the earlier problem with the Poisson distribution, using GMM instead of MLE, to compute an estimator for the rate parameter \\(\\param{\\lambda}\\).\n\n\n\n\n\n\nExample 5: GMM Estimate for Poisson-Distributed RV\n\n\n\nGiven a dataset consisting of \\(N\\) i.i.d. realizations \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) of Poisson-distributed random variables\n\\[\nX_1, \\ldots, X_n \\sim \\text{Pois}(\\param{\\lambda}),\n\\]\nfind the Generalized Method of Moments estimate for the parameter \\(\\param{\\lambda}\\).\n\n\nHere, since our distribution only has one parameter \\(\\param{\\lambda}\\), we only need one equation in our system of equations, which will ‚Äúmatch‚Äù the expected value of a Poisson-distributed RV with the mean of our dataset \\(\\mathbf{x}\\):\n\\[\n\\begin{align*}\n&\\mu = \\widehat{\\mu} \\\\\n\\iff &\\mathbb{E}[X_i] = \\frac{1}{N}\\sum_{i=1}^{N}x_i \\\\\n\\iff &\\param{\\lambda} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\n\\end{align*}\n\\]\nAnd‚Ä¶ yup, that‚Äôs it! Since the expected value of a poisson-distributed Random Variable \\(X_i\\) is just exactly the rate parameter \\(\\param{\\lambda}\\)3, we‚Äôve obtained the GMM estimate of \\(\\param{\\lambda}\\) as desired here, and we see that in fact the GMM estimator is the same as the MLE estimator in this case."
  },
  {
    "objectID": "writeups/optimization/index.html#general-constrained-optimization",
    "href": "writeups/optimization/index.html#general-constrained-optimization",
    "title": "Mathematical Optimization",
    "section": "General Constrained Optimization",
    "text": "General Constrained Optimization\nThough we introduced objective functions and constraints separately here, in reality most problems will require you to optimize with respect to both of these to find the solution to your problem.\nTo use the example I used throughout the semester one more time: if you are trying to estimate the population mean height from a sample of heights, you may want to have:\n\nAn objective function quantifying how ‚Äúgood‚Äù a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) is in terms of fitting the data (this would be precisely the likelihood function), but also\nA constraint on \\(\\mu\\) to ensure that you don‚Äôt get negative values (since heights in reality can‚Äôt be negative), and perhaps another constraint to ensure that you don‚Äôt get absurdly high numbers in the millions and billions as well.\n\nWhile you‚Äôve seen likelihood functions, constraints in optimization world are written out as:\n\nEqualities like \\(\\sum_{i=1}^{N}\\theta_i = 1\\) or\nInequalities like \\(\\theta_i &lt; 0.5 ~ \\forall i\\).\n\nSo, for the normal distribution example just mentioned, you could introduce inequality constraints to re-write the problem in the following form (where we just include the non-negative constraint, for simplicity):\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\mu}, \\sigma} \\quad &\\mathcal{L}(X = v_X \\mid \\param{\\mu}, \\sigma) \\\\\n&& \\text{s.t.} \\quad & \\param{\\mu} &gt; 0\n\\end{alignat}\n\\]\nThese two working together (the objective function and the constraints), it turns out, will ‚Äúsupercharge‚Äù your estimation! Now, instead of just guessing a random number from \\(-\\infty\\) to \\(\\infty\\) as your initial guess for \\(\\widehat{\\mu}\\), you have the power to ‚Äúguide‚Äù the optimization by (say) starting at the lower bound on \\(\\widehat{\\mu}\\), in this case, 0.\nFor solving by hand, though, we‚Äôll need to use a technique called the Lagrange Multiplier approach, to turn this constrained optimization problem back into an unconstrained optimization, since this is the case where we know how to use calculus to solve!\n\n\n\n\n\n\nExample 6: Constrained Optimization via Lagrange Multipliers\n\n\n\nFind the optimal value \\(x^*\\) for the following optimization problem:\n\\[\n\\begin{alignat}{2}\nx^* = &&\\min_{x} \\quad &f(x, y) = 2 - x^2 - 2y^2 \\\\\n&& \\text{s.t.} \\quad & x^2 + y^2 = 1\n\\end{alignat}\n\\]\n\n\nWe can visualize the two ‚Äúpieces‚Äù of this optimization, to see (finally!) how the objective function and the constraints come together:\n\n\nCode\nlibrary(ggforce)\nmy_f &lt;- function(x,y) 2 - x^2 - 2*y^2\nx_vals &lt;- seq(from=-2, to=2, by=0.1)\ny_vals &lt;- seq(from=-2, to=2, by=0.1)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\ndata_df |&gt; ggplot(aes(x=x, y=y, z=z)) +\n  # geom_rect(xmin=0, xmax=1, ymin=-Inf, ymax=Inf, alpha=0.5, fill=cb_palette[1]) +\n  # geom_vline(xintercept=0, linewidth=0.75) +\n  geom_contour_filled(alpha=0.9, binwidth = 0.5, color='black', linewidth=0.2) +\n  geom_point(aes(x=0, y=0)) +\n  geom_circle(aes(x0=0, y0=0, r=1)) +\n  geom_segment(aes(x=0, y=0, xend=1, yend=0), linetype=\"dashed\") +\n  scale_fill_viridis_d(option=\"C\") +\n  theme_classic() +\n  # theme(legend.position=\"none\") +\n  coord_equal()\n\n\n\n\n\n\n\n\nFigure¬†1: The goal of our constrained optimization problem is to find the optimal value(s) \\((x^*,y^*)\\) which maximize \\(f(x,y)\\) (higher values are brighter yellow here), subject to the constraint that the point \\((x^*, y^*)\\) lies on the unit circle.\n\n\n\n\n\nSadly, our earlier approach of finding the derivative of the objective function, setting it equal to zero, and solving, won‚Äôt work here. Let‚Äôs see why. First compute the partial derivatives:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x,y) = -2x \\\\\n\\frac{\\partial}{\\partial y}f(x,y) = -4y\n\\end{align*}\n\\]\nThen set them equal to zero and solve the system of equations:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x,y) = 0 \\iff -2x^* = 0 \\iff &\\boxed{x^* = 0} \\\\\n\\frac{\\partial}{\\partial y}f(x,y) = 0 \\iff -4y^* = 0 \\iff &\\boxed{y^* = 0}.\n\\end{align*}\n\\]\nWe can see now that our computed optimal point, \\((0, 0)\\), violates the desired constraint, since it does not lie on the unit circle \\(x^2 + y^2 = 1\\):\n\\[\n(x^*)^2 + (y^*)^2 = 0^2 + 0^2 = 0 \\neq 1.\n\\]\nThe Lagrange Multiplier approach comes to the rescue here, because it allows us to use this same derivative-based method by incorporating the constraints into the function we take the derivative of and set equal to zero to solve.\nThe new approach you can use to solve for \\(x^*\\) in situations like this is based on constructing a new function \\(\\mathscr{L}(x, y, \\lambda)\\), called the Lagrangian, where the new parameter \\(\\lambda\\) is just the coefficient on a constraint function \\(g(x, y)\\).\nThis \\(g(x, y)\\) may seem complicated at first, but I think of it like a ‚Äúhelper function‚Äù which you derive from the constraint(s), finding a \\(g(x, y)\\) such that:\n\n\\(g(x) = 0\\) when the constraint is satisfied, and\n\\(g(x) \\neq 0\\) when the constraint is violated.\n\nIn our case, therefore, we can rewrite the constraint like:\n\\[\nx^2 + y^2 = 1 \\iff x^2 + y^2 - 1 = 0,\n\\]\nand choose our \\(g(x,y)\\) to be the left side of this equation: \\(g(x,y) = x^2 + y^2 - 1\\).\nWith our constraint function now chosen, we construct the Lagrangian:\n\\[\n\\begin{align*}\n\\mathscr{L}(x, \\lambda) &= f(x) + \\lambda g(x) = 2 - x^2 - 2y^2 + \\lambda(x^2 + y^2 - 1) \\\\\n&= 2 - x^2 - 2y^2 + \\lambda x^2 + \\lambda y^2 - \\lambda\n\\end{align*}\n\\]\nand optimize in the same way we‚Äôve always optimized via calculus, making sure to compute all three of the partial derivatives:\n\nWith respect to \\(x\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial x} = -2x + 2\\lambda x\n\\]\nWith respect to \\(y\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial y} = -4y + 2\\lambda y\n\\]\nAnd with respect to \\(\\lambda\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial \\lambda} = x^2 + y^2 - 1\n\\]\n\nwe can now set all of these derivatives equal to zero, to obtain a system of equations:\n\\[\n\\begin{align*}\n-2x + 2\\lambda x &= 0 \\\\\n-4y + 2\\lambda y &= 0 \\\\\nx^2 + y^2 - 1 &= 0\n\\end{align*}\n\\]\nThere are a few ways to solve this system (including using matrices!), but here‚Äôs how I solved it, by solving for \\(\\lambda^*\\) first:\n\\[\n\\begin{align*}\n-2x + 2\\lambda^* x = 0 \\iff 2\\lambda^* x = 2x \\iff \\boxed{\\lambda^* = 1},\n\\end{align*}\n\\]\nthen deriving the value of \\(y\\):\n\\[\n\\begin{align*}\n&-4y^* + 2\\lambda^* y^* = 0 \\iff -4y^* + 2(1)y^* = 0 \\\\\n&\\iff -4y^* + 2y^* = 0 \\iff \\boxed{y^* = 0},\n\\end{align*}\n\\]\nand \\(x\\):\n\\[\n\\begin{align*}\n&(x^*)^2 + (y^*)^2 - 1 = 0 \\iff (x^*)^2 + (0)^2 - 1 = 0 \\\\\n&\\iff (x^*)^2 = 1 \\iff \\boxed{x^* = \\pm 1}\n\\end{align*}\n\\]\nAnd indeed, by looking at the plot in Figure¬†1 above, we see that \\((-1,0)\\) and \\((1,0)\\) are the two optimal values here!"
  },
  {
    "objectID": "writeups/optimization/index.html#references",
    "href": "writeups/optimization/index.html#references",
    "title": "Mathematical Optimization",
    "section": "References",
    "text": "References\n\n\nBoyd, Stephen P., and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf."
  },
  {
    "objectID": "writeups/optimization/index.html#footnotes",
    "href": "writeups/optimization/index.html#footnotes",
    "title": "Mathematical Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA somewhat important point, but I thought I‚Äôd make it a footnote in case it‚Äôs already clear to some folks: when you enter the ‚Äúworld‚Äù of optimization problems, you stop caring so much about whether the problem is a maximization or minimization problem. Instead, you literally just use whichever is easier to compute: the max of \\(f(x)\\) or the min of \\(-f(x)\\). To be a little more specific: in convex optimization, the most general type of optimization we have efficient algorithms for, you always minimize, and then just transform \\(f(x)\\) into \\(-f(x)\\) if your original intent was to maximize (this is‚Ä¶ by convention essentially. See Boyd and Vandenberghe (2004) for more!).‚Ü©Ô∏é\nNote how we use ‚Äú;‚Äù to separate random variables like \\(X\\) in this case from non-random variables like \\(\\lambda\\) in this case: sometimes people use the conditional operator ‚Äú|‚Äù for this, but that can sometimes lead to confusion since conditioning on the value of a Random Variable is different from having the value of a non-random variable as a parameter to the function.‚Ü©Ô∏é\nYou can find a proof of this fact here.‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/hw3-guide/index.html",
    "href": "writeups/hw3-guide/index.html",
    "title": "Getting Started with HW 3",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nOriginal version posted 18 Mar 2025, 7:00pm\nThe following clarifications on different pieces of HW3 helped me when going through it with students in office hours."
  },
  {
    "objectID": "writeups/hw3-guide/index.html#hw-3.2-advertising-budgets",
    "href": "writeups/hw3-guide/index.html#hw-3.2-advertising-budgets",
    "title": "Getting Started with HW 3",
    "section": "HW-3.2: Advertising Budgets",
    "text": "HW-3.2: Advertising Budgets\nIn the first code cell within this question, we do provide starter code, but you will still need to carry out the last part of the instructions, ‚ÄúSplit the data into a training and test set (70% / 30%)‚Äù. The starter code does much of the work for you, since you should be able to use the created train object to construct a tibble like train_df, which can then be used with gam() in the next part."
  },
  {
    "objectID": "writeups/hw3-guide/index.html#hw-3.2a",
    "href": "writeups/hw3-guide/index.html#hw-3.2a",
    "title": "Getting Started with HW 3",
    "section": "HW-3.2a",
    "text": "HW-3.2a\nHere, as a more specific clarification: there are multiple ways to specify a regression spline in R, but the two ways that I have encountered the most are as follows.\nFirst, using just the gam library in the ‚Äúdefault‚Äù way it is set up, you should be able to use the s() function to specify spline terms, and then include the optional df argument to specify the degree of each term:\nlibrary(gam)\ngam_model &lt;- gam(\n    dependent ~ s(indep1, df = degree1) + s(indep2, df = degree2) + s(indep3, df = degree3),\n    data=train_df\n)\nHowever, as a second approach which is more or less popular depending on the field (and the time the code was written), you may also see example code that looks like:\nlibrary(gam)\ngam_model &lt;- gam(\n    dependent ~ s(indep1, k = degree1) + s(indep2, k = degree2) + s(indep3, k = degree3),\n    data=train_df\n)\nAt first this looks suspiciously close to the previous code block, but, the difference is that the k argument used in the second block is not one of the arguments included ‚Äúby default‚Äù in the s() function provided by gam. From what I understand, there is a second library called mgcv which modifies gam‚Äôs s() function to allow this k argument.\nThis means, to get this second approach to work, in terms of making sure that the s() function allows the degree argument k, you‚Äôll need to load the mgcv library in addition to gam** (which is already loaded for you in a cell near the top of the distributed homework files). In other words, if you just copy-and-paste the previous code, you‚Äôll get an error (at least in my version of R) that looks like\nError in s(indep1, k = degree1) : unused argument (k = degree1)\nSo, here, the mgcv library essentially adds in this new k argument that can be passed to s() (which is from gam itself) to specify the degree parameter."
  },
  {
    "objectID": "writeups/lab-1/index.html",
    "href": "writeups/lab-1/index.html",
    "title": "Getting Started with Lab 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncb_palette &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n)\ncenter_title &lt;- function(orig_theme) {\n  theme_centered &lt;- orig_theme + theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n  return(theme_centered)\n}\n\n\n\nFirst, let‚Äôs visualize the objective function one more time, then we‚Äôll see what the gradient vector (specifically, the gradient vector) tells us about how we should move/update our guess after each step.\nIn this case, we‚Äôre given the following loss function \\(L(w)\\): (but, see sidebar on loss functions below!)\n\\[\nL(w) = (w - 10)^2 + 5\n\\]\n\n\n\n\n\n\nWhere Does the Loss Function Come From?\n\n\n\n\n\nThe focus of this assignment is to help you see how numerical methods like gradient descent can use a loss function \\(L\\), along with its first and second derivates (whether exact or approximate), to optimize parameters of a model by finding the minimum of \\(L\\) with respect to these parameters.\nIn Section 01, for example, we looked at the contrived but (imo) useful-for-intuition model of regression without an intercept:\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\nAnd then we saw how, once we choose some particular value \\(b\\) for \\(\\beta_1\\), we can compute how well this model with this parameter setting fits a dataset \\((\\mathbf{x}, \\mathbf{y}) = ((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n))\\) by computing the residual sum of squares (RSS)‚Äîthe differences between the predictions \\(\\widehat{y}_i = b x_i\\) generated by the model (again, using that choice \\(\\beta_1 = b\\)) and the actual observed values \\(y_i\\):\n\\[\nL(b) = RSS(b) = \\sum_{i=1}^{n}(\\widehat{y}_i(b) - y_i)^2\n\\]\nThis is why using a quadratic function as our starting example of a loss function is useful here‚Äîwhile in practice the loss function is a potentially-complex function of the data \\((\\mathbf{x}, \\mathbf{y})\\) and the model parameters, here we simplify the above RSS computation down to its essence of a quadratic function like \\(L(w) = (w - 10)^2 + 5\\), so that we can explore how a function like this can be optimized via numerical methods.\n\n\n\nThe given loss function \\(L(w)\\) on its own looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nloss_fn &lt;- function(w) {\n    # Make predictions using w, then sum the\n    # squared residuals, how good/bad is a line\n    # with slope w\n    return((w - 10)^2 + 5)\n}\nggplot() +\n  stat_function(data=tibble(x=c(0, 10)), fun=loss_fn, linewidth=1) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title = \"Quadratic Loss Function\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "writeups/lab-1/index.html#visualizing-the-loss-function",
    "href": "writeups/lab-1/index.html#visualizing-the-loss-function",
    "title": "Getting Started with Lab 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncb_palette &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n)\ncenter_title &lt;- function(orig_theme) {\n  theme_centered &lt;- orig_theme + theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n  return(theme_centered)\n}\n\n\n\nFirst, let‚Äôs visualize the objective function one more time, then we‚Äôll see what the gradient vector (specifically, the gradient vector) tells us about how we should move/update our guess after each step.\nIn this case, we‚Äôre given the following loss function \\(L(w)\\): (but, see sidebar on loss functions below!)\n\\[\nL(w) = (w - 10)^2 + 5\n\\]\n\n\n\n\n\n\nWhere Does the Loss Function Come From?\n\n\n\n\n\nThe focus of this assignment is to help you see how numerical methods like gradient descent can use a loss function \\(L\\), along with its first and second derivates (whether exact or approximate), to optimize parameters of a model by finding the minimum of \\(L\\) with respect to these parameters.\nIn Section 01, for example, we looked at the contrived but (imo) useful-for-intuition model of regression without an intercept:\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\nAnd then we saw how, once we choose some particular value \\(b\\) for \\(\\beta_1\\), we can compute how well this model with this parameter setting fits a dataset \\((\\mathbf{x}, \\mathbf{y}) = ((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n))\\) by computing the residual sum of squares (RSS)‚Äîthe differences between the predictions \\(\\widehat{y}_i = b x_i\\) generated by the model (again, using that choice \\(\\beta_1 = b\\)) and the actual observed values \\(y_i\\):\n\\[\nL(b) = RSS(b) = \\sum_{i=1}^{n}(\\widehat{y}_i(b) - y_i)^2\n\\]\nThis is why using a quadratic function as our starting example of a loss function is useful here‚Äîwhile in practice the loss function is a potentially-complex function of the data \\((\\mathbf{x}, \\mathbf{y})\\) and the model parameters, here we simplify the above RSS computation down to its essence of a quadratic function like \\(L(w) = (w - 10)^2 + 5\\), so that we can explore how a function like this can be optimized via numerical methods.\n\n\n\nThe given loss function \\(L(w)\\) on its own looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nloss_fn &lt;- function(w) {\n    # Make predictions using w, then sum the\n    # squared residuals, how good/bad is a line\n    # with slope w\n    return((w - 10)^2 + 5)\n}\nggplot() +\n  stat_function(data=tibble(x=c(0, 10)), fun=loss_fn, linewidth=1) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title = \"Quadratic Loss Function\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "writeups/lab-1/index.html#how-the-derivative-helps-us",
    "href": "writeups/lab-1/index.html#how-the-derivative-helps-us",
    "title": "Getting Started with Lab 1",
    "section": "How the Derivative Helps Us",
    "text": "How the Derivative Helps Us\nIn this case, our loss function actually has an easily-computed closed-form derivative (though, as mentioned in the info box above, this is usually not the case as we move to more complex models like neural networks). We can use the chain rule \\(\\frac{\\partial}{\\partial x}f(g(w)) = f'(g(w))g'(w)\\) to make our lives easier, letting \\(f(x) = x^2 + 5\\) and \\(g(x) = x - 10\\) and recalling that \\(\\frac{\\partial}{\\partial x}x^2 = 2x\\):\n\\[\nL'(w) = \\frac{\\partial L(w)}{\\partial w} = 2(w - 10)\n\\]\nThe reason this matters / the reason it helps us is as follows. Recall how, in calculus class, we were able to use the derivative as a tool for finding minima and maxima of functions since the minima and maxima of these functions are precisely the values at which the function‚Äôs derivative is zero!\nBut what happens if we‚Äôre not exactly at a minimum, in this case? Calculus classes usually gloss over this question, since the answer would usually be ‚ÄúWhy do we care about points besides these optimal points? We can just compute the minimum and then we‚Äôre done! No need to worry about non-optimal points‚Äù\nHowever, when we work with more complicated models like neural networks, we don‚Äôt necessarily have an exact closed-form solution allowing us to take a derivative, set to zero, and solve. We therefore need to utilize numerical optimization approaches, which use the derivative as information telling us which direction we should move in and (approximately) how much we should move if we want to move from a non-optimal point towards the optimal point.\nLet‚Äôs pick three values of \\(w\\):\n\nA value below the minimizing value, \\(w_&lt; = 5\\),\nThe minimizing value itself, \\(w_0 = 10\\), and\nA value above the minimizing value, \\(w_&gt; = 19\\)\n\n\n\nCode\nlibrary(latex2exp)\nloss_deriv &lt;- function(w) {\n    return(2 * (w - 10))\n}\nw_vals &lt;- c(5, 10, 19)\nw_labels &lt;- factor(c(\"wlt\",\"w0\",\"wgt\"), levels=c(\"wlt\",\"w0\",\"wgt\"))\ndata_df &lt;- tibble(w=w_vals, label=w_labels)\ndata_df &lt;- data_df |&gt;\n  mutate(\n    loss = loss_fn(w),\n    deriv = loss_deriv(w),\n    second_deriv = 2\n  )\nggplot() +\n  stat_function(fun=loss_fn, linewidth=1) +\n  geom_point(\n    data=data_df,\n    aes(x=w, y=loss, color=factor(label)),\n    size=3\n  ) +\n  xlim(0, 20) +\n  scale_color_manual(\n    \"Our Three Values\",\n    values=c(\"wlt\"=cb_palette[1], \"w0\"=cb_palette[2], \"wgt\"=cb_palette[3]),\n    labels=c(\"wlt\"=TeX(\"$w_&lt; = 5$\"),\"w0\"=TeX(\"$w_0 = 10$\"),\"wgt\"=TeX(\"$w_&gt; = 19$\"))\n  ) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title=TeX(\"Points At, Below, and Above the Optimal $w^*$\"),\n    x = \"Parameter Value (w)\",\n    y = \"Loss L(w)\"\n  )\n\n\n\n\n\n\n\n\n\nAnd let‚Äôs evaluate both the loss function itself (loss) as well as the derivative of the loss function (deriv) at each point, looking closely at what these values tell us:\n\n\nCode\ndata_df\n\n\n\n\n\n\nw\nlabel\nloss\nderiv\nsecond_deriv\n\n\n\n\n5\nwlt\n30\n-10\n2\n\n\n10\nw0\n5\n0\n2\n\n\n19\nwgt\n86\n18\n2\n\n\n\n\n\n\nHere we can notice that, when we are at a value below the optimal value like \\(w_&lt;\\), the derivative has a negative sign, whereas at a value above the optimal value like \\(w_&gt;\\) the derivative has a positive sign. This relates to one of the natural interpretations of the derivative, one that your calculus class hopefully talked about, as the slope of the line tangent to the curve at that point. Adding to the previous plot of just the points, we can see this ‚Äúslope interpretation‚Äù in action: while the loss value tells us how high or low we are on the \\(y\\)-axis here, the deriv value tells us how steep the loss function is at this point. If we adopt the convention of drawing the tangent lines (for nonzero slopes) as vectors, we get a picture that looks like:\n\n\nCode\ntangent_at_x0 &lt;- function(x,x0) loss_deriv(x0)*(x - x0) + loss_fn(x0)\ntan_wlt &lt;- function(x) tangent_at_x0(x, data_df$w[1])\ntan_w0 &lt;- function(x) tangent_at_x0(x, data_df$w[2])\ntan_wgt &lt;- function(x) tangent_at_x0(x, data_df$w[3])\nslopes &lt;- round(c(\n  data_df$deriv[1],\n  data_df$deriv[2],\n  data_df$deriv[3]\n), 3)\nggplot() +\n  stat_function(fun=loss_fn, linewidth=1) +\n  geom_function(\n    fun=tan_wlt, aes(color=data_df$label[1]), linewidth=1,\n    xlim=c(0,7.5), arrow = arrow(length=unit(0.30,\"cm\"))\n  ) +\n  geom_function(\n    fun=tan_w0, aes(color=data_df$label[2]), linewidth=1,\n    xlim=c(8,12)\n  ) +\n  geom_function(\n    fun=tan_wgt, aes(color=data_df$label[3]), linewidth=1,\n    xlim=c(14.5,20), arrow = arrow(length=unit(0.30,\"cm\"), ends=\"first\")\n  ) +\n  geom_point(\n    data=data_df,\n    aes(x=w, y=loss, color=label),\n    size=3\n  ) +\n  xlim(0, 20) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"MLE Estimate\"\n  ) +\n  scale_color_manual(\n    \"Slope at w\",\n    values=c(\"wlt\"=cb_palette[1], \"w0\"=cb_palette[2], \"wgt\"=cb_palette[3]),\n    labels=c(\"wlt\"=TeX(\"$L'(w_&lt;) = -10$\"),\"w0\"=TeX(\"$L'(w_0) = 0$\"),\"wgt\"=TeX(\"$L'(w_&gt;) = 18$\"))\n  ) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title=TeX(\"Points At, Below, and Above the Optimal $w^*$\"),\n    x = \"Parameter Value (w)\",\n    y = \"Loss L(w)\"\n  )\n\n\n\n\n\nThe derivatives ‚Äúpoint‚Äù in the direction you should go to get to the maximum value, at which it has value zero\n\n\n\n\nAnd this shows us exactly why computing the derivative at a point away from the optimal value still helps us when it comes to numerical optimization, in two ways:\n\nFirst, the derivative at these points (literally) points us in the direction we should go in if we want to move towards the optimal value.\nThen (though I‚Äôll stop after this and let you see the effect of this point by going through the assignment‚Äôs different parts, since it relates to the second derivative rather than the first), notice also how the monotonicity properties of the quadratic function \\(f(x) = x^2\\) also tells us ‚Äúhow wrong‚Äù we are, in a sense:\n\n\\(w_&gt; = 19\\) is further away from the optimal point than \\(w_&lt; = 5\\), therefore\nThe magnitude of the derivative at \\(w_&gt;\\) (18) is greater than the magnitude of the derivative at \\(w_&lt;\\) (10).\n\n\nLike I mentioned, I‚Äôm stopping here since the later portions of the assignment dive into the information that the second derivative at a point can provide for our numerical optimizer, but to test your understanding you can imagine writing a third section here titled ‚ÄúHow The (Second) Derivative Helps Us‚Äù.\nFor example, the Huber Loss is often used as an alternative to both ‚Äúpure‚Äù quadratic loss and ‚Äúpure‚Äù absolute loss functions because it penalizes outliers less harshly than \\(f(x) = x^2\\) but more harshly than \\(f(x) = |x|\\). The following plot illustrates a ‚ÄúHuberized‚Äù version of Lab 1‚Äôs loss function, where values within \\(\\delta = 4\\) units of the optimal value \\(w^* = 10\\) are penalized quadratically, but values more than 4 units away from the optimal value are penalized linearly. Think through what is happening to the second derivative as we move from left to right here (relative to quadratic loss with \\(L''(w) = 2\\) and absolute loss with \\(L''(w) = 1\\)):\n\n\nCode\nabs_loss &lt;- function(w) {\n  return(abs(w - 10) + 5)\n}\ndelta &lt;- 4\nhuberized_loss &lt;- function(w) {\n  cases_result &lt;- ifelse(\n    abs(w - 10) &lt;= delta,\n    (1/2)*(w - 10)^2,\n    delta * (abs(w-10) - (1/2)*delta)\n  )\n  return(cases_result + 5)\n}\ntext_df &lt;- tibble::tribble(\n  ~x, ~y, ~label,\n  4, 100, \"‚Üê Linear\",\n  10, 100, \"Quadratic\",\n  16, 100, \"Linear ‚Üí\"\n)\nggplot() +\n  stat_function(\n    data=tibble(x=c(0, 10)),\n    fun=abs_loss,\n    aes(color='Absolute'),\n    linewidth=0.5\n  ) +\n  stat_function(\n    data=tibble(x=c(0, 10)),\n    fun=huberized_loss,\n    aes(color='Huber'),\n    linewidth=1\n  ) +\n  stat_function(\n    data=tibble(x=c(0,10)),\n    fun=loss_fn,\n    aes(color='Quadratic'),\n    linewidth=0.5\n  ) +\n  geom_vline(\n    xintercept=10 - delta,\n    linetype=\"dashed\",\n    linewidth=1,\n    color=cb_palette[2]\n  ) +\n  geom_vline(\n    xintercept=10+delta,\n    linetype=\"dashed\",\n    linewidth=1,\n    color=cb_palette[2]\n  ) +\n  geom_text(\n    data=text_df,\n    aes(x=x, y=y, label=label),\n    color=cb_palette[2]\n  ) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  scale_color_manual(\n    \"Loss Functions\",\n    values=c('Absolute'=cb_palette[1], 'Huber'=cb_palette[2], 'Quadratic'=cb_palette[3])\n    #labels=c('Absolute', 'Huber', 'Quadratic')\n  ) +\n  labs(\n    title = \"Loss Functions: Huber vs. 'Pure' Squared or Absolute\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "w02/slides.html#how-do-we-define-best",
    "href": "w02/slides.html#how-do-we-define-best",
    "title": "Week 2: Linear Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "w02/slides.html#predictive-example-advertising-effects",
    "href": "w02/slides.html#predictive-example-advertising-effects",
    "title": "Week 2: Linear Regression",
    "section": "Predictive Example: Advertising Effects",
    "text": "Predictive Example: Advertising Effects\n\nIndependent variable: $ put into advertisements\nDependent variable: Sales\nGoal: Figure out a good way to allocate an advertising budget\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\nlong_df &lt;- ad_df |&gt; pivot_longer(-c(id, sales), names_to=\"medium\", values_to=\"allocation\")\nlong_df |&gt; ggplot(aes(x=allocation, y=sales)) +\n  geom_point() +\n  facet_wrap(vars(medium), scales=\"free_x\") +\n  geom_smooth(method='lm', formula=\"y ~ x\") +\n  theme_dsan() +\n  labs(\n    x = \"Allocation ($1K)\",\n    y = \"Sales (1K Units)\"\n  )"
  },
  {
    "objectID": "w02/slides.html#explanatory-example-industrialization-effects",
    "href": "w02/slides.html#explanatory-example-industrialization-effects",
    "title": "Week 2: Linear Regression",
    "section": "Explanatory Example: Industrialization Effects",
    "text": "Explanatory Example: Industrialization Effects\n\n\nCode\nlibrary(tidyverse)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\nmil_plot &lt;- gdp_df |&gt; ggplot(aes(x=industrial, y=military)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Military Exports vs. Industrialization\",\n    x=\"Industrial Production (% of GDP)\",\n    y=\"Military Exports (% of All Exports)\"\n  )\nmil_plot"
  },
  {
    "objectID": "w02/slides.html#simple-linear-regression",
    "href": "w02/slides.html#simple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nFor now, we treat Newspaper, Radio, TV advertising separately: how much do sales increase per $1 into [medium]? (Later we‚Äôll consider them jointly: multiple regression)\nOur model:\n\\[\nY = \\underbrace{\\param{\\beta_0}}_{\\mathclap{\\text{Intercept}}} + \\underbrace{\\param{\\beta_1}}_{\\mathclap{\\text{Slope}}}X + \\varepsilon\n\\]\nThis model generates predictions via\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta_1}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nNote how these predictions will be wrong (unless the data is perfectly linear)\nWe‚Äôve accounted for this in our model (by including \\(\\varepsilon\\) term)!\nBut, we‚Äôd like to find estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that produce the ‚Äúleast wrong‚Äù predictions: motivates focus on residuals‚Ä¶"
  },
  {
    "objectID": "w02/slides.html#least-squares-minimizing-residuals",
    "href": "w02/slides.html#least-squares-minimizing-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Least Squares: Minimizing Residuals",
    "text": "Least Squares: Minimizing Residuals\nWhat can we optimize to ensure these residuals are as small as possible?\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_lg_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_lg_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nlarge_sum &lt;- sum(sim_lg_df$spread)\nwriteLines(fmt_decimal(large_sum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nlarge_sqsum &lt;- sum((sim_lg_df$spread)^2)\nwriteLines(fmt_decimal(large_sqsum))\n\n\n3.8405017200\n\n\n\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.05)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_sm_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_sm_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nsmall_rsum &lt;- sum(sim_sm_df$spread)\nwriteLines(fmt_decimal(small_rsum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nsmall_sqrsum &lt;- sum((sim_sm_df$spread)^2)\nwriteLines(fmt_decimal(small_sqrsum))\n\n\n1.9748635217"
  },
  {
    "objectID": "w02/slides.html#why-not-absolute-value",
    "href": "w02/slides.html#why-not-absolute-value",
    "title": "Week 2: Linear Regression",
    "section": "Why Not Absolute Value?",
    "text": "Why Not Absolute Value?\n\nTwo feasible ways to prevent positive and negative residuals cancelling out:\n\nAbsolute value \\(\\left|y - \\widehat{y}\\right|\\) or squaring \\(\\left( y - \\widehat{y} \\right)^2\\)\n\nBut remember that we‚Äôre aiming to minimize these residuals‚Ä¶\nGhost of calculus past üò±: which is differentiable everywhere?\n\n\n\n\n\nCode\nlibrary(latex2exp)\nx2_label &lt;- latex2exp(\"$f(x) = x^2$\")\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ .x^2, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=x2_label,\n    y=\"f(x)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Could use facet_grid() here, but it doesn't work too nicely with stat_function() :(\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ abs(.x), linewidth=g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=\"f(x) = |x|\",\n    y=\"f(x)\"\n  )"
  },
  {
    "objectID": "w02/slides.html#outliers-penalized-quadratically",
    "href": "w02/slides.html#outliers-penalized-quadratically",
    "title": "Week 2: Linear Regression",
    "section": "Outliers Penalized Quadratically",
    "text": "Outliers Penalized Quadratically\n\nImage Source"
  },
  {
    "objectID": "w02/slides.html#key-features-of-regression-line",
    "href": "w02/slides.html#key-features-of-regression-line",
    "title": "Week 2: Linear Regression",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta}_0}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta}_1}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\widehat{\\theta} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(~~\\overbrace{\\widehat{y}(x_i)}^{\\mathclap{\\small\\text{Predicted }y}} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^{2~} \\right]\n\\]"
  },
  {
    "objectID": "w02/slides.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "href": "w02/slides.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "title": "Week 2: Linear Regression",
    "section": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?",
    "text": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?\n\nImage Source"
  },
  {
    "objectID": "w02/slides.html#but-what-about-all-the-other-types-of-vars",
    "href": "w02/slides.html#but-what-about-all-the-other-types-of-vars",
    "title": "Week 2: Linear Regression",
    "section": "But‚Ä¶ What About All the Other Types of Vars?",
    "text": "But‚Ä¶ What About All the Other Types of Vars?\n\n5000: you saw, e.g., nominal, ordinal, cardinal vars\n5100: you wrestled with discrete vs.¬†continuous RVs\nGood News #1: Regression can handle all these types+more!\nGood News #2: Distinctions between classification and regression start to diminish as you learn fancier regression methods! (One key tool here: link functions)\nBy end of 5300 you should have something on your toolbelt for handling most cases like ‚ÄúI want to do [regression / classification], but my data is [not cardinal+continuous]‚Äù"
  },
  {
    "objectID": "w02/slides.html#a-sketch-hw-is-the-full-thing",
    "href": "w02/slides.html#a-sketch-hw-is-the-full-thing",
    "title": "Week 2: Linear Regression",
    "section": "A Sketch (HW is the Full Thing)",
    "text": "A Sketch (HW is the Full Thing)\n\nOLS for regression without intercept \\(\\param{\\beta_0}\\): Which line through origin best predicts \\(Y\\)?\n(Good practice + reminder of how restricted linear models are!)\n\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\n\n\nCode\nlibrary(latex2exp)\nset.seed(5300)\n# rand_slope &lt;- log(runif(80, min=0, max=1))\n# rand_slope[41:80] &lt;- -rand_slope[41:80]\n# rand_lines &lt;- tibble::tibble(\n#   id=1:80, slope=rand_slope, intercept=0\n# )\n# angles &lt;- runif(100, -pi/2, pi/2)\nangles &lt;- seq(from=-pi/2, to=pi/2, length.out=50)\npossible_lines &lt;- tibble::tibble(\n  slope=tan(angles), intercept=0\n)\nnum_points &lt;- 30\nx_vals &lt;- runif(num_points, 0, 1)\ny0_vals &lt;- 0.5 * x_vals + 0.25\ny_noise &lt;- rnorm(num_points, 0, 0.07)\ny_vals &lt;- y0_vals + y_noise\nrand_df &lt;- tibble::tibble(x=x_vals, y=y_vals)\ntitle_exp &lt;- latex2exp(\"Parameter Space ($\\\\beta_1$)\")\n# Main plot object\ngen_lines_plot &lt;- function(point_size=2.5) {\n  lines_plot &lt;- rand_df |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=point_size) +\n    geom_hline(yintercept=0, linewidth=1.5) +\n    geom_vline(xintercept=0, linewidth=1.5) +\n    # Point at origin\n    geom_point(data=data.frame(x=0, y=0), aes(x=x, y=y), size=4) +\n    xlim(-1,1) +\n    ylim(-1,1) +\n    # coord_fixed() +\n    theme_dsan_min(base_size=28)\n  return(lines_plot)\n}\nmain_lines_plot &lt;- gen_lines_plot()\nmain_lines_plot +\n  # Parameter space of possible lines\n  geom_abline(\n    data=possible_lines,\n    aes(slope=slope, intercept=intercept, color='possible'),\n    # linetype=\"dotted\",\n    # linewidth=0.75,\n    alpha=0.25\n  ) +\n  # True DGP\n  geom_abline(\n    aes(\n      slope=0.5,\n      intercept=0.25,\n      color='true'\n    ), linewidth=1, alpha=0.8\n  ) + \n  scale_color_manual(\n    element_blank(),\n    values=c('possible'=\"black\", 'true'=cb_palette[2]),\n    labels=c('possible'=\"Possible Fits\", 'true'=\"True DGP\")\n  ) +\n  remove_legend_title() +\n  labs(\n    title=title_exp\n  )"
  },
  {
    "objectID": "w02/slides.html#evaluating-with-residuals",
    "href": "w02/slides.html#evaluating-with-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Evaluating with Residuals",
    "text": "Evaluating with Residuals\n\n\n\n\nCode\nrc1_df &lt;- possible_lines |&gt; slice(n() - 14)\n# Predictions for this choice\nrc1_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc1_df$slope * x,\n  resid = y - y_pred\n)\nrc1_label &lt;- latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \",round(rc1_df$slope, 3),\"$\"))\nrc1_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc1_lines_plot +\n  geom_abline(\n    data=rc1_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[1]\n  ) +\n  geom_segment(\n    data=rc1_pred_df,\n    aes(x=x, y=y, xend=x, yend=y_pred),\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title = rc1_label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngen_resid_plot &lt;- function(pred_df) {\n  rc_rss &lt;- sum((pred_df$resid)^2)\n  rc_resid_label &lt;- latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \",round(rc_rss,3)))\n  rc_resid_plot &lt;- pred_df |&gt; ggplot(aes(x=x, y=resid)) +\n    geom_point(size=5) +\n    geom_hline(\n      yintercept=0,\n      color=cb_palette[1],\n      linewidth=1.5\n    ) +\n    geom_segment(\n      aes(xend=x, yend=0)\n    ) +\n    theme_dsan(base_size=28) +\n    theme(axis.line.x = element_blank()) +\n    labs(\n      title=rc_resid_label\n    )\n  return(rc_resid_plot)\n}\nrc1_resid_plot &lt;- gen_resid_plot(rc1_pred_df)\nrc1_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_df &lt;- possible_lines |&gt; slice(n() - 9)\n# Predictions for this choice\nrc2_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc2_df$slope * x,\n  resid = y - y_pred\n)\nrc2_label &lt;- latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \",round(rc2_df$slope,3),\"$\"))\nrc2_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc2_lines_plot +\n  geom_abline(\n    data=rc2_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[3]\n  ) +\n  geom_segment(\n    data=rc2_pred_df,\n    aes(\n      x=x, y=y, xend=x,\n      yend=ifelse(y_pred &lt;= 1, y_pred, Inf)\n    )\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title=rc2_label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_resid_plot &lt;- gen_resid_plot(rc2_pred_df)\nrc2_resid_plot"
  },
  {
    "objectID": "w02/slides.html#now-the-math",
    "href": "w02/slides.html#now-the-math",
    "title": "Week 2: Linear Regression",
    "section": "Now the Math",
    "text": "Now the Math\n\\[\n\\begin{align*}\n\\beta_1^* = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\widehat{y}_i - y_i)^2 \\right] = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right]\n\\end{align*}\n\\]\nWe can compute this derivative to obtain:\n\\[\n\\frac{\\partial}{\\partial\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right] = \\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\beta_1}(\\beta_1x_i - y_i)^2 = \\sum_{i=1}^{n}2(\\beta_1x_i - y_i)x_i\n\\]\nAnd our first-order condition means that:\n\\[\n\\sum_{i=1}^{n}2(\\beta_1^*x_i - y_i)x_i = 0 \\iff \\beta_1^*\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i \\iff \\boxed{\\beta_1^* = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2}}\n\\]"
  },
  {
    "objectID": "w02/slides.html#regression-r-vs.-statsmodels",
    "href": "w02/slides.html#regression-r-vs.-statsmodels",
    "title": "Week 2: Linear Regression",
    "section": "Regression: R vs.¬†statsmodels",
    "text": "Regression: R vs.¬†statsmodels\n\n\n\nIn (Base) R: lm()\n\n\n\nCode\nlin_model &lt;- lm(sales ~ TV, data=ad_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = sales ~ TV, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nGeneral syntax:\nlm(\n  dependent ~ independent + controls,\n  data = my_df\n)\n\n\nIn Python: smf.ols()\n\n\n\nCode\nimport statsmodels.formula.api as smf\nresults = smf.ols(\"sales ~ TV\", data=ad_df).fit()\nprint(results.summary(slim=True))\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nNo. Observations:                 200   F-statistic:                     312.1\nCovariance Type:            nonrobust   Prob (F-statistic):           1.47e-42\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nGeneral syntax:\nsmf.ols(\n  \"dependent ~ independent + controls\",\n  data = my_df\n)"
  },
  {
    "objectID": "w02/slides.html#interpreting-output",
    "href": "w02/slides.html#interpreting-output",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting Output",
    "text": "Interpreting Output\n\n\n\n\nCode\nmil_plot + theme_dsan(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngdp_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(gdp_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "w02/slides.html#zooming-in-coefficients",
    "href": "w02/slides.html#zooming-in-coefficients",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "w02/slides.html#zooming-in-significance",
    "href": "w02/slides.html#zooming-in-significance",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth, linetype=\"dashed\") +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"solid\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"dashed\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"solid\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))"
  },
  {
    "objectID": "w02/slides.html#the-residual-plot",
    "href": "w02/slides.html#the-residual-plot",
    "title": "Week 2: Linear Regression",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(gdp_model)\nggplot(gdp_resid_df, aes(x = industrial, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Military ~ Industrial\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )"
  },
  {
    "objectID": "w02/slides.html#q-q-plot",
    "href": "w02/slides.html#q-q-plot",
    "title": "Week 2: Linear Regression",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )"
  },
  {
    "objectID": "w02/slides.html#multiple-linear-regression",
    "href": "w02/slides.html#multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "w02/slides.html#visualizing-multiple-linear-regression",
    "href": "w02/slides.html#visualizing-multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided."
  },
  {
    "objectID": "w02/slides.html#interpreting-mlr",
    "href": "w02/slides.html#interpreting-mlr",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\n\n\nCode\nmlr_model &lt;- lm(sales ~ TV + radio + newspaper, data=ad_df)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w02/slides.html#but-wait",
    "href": "w02/slides.html#but-wait",
    "title": "Week 2: Linear Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nlr_model &lt;- lm(sales ~ newspaper, data=ad_df)\nprint(summary(lr_model))\n\n\n\nCall:\nlm(formula = sales ~ newspaper, data = ad_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!"
  },
  {
    "objectID": "w02/slides.html#correlations-among-features",
    "href": "w02/slides.html#correlations-among-features",
    "title": "Week 2: Linear Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\ncor(ad_df |&gt; select(-id))\n\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales"
  },
  {
    "objectID": "w02/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w02/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 2: Linear Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n(Preview for next week)\n\n\n\\[\nY = \\beta_0 + \\beta_1 \\times \\texttt{income}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Linear Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Linear Regression",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#how-do-we-define-best",
    "href": "w02/index.html#how-do-we-define-best",
    "title": "Week 2: Linear Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#predictive-example-advertising-effects",
    "href": "w02/index.html#predictive-example-advertising-effects",
    "title": "Week 2: Linear Regression",
    "section": "Predictive Example: Advertising Effects",
    "text": "Predictive Example: Advertising Effects\n\nIndependent variable: $ put into advertisements\nDependent variable: Sales\nGoal: Figure out a good way to allocate an advertising budget\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\n\n\nNew names:\nRows: 200 Columns: 5\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...1`\n\n\nCode\nlong_df &lt;- ad_df |&gt; pivot_longer(-c(id, sales), names_to=\"medium\", values_to=\"allocation\")\nlong_df |&gt; ggplot(aes(x=allocation, y=sales)) +\n  geom_point() +\n  facet_wrap(vars(medium), scales=\"free_x\") +\n  geom_smooth(method='lm', formula=\"y ~ x\") +\n  theme_dsan() +\n  labs(\n    x = \"Allocation ($1K)\",\n    y = \"Sales (1K Units)\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#explanatory-example-industrialization-effects",
    "href": "w02/index.html#explanatory-example-industrialization-effects",
    "title": "Week 2: Linear Regression",
    "section": "Explanatory Example: Industrialization Effects",
    "text": "Explanatory Example: Industrialization Effects\n\n\nCode\nlibrary(tidyverse)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\n\n\nRows: 89 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): country_code, country_name\ndbl (12): .rownames, services, agriculture, industrial, manufacturing, resou...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nmil_plot &lt;- gdp_df |&gt; ggplot(aes(x=industrial, y=military)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Military Exports vs. Industrialization\",\n    x=\"Industrial Production (% of GDP)\",\n    y=\"Military Exports (% of All Exports)\"\n  )\nmil_plot\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#simple-linear-regression",
    "href": "w02/index.html#simple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nFor now, we treat Newspaper, Radio, TV advertising separately: how much do sales increase per $1 into [medium]? (Later we‚Äôll consider them jointly: multiple regression)\nOur model:\n\\[\nY = \\underbrace{\\param{\\beta_0}}_{\\mathclap{\\text{Intercept}}} + \\underbrace{\\param{\\beta_1}}_{\\mathclap{\\text{Slope}}}X + \\varepsilon\n\\]\nThis model generates predictions via\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta_1}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nNote how these predictions will be wrong (unless the data is perfectly linear)\nWe‚Äôve accounted for this in our model (by including \\(\\varepsilon\\) term)!\nBut, we‚Äôd like to find estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that produce the ‚Äúleast wrong‚Äù predictions: motivates focus on residuals‚Ä¶",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#least-squares-minimizing-residuals",
    "href": "w02/index.html#least-squares-minimizing-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Least Squares: Minimizing Residuals",
    "text": "Least Squares: Minimizing Residuals\nWhat can we optimize to ensure these residuals are as small as possible?\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_lg_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_lg_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nlarge_sum &lt;- sum(sim_lg_df$spread)\nwriteLines(fmt_decimal(large_sum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nlarge_sqsum &lt;- sum((sim_lg_df$spread)^2)\nwriteLines(fmt_decimal(large_sqsum))\n\n\n3.8405017200\n\n\n\n\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.05)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_sm_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_sm_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nsmall_rsum &lt;- sum(sim_sm_df$spread)\nwriteLines(fmt_decimal(small_rsum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nsmall_sqrsum &lt;- sum((sim_sm_df$spread)^2)\nwriteLines(fmt_decimal(small_sqrsum))\n\n\n1.9748635217",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#why-not-absolute-value",
    "href": "w02/index.html#why-not-absolute-value",
    "title": "Week 2: Linear Regression",
    "section": "Why Not Absolute Value?",
    "text": "Why Not Absolute Value?\n\nTwo feasible ways to prevent positive and negative residuals cancelling out:\n\nAbsolute value \\(\\left|y - \\widehat{y}\\right|\\) or squaring \\(\\left( y - \\widehat{y} \\right)^2\\)\n\nBut remember that we‚Äôre aiming to minimize these residuals‚Ä¶\nGhost of calculus past üò±: which is differentiable everywhere?\n\n\n\n\n\nCode\nlibrary(latex2exp)\nx2_label &lt;- latex2exp(\"$f(x) = x^2$\")\n\n\nWarning in latex2exp(\"$f(x) = x^2$\"): 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ .x^2, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=x2_label,\n    y=\"f(x)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Could use facet_grid() here, but it doesn't work too nicely with stat_function() :(\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ abs(.x), linewidth=g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=\"f(x) = |x|\",\n    y=\"f(x)\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#outliers-penalized-quadratically",
    "href": "w02/index.html#outliers-penalized-quadratically",
    "title": "Week 2: Linear Regression",
    "section": "Outliers Penalized Quadratically",
    "text": "Outliers Penalized Quadratically\n\n\n\nImage Source",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#key-features-of-regression-line",
    "href": "w02/index.html#key-features-of-regression-line",
    "title": "Week 2: Linear Regression",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta}_0}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta}_1}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\widehat{\\theta} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(~~\\overbrace{\\widehat{y}(x_i)}^{\\mathclap{\\small\\text{Predicted }y}} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^{2~} \\right]\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "href": "w02/index.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "title": "Week 2: Linear Regression",
    "section": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?",
    "text": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?\n\n\n\nImage Source",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#but-what-about-all-the-other-types-of-vars",
    "href": "w02/index.html#but-what-about-all-the-other-types-of-vars",
    "title": "Week 2: Linear Regression",
    "section": "But‚Ä¶ What About All the Other Types of Vars?",
    "text": "But‚Ä¶ What About All the Other Types of Vars?\n\n5000: you saw, e.g., nominal, ordinal, cardinal vars\n5100: you wrestled with discrete vs.¬†continuous RVs\nGood News #1: Regression can handle all these types+more!\nGood News #2: Distinctions between classification and regression start to diminish as you learn fancier regression methods! (One key tool here: link functions)\nBy end of 5300 you should have something on your toolbelt for handling most cases like ‚ÄúI want to do [regression / classification], but my data is [not cardinal+continuous]‚Äù",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#a-sketch-hw-is-the-full-thing",
    "href": "w02/index.html#a-sketch-hw-is-the-full-thing",
    "title": "Week 2: Linear Regression",
    "section": "A Sketch (HW is the Full Thing)",
    "text": "A Sketch (HW is the Full Thing)\n\nOLS for regression without intercept \\(\\param{\\beta_0}\\): Which line through origin best predicts \\(Y\\)?\n(Good practice + reminder of how restricted linear models are!)\n\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\n\n\nCode\nlibrary(latex2exp)\nset.seed(5300)\n# rand_slope &lt;- log(runif(80, min=0, max=1))\n# rand_slope[41:80] &lt;- -rand_slope[41:80]\n# rand_lines &lt;- tibble::tibble(\n#   id=1:80, slope=rand_slope, intercept=0\n# )\n# angles &lt;- runif(100, -pi/2, pi/2)\nangles &lt;- seq(from=-pi/2, to=pi/2, length.out=50)\npossible_lines &lt;- tibble::tibble(\n  slope=tan(angles), intercept=0\n)\nnum_points &lt;- 30\nx_vals &lt;- runif(num_points, 0, 1)\ny0_vals &lt;- 0.5 * x_vals + 0.25\ny_noise &lt;- rnorm(num_points, 0, 0.07)\ny_vals &lt;- y0_vals + y_noise\nrand_df &lt;- tibble::tibble(x=x_vals, y=y_vals)\ntitle_exp &lt;- latex2exp(\"Parameter Space ($\\\\beta_1$)\")\n\n\nWarning in latex2exp(\"Parameter Space ($\\\\beta_1$)\"): 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\n# Main plot object\ngen_lines_plot &lt;- function(point_size=2.5) {\n  lines_plot &lt;- rand_df |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=point_size) +\n    geom_hline(yintercept=0, linewidth=1.5) +\n    geom_vline(xintercept=0, linewidth=1.5) +\n    # Point at origin\n    geom_point(data=data.frame(x=0, y=0), aes(x=x, y=y), size=4) +\n    xlim(-1,1) +\n    ylim(-1,1) +\n    # coord_fixed() +\n    theme_dsan_min(base_size=28)\n  return(lines_plot)\n}\nmain_lines_plot &lt;- gen_lines_plot()\nmain_lines_plot +\n  # Parameter space of possible lines\n  geom_abline(\n    data=possible_lines,\n    aes(slope=slope, intercept=intercept, color='possible'),\n    # linetype=\"dotted\",\n    # linewidth=0.75,\n    alpha=0.25\n  ) +\n  # True DGP\n  geom_abline(\n    aes(\n      slope=0.5,\n      intercept=0.25,\n      color='true'\n    ), linewidth=1, alpha=0.8\n  ) + \n  scale_color_manual(\n    element_blank(),\n    values=c('possible'=\"black\", 'true'=cb_palette[2]),\n    labels=c('possible'=\"Possible Fits\", 'true'=\"True DGP\")\n  ) +\n  remove_legend_title() +\n  labs(\n    title=title_exp\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#evaluating-with-residuals",
    "href": "w02/index.html#evaluating-with-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Evaluating with Residuals",
    "text": "Evaluating with Residuals\n\n\n\n\nCode\nrc1_df &lt;- possible_lines |&gt; slice(n() - 14)\n# Predictions for this choice\nrc1_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc1_df$slope * x,\n  resid = y - y_pred\n)\nrc1_label &lt;- latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \",round(rc1_df$slope, 3),\"$\"))\n\n\nWarning in latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \", round(rc1_df$slope, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc1_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc1_lines_plot +\n  geom_abline(\n    data=rc1_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[1]\n  ) +\n  geom_segment(\n    data=rc1_pred_df,\n    aes(x=x, y=y, xend=x, yend=y_pred),\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title = rc1_label\n  )\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngen_resid_plot &lt;- function(pred_df) {\n  rc_rss &lt;- sum((pred_df$resid)^2)\n  rc_resid_label &lt;- latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \",round(rc_rss,3)))\n  rc_resid_plot &lt;- pred_df |&gt; ggplot(aes(x=x, y=resid)) +\n    geom_point(size=5) +\n    geom_hline(\n      yintercept=0,\n      color=cb_palette[1],\n      linewidth=1.5\n    ) +\n    geom_segment(\n      aes(xend=x, yend=0)\n    ) +\n    theme_dsan(base_size=28) +\n    theme(axis.line.x = element_blank()) +\n    labs(\n      title=rc_resid_label\n    )\n  return(rc_resid_plot)\n}\nrc1_resid_plot &lt;- gen_resid_plot(rc1_pred_df)\n\n\nWarning in latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \", round(rc_rss, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc1_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_df &lt;- possible_lines |&gt; slice(n() - 9)\n# Predictions for this choice\nrc2_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc2_df$slope * x,\n  resid = y - y_pred\n)\nrc2_label &lt;- latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \",round(rc2_df$slope,3),\"$\"))\n\n\nWarning in latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \", round(rc2_df$slope, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc2_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc2_lines_plot +\n  geom_abline(\n    data=rc2_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[3]\n  ) +\n  geom_segment(\n    data=rc2_pred_df,\n    aes(\n      x=x, y=y, xend=x,\n      yend=ifelse(y_pred &lt;= 1, y_pred, Inf)\n    )\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title=rc2_label\n  )\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_resid_plot &lt;- gen_resid_plot(rc2_pred_df)\n\n\nWarning in latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \", round(rc_rss, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc2_resid_plot",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#now-the-math",
    "href": "w02/index.html#now-the-math",
    "title": "Week 2: Linear Regression",
    "section": "Now the Math",
    "text": "Now the Math\n\\[\n\\begin{align*}\n\\beta_1^* = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\widehat{y}_i - y_i)^2 \\right] = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right]\n\\end{align*}\n\\]\nWe can compute this derivative to obtain:\n\\[\n\\frac{\\partial}{\\partial\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right] = \\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\beta_1}(\\beta_1x_i - y_i)^2 = \\sum_{i=1}^{n}2(\\beta_1x_i - y_i)x_i\n\\]\nAnd our first-order condition means that:\n\\[\n\\sum_{i=1}^{n}2(\\beta_1^*x_i - y_i)x_i = 0 \\iff \\beta_1^*\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i \\iff \\boxed{\\beta_1^* = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2}}\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#regression-r-vs.-statsmodels",
    "href": "w02/index.html#regression-r-vs.-statsmodels",
    "title": "Week 2: Linear Regression",
    "section": "Regression: R vs.¬†statsmodels",
    "text": "Regression: R vs.¬†statsmodels\n\n\n\nIn (Base) R: lm()\n\n\n\nCode\nlin_model &lt;- lm(sales ~ TV, data=ad_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = sales ~ TV, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nGeneral syntax:\nlm(\n  dependent ~ independent + controls,\n  data = my_df\n)\n\n\nIn Python: smf.ols()\n\n\n\nCode\nimport statsmodels.formula.api as smf\nresults = smf.ols(\"sales ~ TV\", data=ad_df).fit()\nprint(results.summary(slim=True))\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nNo. Observations:                 200   F-statistic:                     312.1\nCovariance Type:            nonrobust   Prob (F-statistic):           1.47e-42\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nGeneral syntax:\nsmf.ols(\n  \"dependent ~ independent + controls\",\n  data = my_df\n)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#interpreting-output",
    "href": "w02/index.html#interpreting-output",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting Output",
    "text": "Interpreting Output\n\n\n\n\nCode\nmil_plot + theme_dsan(\"quarter\")\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngdp_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(gdp_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#zooming-in-coefficients",
    "href": "w02/index.html#zooming-in-coefficients",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#zooming-in-significance",
    "href": "w02/index.html#zooming-in-significance",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth, linetype=\"dashed\") +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"solid\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\n‚Ñπ Results may be unexpected or may change in future versions of ggplot2.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"dashed\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"solid\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\n‚Ñπ Results may be unexpected or may change in future versions of ggplot2.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-residual-plot",
    "href": "w02/index.html#the-residual-plot",
    "title": "Week 2: Linear Regression",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(gdp_model)\nggplot(gdp_resid_df, aes(x = industrial, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Military ~ Industrial\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#q-q-plot",
    "href": "w02/index.html#q-q-plot",
    "title": "Week 2: Linear Regression",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#multiple-linear-regression",
    "href": "w02/index.html#multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-multiple-linear-regression",
    "href": "w02/index.html#visualizing-multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#interpreting-mlr",
    "href": "w02/index.html#interpreting-mlr",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\n\n\nCode\nmlr_model &lt;- lm(sales ~ TV + radio + newspaper, data=ad_df)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#but-wait",
    "href": "w02/index.html#but-wait",
    "title": "Week 2: Linear Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nlr_model &lt;- lm(sales ~ newspaper, data=ad_df)\nprint(summary(lr_model))\n\n\n\nCall:\nlm(formula = sales ~ newspaper, data = ad_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#correlations-among-features",
    "href": "w02/index.html#correlations-among-features",
    "title": "Week 2: Linear Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\ncor(ad_df |&gt; select(-id))\n\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w02/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 2: Linear Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n(Preview for next week)\n\n\n\\[\nY = \\beta_0 + \\beta_1 \\times \\texttt{income}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\n\n\nRows: 400 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): Own, Student, Married, Region\ndbl (7): Income, Limit, Rating, Cards, Age, Education, Balance\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Linear Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w04/slides.html#what-we-have-thus-far",
    "href": "w04/slides.html#what-we-have-thus-far",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "What We Have Thus Far",
    "text": "What We Have Thus Far\n\nWe have a core model, regression, that we can build up into p much anything we want!\n\n\n\n\n\n\n\n\nClass Topic\nThis Video\n\n\n\n\nLinear regression\nPachelbel‚Äôs Canon in D (1m26s-1m46s)\n\n\nLogistic regression\nAdd swing:  (1m46s)\n\n\nNeural networks\n(triads \\(\\mapsto\\) 7th/9th chords) (5m24s-5m53s)"
  },
  {
    "objectID": "w04/slides.html#how-can-we-attain-hiromis-aura",
    "href": "w04/slides.html#how-can-we-attain-hiromis-aura",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "How Can We Attain Hiromi‚Äôs Aura?",
    "text": "How Can We Attain Hiromi‚Äôs Aura?\n\nIngredient 1: Lots of examples: find mysteries/questions you care about in the world and think of how regression could help us understand them!\nBut, Ingredient 2 is Generalized Linear Models (GLM), which I‚Äôll give an intro to on the board üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è"
  },
  {
    "objectID": "w04/slides.html#where-are-we-going-what-problems-are-we-solving",
    "href": "w04/slides.html#where-are-we-going-what-problems-are-we-solving",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Where Are We Going / What Problems Are We Solving?",
    "text": "Where Are We Going / What Problems Are We Solving?\n\nAnother nice property we have: OLS estimator is BLUE (Best Linear Unbiased Estimator) of conditional mean \\(\\mathbb{E}[Y \\mid X]\\)\nThe first problem we‚Äôll tackle is: as we move from linear models with these kinds of guarantees to fancier models with more uncertainties / ‚Äúpotholes‚Äù‚Ä¶ how do we ensure they still achieve want we want them to achieve?\nTldr: we can study more complex relationships between \\(X\\) and \\(Y\\) than linear ones, but we lose guarantees like ‚ÄúIf it‚Äôs linear, then it is [this]‚Äù: in other words, we lose this automatic generalizability\n(With great[er] power comes great[er] responsibility!)"
  },
  {
    "objectID": "w04/slides.html#the-level-2-goal-generalizability",
    "href": "w04/slides.html#the-level-2-goal-generalizability",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "The Level 2 Goal: Generalizability",
    "text": "The Level 2 Goal: Generalizability\n\n\n\n\n The Goal of Statistical Learning\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì"
  },
  {
    "objectID": "w04/slides.html#can-we-just-like-not",
    "href": "w04/slides.html#can-we-just-like-not",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Can We Just, Like, Not?",
    "text": "‚Ä¶Can We Just, Like, Not?\n\n\n\nWhat happens if we ‚Äúunleash‚Äù fancier non-linear models on data the same way we‚Äôve been using linear models?\nThe evil scourge of‚Ä¶ OVERFITTING (‚ö°Ô∏è a single overly-dramatic lightning bolt strikes the whiteboard behind me right at this exact moment what are the odds ‚ö°Ô∏è)\n\n\n\n\n\nYour computer is Yes Man\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nset.seed(5300)\nN &lt;- 30\nx_vals &lt;- runif(N, min=0, max=1)\ny_vals_raw &lt;- 3 * x_vals\ny_noise &lt;- rnorm(N, mean=0, sd=0.5)\ny_vals &lt;- y_vals_raw + y_noise\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2) +\n  stat_smooth(\n    method=\"lm\",\n    formula=\"y ~ x\",\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Linear Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶ but you‚Äôre only allowed to be linear!‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n\nCode\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2.5) +\n  stat_smooth(\n    method=\"lm\",\n    formula=y ~ poly(x, N, raw=TRUE),\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Polynomial Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n(Image Source)"
  },
  {
    "objectID": "w04/slides.html#memorizing-data-vs.-learning-the-relationship",
    "href": "w04/slides.html#memorizing-data-vs.-learning-the-relationship",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Memorizing Data vs.¬†Learning the Relationship",
    "text": "Memorizing Data vs.¬†Learning the Relationship\n\n\n\n\nCode\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHow have we measured ‚Äúgood‚Äù fit? High \\(R^2\\)? Low \\(RSS\\)?\n\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5679903\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.5638522\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0"
  },
  {
    "objectID": "w04/slides.html#accuracy-leadsto-5300-generalization",
    "href": "w04/slides.html#accuracy-leadsto-5300-generalization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization",
    "text": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\nCode\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  theme_classic() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Evaluation: Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8906269\n\n\nCode\nlin_rss_test\n\n\n[1] 0.139159\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.5237016\n\n\nCode\npoly_rss_test\n\n\n[1] 0.6060099"
  },
  {
    "objectID": "w04/slides.html#in-other-words",
    "href": "w04/slides.html#in-other-words",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "In Other Words‚Ä¶",
    "text": "In Other Words‚Ä¶\n\nImage source: circulated as secret shitposting among PhD students in seminars"
  },
  {
    "objectID": "w04/slides.html#ok-so-how-do-we-avoid-overfitting",
    "href": "w04/slides.html#ok-so-how-do-we-avoid-overfitting",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Ok So, How Do We Avoid Overfitting?",
    "text": "Ok So, How Do We Avoid Overfitting?\n\nThe gist: penalize model complexity\nOriginal optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y, \\widehat{y})\n\\]\nNew optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\lambda \\mathcal{L}(y, \\widehat{y}) + (1-\\lambda) \\mathsf{Complexity}(\\theta) \\right]\n\\]\nBut how do we measure, and penalize, ‚Äúcomplexity‚Äù?"
  },
  {
    "objectID": "w04/slides.html#regularization-measuring-and-penalizing-complexity",
    "href": "w04/slides.html#regularization-measuring-and-penalizing-complexity",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "w04/slides.html#lasso-and-elastic-net-regularization",
    "href": "w04/slides.html#lasso-and-elastic-net-regularization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 (Tibshirani 1996) is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\nEnsures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\)\n\nLeast Absolute Shrinkage and Selection Operator"
  },
  {
    "objectID": "w04/slides.html#training-vs.-test-data",
    "href": "w04/slides.html#training-vs.-test-data",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"TB\"\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n    N1[label=\"20%\"] N2[label=\"20%\"] N3[label=\"20%\"] N4[label=\"20%\"]\n    }\n    subgraph cluster_02 {\n        label=\"Test Set (20%)\"\n    N5[label=\"20%\",fillcolor=orange]\n    }\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "w04/slides.html#cross-validation",
    "href": "w04/slides.html#cross-validation",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "w04/slides.html#hyperparameters",
    "href": "w04/slides.html#hyperparameters",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several you‚Äôve already seen in e.g.¬†5000 ‚Äì can you name them?"
  },
  {
    "objectID": "w04/slides.html#hyperparameters-youve-already-seen",
    "href": "w04/slides.html#hyperparameters-youve-already-seen",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters You‚Äôve Already Seen",
    "text": "Hyperparameters You‚Äôve Already Seen\n\nUnsupervised Clustering: The number of clusters we want \\(K\\)\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "w04/slides.html#hyperparameter-selection",
    "href": "w04/slides.html#hyperparameter-selection",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, nodes per layer\nDecision Trees: Max tree depth, max features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM (Kingma and Ba 2017) for Neural Network learning rates)"
  },
  {
    "objectID": "w04/slides.html#now-what",
    "href": "w04/slides.html#now-what",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Now What?",
    "text": "‚Ä¶Now What?\n\nSo we‚Äôve got a trained model‚Ä¶\n\nData collected ‚úÖ\nLoss function chosen ‚úÖ\nGradient descent complete ‚úÖ\nHyperparameters tuned ‚úÖ\nGood \\(F_1\\) score on test data ‚úÖ\n\nWhat‚Äôs our next step?\n\nThis is where engineers and social scientists diverge‚Ä¶\nStay tuned!"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "References",
    "text": "References\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. ‚ÄúAdam: A Method for Stochastic Optimization.‚Äù arXiv. https://doi.org/10.48550/arXiv.1412.6980.\n\n\nTibshirani, Robert. 1996. ‚ÄúRegression Shrinkage and Selection via the Lasso.‚Äù Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267‚Äì88. https://www.jstor.org/stable/2346178."
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#what-we-have-thus-far",
    "href": "w04/index.html#what-we-have-thus-far",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "What We Have Thus Far",
    "text": "What We Have Thus Far\n\nWe have a core model, regression, that we can build up into p much anything we want!\n\n\n\n\n\n\n\n\nClass Topic\nThis Video\n\n\n\n\nLinear regression\nPachelbel‚Äôs Canon in D (1m26s-1m46s)\n\n\nLogistic regression\nAdd swing:  (1m46s)\n\n\nNeural networks\n(triads \\(\\mapsto\\) 7th/9th chords) (5m24s-5m53s)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#how-can-we-attain-hiromis-aura",
    "href": "w04/index.html#how-can-we-attain-hiromis-aura",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "How Can We Attain Hiromi‚Äôs Aura?",
    "text": "How Can We Attain Hiromi‚Äôs Aura?\n\nIngredient 1: Lots of examples: find mysteries/questions you care about in the world and think of how regression could help us understand them!\nBut, Ingredient 2 is Generalized Linear Models (GLM), which I‚Äôll give an intro to on the board üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#where-are-we-going-what-problems-are-we-solving",
    "href": "w04/index.html#where-are-we-going-what-problems-are-we-solving",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Where Are We Going / What Problems Are We Solving?",
    "text": "Where Are We Going / What Problems Are We Solving?\n\nAnother nice property we have: OLS estimator is BLUE (Best Linear Unbiased Estimator) of conditional mean \\(\\mathbb{E}[Y \\mid X]\\)\nThe first problem we‚Äôll tackle is: as we move from linear models with these kinds of guarantees to fancier models with more uncertainties / ‚Äúpotholes‚Äù‚Ä¶ how do we ensure they still achieve want we want them to achieve?\nTldr: we can study more complex relationships between \\(X\\) and \\(Y\\) than linear ones, but we lose guarantees like ‚ÄúIf it‚Äôs linear, then it is [this]‚Äù: in other words, we lose this automatic generalizability\n(With great[er] power comes great[er] responsibility!)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-level-2-goal-generalizability",
    "href": "w04/index.html#the-level-2-goal-generalizability",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "The Level 2 Goal: Generalizability",
    "text": "The Level 2 Goal: Generalizability\n\n\n\n\n\n\n The Goal of Statistical Learning\n\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#can-we-just-like-not",
    "href": "w04/index.html#can-we-just-like-not",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Can We Just, Like, Not?",
    "text": "‚Ä¶Can We Just, Like, Not?\n\n\n\nWhat happens if we ‚Äúunleash‚Äù fancier non-linear models on data the same way we‚Äôve been using linear models?\nThe evil scourge of‚Ä¶ OVERFITTING (‚ö°Ô∏è a single overly-dramatic lightning bolt strikes the whiteboard behind me right at this exact moment what are the odds ‚ö°Ô∏è)\n\n\n\n\n\nYour computer is Yes Man\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nset.seed(5300)\nN &lt;- 30\nx_vals &lt;- runif(N, min=0, max=1)\ny_vals_raw &lt;- 3 * x_vals\ny_noise &lt;- rnorm(N, mean=0, sd=0.5)\ny_vals &lt;- y_vals_raw + y_noise\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2) +\n  stat_smooth(\n    method=\"lm\",\n    formula=\"y ~ x\",\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Linear Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶ but you‚Äôre only allowed to be linear!‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n\nCode\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2.5) +\n  stat_smooth(\n    method=\"lm\",\n    formula=y ~ poly(x, N, raw=TRUE),\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Polynomial Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n\n(Image Source)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#memorizing-data-vs.-learning-the-relationship",
    "href": "w04/index.html#memorizing-data-vs.-learning-the-relationship",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Memorizing Data vs.¬†Learning the Relationship",
    "text": "Memorizing Data vs.¬†Learning the Relationship\n\n\n\n\nCode\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHow have we measured ‚Äúgood‚Äù fit? High \\(R^2\\)? Low \\(RSS\\)?\n\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5679903\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.5638522\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#accuracy-leadsto-5300-generalization",
    "href": "w04/index.html#accuracy-leadsto-5300-generalization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization",
    "text": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\nCode\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  theme_classic() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Evaluation: Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8906269\n\n\nCode\nlin_rss_test\n\n\n[1] 0.139159\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.5237016\n\n\nCode\npoly_rss_test\n\n\n[1] 0.6060099",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#in-other-words",
    "href": "w04/index.html#in-other-words",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "In Other Words‚Ä¶",
    "text": "In Other Words‚Ä¶\n\n\n\nImage source: circulated as secret shitposting among PhD students in seminars",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#ok-so-how-do-we-avoid-overfitting",
    "href": "w04/index.html#ok-so-how-do-we-avoid-overfitting",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Ok So, How Do We Avoid Overfitting?",
    "text": "Ok So, How Do We Avoid Overfitting?\n\nThe gist: penalize model complexity\nOriginal optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y, \\widehat{y})\n\\]\nNew optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\lambda \\mathcal{L}(y, \\widehat{y}) + (1-\\lambda) \\mathsf{Complexity}(\\theta) \\right]\n\\]\nBut how do we measure, and penalize, ‚Äúcomplexity‚Äù?",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#regularization-measuring-and-penalizing-complexity",
    "href": "w04/index.html#regularization-measuring-and-penalizing-complexity",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#lasso-and-elastic-net-regularization",
    "href": "w04/index.html#lasso-and-elastic-net-regularization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 (Tibshirani 1996) is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\nEnsures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#training-vs.-test-data",
    "href": "w04/index.html#training-vs.-test-data",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"TB\"\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n    N1[label=\"20%\"] N2[label=\"20%\"] N3[label=\"20%\"] N4[label=\"20%\"]\n    }\n    subgraph cluster_02 {\n        label=\"Test Set (20%)\"\n    N5[label=\"20%\",fillcolor=orange]\n    }\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#cross-validation",
    "href": "w04/index.html#cross-validation",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_04\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#hyperparameters",
    "href": "w04/index.html#hyperparameters",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several you‚Äôve already seen in e.g.¬†5000 ‚Äì can you name them?",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#hyperparameters-youve-already-seen",
    "href": "w04/index.html#hyperparameters-youve-already-seen",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters You‚Äôve Already Seen",
    "text": "Hyperparameters You‚Äôve Already Seen\n\nUnsupervised Clustering: The number of clusters we want \\(K\\)\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#hyperparameter-selection",
    "href": "w04/index.html#hyperparameter-selection",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, nodes per layer\nDecision Trees: Max tree depth, max features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM (Kingma and Ba 2017) for Neural Network learning rates)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#now-what",
    "href": "w04/index.html#now-what",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Now What?",
    "text": "‚Ä¶Now What?\n\nSo we‚Äôve got a trained model‚Ä¶\n\nData collected ‚úÖ\nLoss function chosen ‚úÖ\nGradient descent complete ‚úÖ\nHyperparameters tuned ‚úÖ\nGood \\(F_1\\) score on test data ‚úÖ\n\nWhat‚Äôs our next step?\n\nThis is where engineers and social scientists diverge‚Ä¶\nStay tuned!",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "References",
    "text": "References\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. ‚ÄúAdam: A Method for Stochastic Optimization.‚Äù arXiv. https://doi.org/10.48550/arXiv.1412.6980.\n\n\nTibshirani, Robert. 1996. ‚ÄúRegression Shrinkage and Selection via the Lasso.‚Äù Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267‚Äì88. https://www.jstor.org/stable/2346178.",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLeast Absolute Shrinkage and Selection Operator‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-regression-is-not",
    "href": "w03/index.html#what-regression-is-not",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "What Regression is Not",
    "text": "What Regression is Not\n\nFinal reminder that Regression, PCA have different goals!\nIf your goal was to, e.g., generate realistic \\((x,y)\\) pairs, then (mathematically) you want PCA! Roughly:\n\\[\n\\widehat{f}_{\\text{PCA}} = \\min_{\\mathbf{c}}\\left[ \\sum_{i=1}^{n} (\\widehat{x}_i(\\mathbf{c}) - x_i)^2 + (\\widehat{y}_i(\\mathbf{c}) - y_i)^2 \\right]\n\\]\nOur goal is a good predictor of \\(Y\\):\n\\[\n\\widehat{f}_{\\text{Reg}} = \\min_{\\beta_0, \\beta_1}\\left[ \\sum_{i=1}^{n} (\\widehat{y}_i(\\beta) - y_i)^2 \\right]\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#how-do-we-define-best",
    "href": "w03/index.html#how-do-we-define-best",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#multiple-linear-regression-mlr-model",
    "href": "w03/index.html#multiple-linear-regression-mlr-model",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Multiple Linear Regression (MLR) Model",
    "text": "Multiple Linear Regression (MLR) Model\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#visualizing-multiple-linear-regression",
    "href": "w03/index.html#visualizing-multiple-linear-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#interpreting-mlr",
    "href": "w03/index.html#interpreting-mlr",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\nCode\nmlr_model = smf.ols(\n  formula=\"sales ~ TV + radio + newspaper\",\n  data=ad_df\n)\nmlr_result = mlr_model.fit()\nprint(mlr_result.summary().tables[1])\n\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\n\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\n\n\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#but-wait",
    "href": "w03/index.html#but-wait",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\n# print(mlr_result.summary2(float_format='%.3f'))\nprint(mlr_result.summary2())\n\n\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.896   \nDependent Variable: sales            AIC:                780.3622\nDate:               2025-03-09 05:24 BIC:                793.5555\nNo. Observations:   200              Log-Likelihood:     -386.18 \nDf Model:           3                F-statistic:        570.3   \nDf Residuals:       196              Prob (F-statistic): 1.58e-96\nR-squared:          0.897            Scale:              2.8409  \n------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025  0.975]\n------------------------------------------------------------------\nIntercept       2.9389    0.3119   9.4223  0.0000   2.3238  3.5540\nTV              0.0458    0.0014  32.8086  0.0000   0.0430  0.0485\nradio           0.1885    0.0086  21.8935  0.0000   0.1715  0.2055\nnewspaper      -0.0010    0.0059  -0.1767  0.8599  -0.0126  0.0105\n-----------------------------------------------------------------\nOmnibus:             60.414       Durbin-Watson:          2.084  \nProb(Omnibus):       0.000        Jarque-Bera (JB):       151.241\nSkew:                -1.327       Prob(JB):               0.000  \nKurtosis:            6.332        Condition No.:          454    \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\nCode\nslr_model = smf.ols(\n  formula=\"sales ~ newspaper\",\n  data=ad_df\n)\nslr_result = slr_model.fit()\nprint(slr_result.summary2())\n\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.047    \nDependent Variable: sales            AIC:                1220.6714\nDate:               2025-03-09 05:24 BIC:                1227.2680\nNo. Observations:   200              Log-Likelihood:     -608.34  \nDf Model:           1                F-statistic:        10.89    \nDf Residuals:       198              Prob (F-statistic): 0.00115  \nR-squared:          0.052            Scale:              25.933   \n-------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nIntercept      12.3514    0.6214  19.8761  0.0000  11.1260  13.5769\nnewspaper       0.0547    0.0166   3.2996  0.0011   0.0220   0.0874\n------------------------------------------------------------------\nOmnibus:               6.231        Durbin-Watson:           1.983\nProb(Omnibus):         0.044        Jarque-Bera (JB):        5.483\nSkew:                  0.330        Prob(JB):                0.064\nKurtosis:              2.527        Condition No.:           65   \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#correlations-among-features",
    "href": "w03/index.html#correlations-among-features",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\nad_df.drop(columns=\"id\").corr()\n\n\n                 TV     radio  newspaper     sales\nTV         1.000000  0.054809   0.056648  0.782224\nradio      0.054809  1.000000   0.354104  0.576223\nnewspaper  0.056648  0.354104   1.000000  0.228299\nsales      0.782224  0.576223   0.228299  1.000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w03/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n\n\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 \\times \\texttt{income} \\\\\n&\\phantom{Y}\n\\end{align*}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\n\n\nRows: 400 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): Own, Student, Married, Region\ndbl (7): Income, Limit, Rating, Cards, Age, Education, Balance\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé\n(Dear future Jeff, let‚Äôs go through this on the board! Sincerely, past Jeff)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#from-mlr-to-logistic-regression",
    "href": "w03/index.html#from-mlr-to-logistic-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "From MLR to Logistic Regression",
    "text": "From MLR to Logistic Regression\n\nAs DSAN students, you know that we‚Äôre still sweeping classification under the rug!\nWe saw how to include binary/multiclass covariates, but what if the actual thing we‚Äôre trying to predict is binary?\nThe wrong approach is the ‚ÄúLinear Probability Model‚Äù:\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#credit-default",
    "href": "w03/index.html#credit-default",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Credit Default",
    "text": "Credit Default\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ndefault_df &lt;- read_csv(\"assets/Default.csv\") |&gt;\n  mutate(default_num = ifelse(default==\"Yes\",1,0))\n\n\nRows: 10000 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): default, student\ndbl (2): balance, income\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndefault_plot &lt;- default_df |&gt; ggplot(aes(x=balance, y=income, color=default, shape=default)) +\n  geom_point(alpha=0.6) +\n  theme_classic(base_size=16) +\n  labs(\n    title=\"Credit Defaults by Income and Account Balance\",\n    x = \"Account Balance\",\n    y = \"Income\"\n  )\ndefault_mplot &lt;- default_plot |&gt; ggMarginal(type=\"boxplot\", groupColour=FALSE, groupFill=TRUE)\ndefault_mplot",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#lines-vs.-sigmoids",
    "href": "w03/index.html#lines-vs.-sigmoids",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Lines vs.¬†Sigmoids(!)",
    "text": "Lines vs.¬†Sigmoids(!)\n\n\nHere‚Äôs what lines look like for this dataset:\n\n\nCode\n#lpm_model &lt;- lm(default ~ balance, data=default_df)\ndefault_df |&gt; ggplot(\n    aes(\n      x=balance, y=default_num\n    )\n  ) +\n  geom_point(aes(color=default)) +\n  stat_smooth(method=\"lm\", formula=y~x, color='black') +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs what sigmoids look like:\n\n\nCode\nlibrary(tidyverse)\nlogistic_model &lt;- glm(default_num ~ balance, family=binomial(link='logit'),data=default_df)\ndefault_df$predictions &lt;- predict(logistic_model, newdata = default_df, type = \"response\")\nmy_sigmoid &lt;- function(x) 1 / (1+exp(-x))\ndefault_df |&gt; ggplot(aes(x=balance, y=default_num)) +\n  #stat_function(fun=my_sigmoid) +\n  geom_point(aes(color=default)) +\n  geom_line(\n    data=default_df,\n    aes(x=balance, y=predictions),\n    linewidth=1\n  ) +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\n\\[\n\\log\\underbrace{\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\mathclap{\\smash{\\text{Odds Ratio}}}} = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#but-lets-derive-this",
    "href": "w03/index.html#but-lets-derive-this",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Let‚Äôs Derive This!",
    "text": "But Let‚Äôs Derive This!\n\\[\n\\begin{align*}\n\\Pr(Y \\mid X) &= \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\\\\n\\iff \\underbrace{\\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)}}_{\\text{Odds Ratio}} &= e^{\\beta_0 + \\beta_1X} \\\\\n\\iff \\underbrace{\\log\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\text{Log-Odds Ratio}} &= \\beta_0 + \\beta_1X\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#now-how-will-we-ever-find-good-parameter-values",
    "href": "w03/index.html#now-how-will-we-ever-find-good-parameter-values",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?",
    "text": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?\n\nIf only we had some sort of estimator‚Ä¶ One that would choose \\(\\beta_0\\) and \\(\\beta_1\\) so as to maximize the likelihood of seeing some data‚Ä¶\n\n\\[\nL(\\beta_0, \\beta_1) = \\prod_{\\{i \\mid y_i = 1\\}}\\Pr(Y = 1 \\mid X) \\prod_{\\{i \\mid y_i = 0\\}}(1-\\Pr(Y = 1 \\mid X))\n\\]\n\n(Much more on this later üò∏)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-interpretation-problem",
    "href": "w03/index.html#the-interpretation-problem",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "The Interpretation Problem",
    "text": "The Interpretation Problem\n\n\nCode\noptions(scipen = 999)\nprint(summary(logistic_model))\n\n\n\nCall:\nglm(formula = default_num ~ balance, family = binomial(link = \"logit\"), \n    data = default_df)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nSlope is no longer the same everywhere! It varies across different values of \\(x\\)‚Ä¶\nLet‚Äôs brainstorm some possible ways to make our lives easier when interpreting these coefficients!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#appendix-mlr-in-r",
    "href": "w03/index.html#appendix-mlr-in-r",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Appendix: MLR in R",
    "text": "Appendix: MLR in R\n\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\n\n\nNew names:\nRows: 200 Columns: 5\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...1`\n\n\nCode\nmlr_model &lt;- lm(\n  sales ~ TV + radio + newspaper,\n  data=ad_df\n)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422 &lt;0.0000000000000002 ***\nTV           0.045765   0.001395  32.809 &lt;0.0000000000000002 ***\nradio        0.188530   0.008611  21.893 &lt;0.0000000000000002 ***\nnewspaper   -0.001037   0.005871  -0.177                0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/slides.html#what-regression-is-not",
    "href": "w03/slides.html#what-regression-is-not",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "What Regression is Not",
    "text": "What Regression is Not\n\nFinal reminder that Regression, PCA have different goals!\nIf your goal was to, e.g., generate realistic \\((x,y)\\) pairs, then (mathematically) you want PCA! Roughly:\n\\[\n\\widehat{f}_{\\text{PCA}} = \\min_{\\mathbf{c}}\\left[ \\sum_{i=1}^{n} (\\widehat{x}_i(\\mathbf{c}) - x_i)^2 + (\\widehat{y}_i(\\mathbf{c}) - y_i)^2 \\right]\n\\]\nOur goal is a good predictor of \\(Y\\):\n\\[\n\\widehat{f}_{\\text{Reg}} = \\min_{\\beta_0, \\beta_1}\\left[ \\sum_{i=1}^{n} (\\widehat{y}_i(\\beta) - y_i)^2 \\right]\n\\]"
  },
  {
    "objectID": "w03/slides.html#how-do-we-define-best",
    "href": "w03/slides.html#how-do-we-define-best",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "w03/slides.html#multiple-linear-regression-mlr-model",
    "href": "w03/slides.html#multiple-linear-regression-mlr-model",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Multiple Linear Regression (MLR) Model",
    "text": "Multiple Linear Regression (MLR) Model\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "w03/slides.html#visualizing-multiple-linear-regression",
    "href": "w03/slides.html#visualizing-multiple-linear-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided."
  },
  {
    "objectID": "w03/slides.html#interpreting-mlr",
    "href": "w03/slides.html#interpreting-mlr",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\nCode\nmlr_model = smf.ols(\n  formula=\"sales ~ TV + radio + newspaper\",\n  data=ad_df\n)\nmlr_result = mlr_model.fit()\nprint(mlr_result.summary().tables[1])\n\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\n\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\n\n\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w03/slides.html#but-wait",
    "href": "w03/slides.html#but-wait",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\n# print(mlr_result.summary2(float_format='%.3f'))\nprint(mlr_result.summary2())\n\n\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.896   \nDependent Variable: sales            AIC:                780.3622\nDate:               2025-03-09 05:23 BIC:                793.5555\nNo. Observations:   200              Log-Likelihood:     -386.18 \nDf Model:           3                F-statistic:        570.3   \nDf Residuals:       196              Prob (F-statistic): 1.58e-96\nR-squared:          0.897            Scale:              2.8409  \n------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025  0.975]\n------------------------------------------------------------------\nIntercept       2.9389    0.3119   9.4223  0.0000   2.3238  3.5540\nTV              0.0458    0.0014  32.8086  0.0000   0.0430  0.0485\nradio           0.1885    0.0086  21.8935  0.0000   0.1715  0.2055\nnewspaper      -0.0010    0.0059  -0.1767  0.8599  -0.0126  0.0105\n-----------------------------------------------------------------\nOmnibus:             60.414       Durbin-Watson:          2.084  \nProb(Omnibus):       0.000        Jarque-Bera (JB):       151.241\nSkew:                -1.327       Prob(JB):               0.000  \nKurtosis:            6.332        Condition No.:          454    \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\nCode\nslr_model = smf.ols(\n  formula=\"sales ~ newspaper\",\n  data=ad_df\n)\nslr_result = slr_model.fit()\nprint(slr_result.summary2())\n\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.047    \nDependent Variable: sales            AIC:                1220.6714\nDate:               2025-03-09 05:23 BIC:                1227.2680\nNo. Observations:   200              Log-Likelihood:     -608.34  \nDf Model:           1                F-statistic:        10.89    \nDf Residuals:       198              Prob (F-statistic): 0.00115  \nR-squared:          0.052            Scale:              25.933   \n-------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nIntercept      12.3514    0.6214  19.8761  0.0000  11.1260  13.5769\nnewspaper       0.0547    0.0166   3.2996  0.0011   0.0220   0.0874\n------------------------------------------------------------------\nOmnibus:               6.231        Durbin-Watson:           1.983\nProb(Omnibus):         0.044        Jarque-Bera (JB):        5.483\nSkew:                  0.330        Prob(JB):                0.064\nKurtosis:              2.527        Condition No.:           65   \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!"
  },
  {
    "objectID": "w03/slides.html#correlations-among-features",
    "href": "w03/slides.html#correlations-among-features",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\nad_df.drop(columns=\"id\").corr()\n\n\n                 TV     radio  newspaper     sales\nTV         1.000000  0.054809   0.056648  0.782224\nradio      0.054809  1.000000   0.354104  0.576223\nnewspaper  0.056648  0.354104   1.000000  0.228299\nsales      0.782224  0.576223   0.228299  1.000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales"
  },
  {
    "objectID": "w03/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w03/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n\n\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 \\times \\texttt{income} \\\\\n&\\phantom{Y}\n\\end{align*}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé\n(Dear future Jeff, let‚Äôs go through this on the board! Sincerely, past Jeff)"
  },
  {
    "objectID": "w03/slides.html#from-mlr-to-logistic-regression",
    "href": "w03/slides.html#from-mlr-to-logistic-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "From MLR to Logistic Regression",
    "text": "From MLR to Logistic Regression\n\nAs DSAN students, you know that we‚Äôre still sweeping classification under the rug!\nWe saw how to include binary/multiclass covariates, but what if the actual thing we‚Äôre trying to predict is binary?\nThe wrong approach is the ‚ÄúLinear Probability Model‚Äù:\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]"
  },
  {
    "objectID": "w03/slides.html#credit-default",
    "href": "w03/slides.html#credit-default",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Credit Default",
    "text": "Credit Default\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ndefault_df &lt;- read_csv(\"assets/Default.csv\") |&gt;\n  mutate(default_num = ifelse(default==\"Yes\",1,0))\ndefault_plot &lt;- default_df |&gt; ggplot(aes(x=balance, y=income, color=default, shape=default)) +\n  geom_point(alpha=0.6) +\n  theme_classic(base_size=16) +\n  labs(\n    title=\"Credit Defaults by Income and Account Balance\",\n    x = \"Account Balance\",\n    y = \"Income\"\n  )\ndefault_mplot &lt;- default_plot |&gt; ggMarginal(type=\"boxplot\", groupColour=FALSE, groupFill=TRUE)\ndefault_mplot"
  },
  {
    "objectID": "w03/slides.html#lines-vs.-sigmoids",
    "href": "w03/slides.html#lines-vs.-sigmoids",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Lines vs.¬†Sigmoids(!)",
    "text": "Lines vs.¬†Sigmoids(!)\n\n\nHere‚Äôs what lines look like for this dataset:\n\n\nCode\n#lpm_model &lt;- lm(default ~ balance, data=default_df)\ndefault_df |&gt; ggplot(\n    aes(\n      x=balance, y=default_num\n    )\n  ) +\n  geom_point(aes(color=default)) +\n  stat_smooth(method=\"lm\", formula=y~x, color='black') +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs what sigmoids look like:\n\n\nCode\nlibrary(tidyverse)\nlogistic_model &lt;- glm(default_num ~ balance, family=binomial(link='logit'),data=default_df)\ndefault_df$predictions &lt;- predict(logistic_model, newdata = default_df, type = \"response\")\nmy_sigmoid &lt;- function(x) 1 / (1+exp(-x))\ndefault_df |&gt; ggplot(aes(x=balance, y=default_num)) +\n  #stat_function(fun=my_sigmoid) +\n  geom_point(aes(color=default)) +\n  geom_line(\n    data=default_df,\n    aes(x=balance, y=predictions),\n    linewidth=1\n  ) +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\n\\[\n\\log\\underbrace{\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\mathclap{\\smash{\\text{Odds Ratio}}}} = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]"
  },
  {
    "objectID": "w03/slides.html#but-lets-derive-this",
    "href": "w03/slides.html#but-lets-derive-this",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Let‚Äôs Derive This!",
    "text": "But Let‚Äôs Derive This!\n\\[\n\\begin{align*}\n\\Pr(Y \\mid X) &= \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\\\\n\\iff \\underbrace{\\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)}}_{\\text{Odds Ratio}} &= e^{\\beta_0 + \\beta_1X} \\\\\n\\iff \\underbrace{\\log\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\text{Log-Odds Ratio}} &= \\beta_0 + \\beta_1X\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#now-how-will-we-ever-find-good-parameter-values",
    "href": "w03/slides.html#now-how-will-we-ever-find-good-parameter-values",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?",
    "text": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?\n\nIf only we had some sort of estimator‚Ä¶ One that would choose \\(\\beta_0\\) and \\(\\beta_1\\) so as to maximize the likelihood of seeing some data‚Ä¶\n\n\\[\nL(\\beta_0, \\beta_1) = \\prod_{\\{i \\mid y_i = 1\\}}\\Pr(Y = 1 \\mid X) \\prod_{\\{i \\mid y_i = 0\\}}(1-\\Pr(Y = 1 \\mid X))\n\\]\n\n(Much more on this later üò∏)"
  },
  {
    "objectID": "w03/slides.html#the-interpretation-problem",
    "href": "w03/slides.html#the-interpretation-problem",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "The Interpretation Problem",
    "text": "The Interpretation Problem\n\n\nCode\noptions(scipen = 999)\nprint(summary(logistic_model))\n\n\n\nCall:\nglm(formula = default_num ~ balance, family = binomial(link = \"logit\"), \n    data = default_df)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nSlope is no longer the same everywhere! It varies across different values of \\(x\\)‚Ä¶\nLet‚Äôs brainstorm some possible ways to make our lives easier when interpreting these coefficients!"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "w03/slides.html#appendix-mlr-in-r",
    "href": "w03/slides.html#appendix-mlr-in-r",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Appendix: MLR in R",
    "text": "Appendix: MLR in R\n\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\nmlr_model &lt;- lm(\n  sales ~ TV + radio + newspaper,\n  data=ad_df\n)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422 &lt;0.0000000000000002 ***\nTV           0.045765   0.001395  32.809 &lt;0.0000000000000002 ***\nradio        0.188530   0.008611  21.893 &lt;0.0000000000000002 ***\nnewspaper   -0.001037   0.005871  -0.177                0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#reminder-w04-new-goal-generalizability",
    "href": "w05/index.html#reminder-w04-new-goal-generalizability",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n\n\n The Goal of Statistical Learning\n\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#our-working-dgp",
    "href": "w05/index.html#our-working-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Our Working DGP",
    "text": "Our Working DGP\n\nEach country \\(i\\) has a certain \\(x_i = \\texttt{gdp\\_per\\_capita}_i\\)\nThey spend some portion of it on healthcare each year, which translates (based on the country‚Äôs healthcare system) into health outcomes \\(y_i\\)\nWe operationalize these health outcomes as \\(y_i = \\texttt{DALY}_i\\): Disability Adjusted Life Years, cross-nationally-standardized ‚Äúlost years of minimally-healthy life‚Äù\n\n\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(plotly) |&gt; suppressPackageStartupMessages()\ndaly_df &lt;- read_csv(\"assets/dalys_cleaned.csv\")\n\n\nRows: 162 Columns: 6\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): name, gdp_cap\ndbl (4): dalys_pc, population, gdp_pc_clean, log_dalys_pc\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndaly_df &lt;- daly_df |&gt; mutate(\n  gdp_pc_1k=gdp_pc_clean/1000\n)\nmodel_labels &lt;- labs(\n  x=\"GDP per capita ($1K PPP, 2021)\",\n  y=\"Log(DALYs/n)\",\n  title=\"Decrease in DALYs as GDP/n Increases\"\n)\ndaly_plot &lt;- daly_df |&gt; ggplot(aes(x=gdp_pc_1k, y=log_dalys_pc, label=name)) +\n  geom_point() +\n  # geom_smooth(method=\"loess\", formula=y ~ x) +\n  geom_smooth(method=\"lm\", formula=y ~ poly(x,5), se=FALSE) +\n  theme_dsan(base_size=14) +\n  model_labels\nggplotly(daly_plot)\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\leadsto Y = &10.58 - 0.2346 X + 0.01396 X^2 \\\\\n&- 0.0004 X^3 + 0.000005 X^4 \\\\\n&- 0.00000002 X^5 + \\varepsilon\n\\end{align*}\n\\]\n\n\nCode\neval_fitted_poly &lt;- function(x) {\n  coefs &lt;- c(\n    10.58,  -0.2346, 0.01396,\n    -0.0004156, 0.0000053527, -0.0000000244\n  )\n  x_terms &lt;- c(x^0, x^1, x^2, x^3, x^4, x^5)\n  dot_prod &lt;- sum(coefs * x_terms)\n  return(dot_prod)\n}\nN &lt;- 500\nx_vals &lt;- runif(N, min=0, max=90)\ny_vals &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\nsim_df &lt;- tibble(gdpc=x_vals, ldalys=y_vals)\nggplot() +\n  geom_line(data=sim_df, aes(x=gdpc, y=ldalys)) +\n  geom_point(data=daly_df, aes(x=gdp_pc_1k, y=log_dalys_pc)) +\n  theme_dsan() +\n  model_labels",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#the-true-model",
    "href": "w05/index.html#the-true-model",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The ‚ÄúTrue‚Äù Model",
    "text": "The ‚ÄúTrue‚Äù Model\n\nFrom here onwards, we adopt this as our ‚Äútrue‚Äù model, for pedagogical purposes!\nMeaning: we use this model to get a sense for how‚Ä¶\n\nCV can ‚Äúforesee‚Äù test error \\(\\leadsto\\) confidence in CV\nRegularization can penalize overly-complex models \\(\\leadsto\\) confidence in LASSO\n\nIn the real world we don‚Äôt know the DGP!\n\n\\(\\implies\\) We build our confidence here, then take off the training wheels irl: use CV/Regularization in hopes they can help us ‚Äúuncover‚Äù the unknown DGP\n\n\n\n\nCode\nrun_dgp &lt;- function(world_label=\"Sim\", N=60, x_max=90) {\n  x_vals &lt;- runif(N, min=0, max=x_max)\n  y_raw &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\n  y_noise &lt;- rnorm(N, mean=0, sd=0.8)\n  y_vals &lt;- y_raw + y_noise\n  sim_df &lt;- tibble(\n    gdpc=x_vals,\n    ldalys=y_vals,\n    world=world_label\n  )\n  return(sim_df)\n}\ndf1 &lt;- run_dgp(\"World 1\")\ndf2 &lt;- run_dgp(\"World 2\")\ndf3 &lt;- run_dgp(\"World 3\")\ndgp_df &lt;- bind_rows(df1, df2, df3)\ndgp_df |&gt; ggplot(aes(x=gdpc, y=ldalys)) +\n  geom_point(aes(color=world)) +\n  facet_wrap(vars(world)) +\n  theme_dsan(base_size=22) +\n  remove_legend() +\n  model_labels +\n  labs(title=\"Three Possible Realizations of our DGP\")",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#training-vs.-test-data",
    "href": "w05/index.html#training-vs.-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\nWe introduced this as a first step towards tackling the scourge of overfitting!\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_02\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%\n\n\n\n\n\n\n\n\n\nTraining Set \\(\\leadsto\\) Training Error, Test Set \\(\\leadsto\\) Test Error\nSo‚Ä¶ what‚Äôs the issue? Why do we need to complicate this picture?",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#the-chilling-truth-behind-test-data",
    "href": "w05/index.html#the-chilling-truth-behind-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Chilling Truth Behind Test Data ü´£",
    "text": "The Chilling Truth Behind Test Data ü´£\n\nScience-wise, technically, once you use the test set, you should stop working\nFull gory details in fancy books (Hume (1760) \\(\\rightarrow\\) Popper (1934)), but the essence is captured by visualizing scientific inference (and statistical learning!) like:\n\n\n\n\n\n\n\nSo, what do we do? Use \\(\\mathbf{D}_{\\text{Tr}}\\) along with knowledge of issues like overfitting to estimate test error!\nFulfills our goal: find model which best predicts \\(Y\\) from \\(X\\) for unobserved data",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#the-validation-set-approach",
    "href": "w05/index.html#the-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Validation Set Approach",
    "text": "The Validation Set Approach\n\n‚ö†Ô∏è Remember: under our new goal, ‚Äúgood‚Äù models = models that generalize well!\nOptimizing over test set violates scientific inference axioms\nInstead, optimize over training data by ‚Äúholding out‚Äù some portion of it‚Ä¶ Enter the Validation Set:\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#evaluating-one-model-validation-set-approach",
    "href": "w05/index.html#evaluating-one-model-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Evaluating One Model: Validation Set Approach",
    "text": "Evaluating One Model: Validation Set Approach\n\n Randomly pick 20% of \\(\\mathbf{D}_{\\text{train}}\\) as sub-training set \\(\\mathbf{D}_{\\text{SubTr}}\\)\n The other 80% becomes validation set \\(\\mathbf{D}_{\\text{Val}}\\)\n Train model on \\(\\mathbf{D}_{\\text{SubTr}}\\), then evaluate using \\(\\mathbf{D}_{\\text{Val}}\\), to produce validation error \\(\\boxed{\\varepsilon_{\\text{Val}} = \\widehat{\\text{Err}}_{\\text{Test}}}\\)\n\\(\\varepsilon_{\\text{Val}}\\) gives us an estimate of the test error\n\n\\(\\implies\\) this is what we want to optimize, in place of training error, for our new goal üòé!",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#how-does-it-do-for-our-dgp",
    "href": "w05/index.html#how-does-it-do-for-our-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "How Does It Do for Our DGP?",
    "text": "How Does It Do for Our DGP?\n\nRecall that the ‚Äútrue‚Äù degree is 5, but that you‚Äôre not supposed to know that!\n\n\n\n\n\nCode\nlibrary(boot)\nset.seed(5300)\nsim200_df &lt;- run_dgp(\n  world_label=\"N=200\", N=200, x_max=100\n)\nsim1k_df &lt;- run_dgp(\n  world_label=\"N=1000\", N=1000, x_max=100\n)\ncompute_deltas &lt;- function(df, min_deg=1, max_deg=12) {\n  cv_deltas &lt;- c()\n  for (i in min_deg:max_deg) {\n    cur_poly &lt;- glm(ldalys ~ poly(gdpc, i), data=df)\n    cur_poly_cv_result &lt;- cv.glm(data=df, glmfit=cur_poly, K=5)\n    cur_cv_adj &lt;- cur_poly_cv_result$delta[1]\n    cv_deltas &lt;- c(cv_deltas, cur_cv_adj)\n  }\n  return(cv_deltas)\n}\nsim200_deltas &lt;- compute_deltas(sim200_df)\nsim200_delta_df &lt;- tibble(degree=1:12, delta=sim200_deltas)\nsim200_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 200\")\n\n\n\n\n\nHere, validation error fails to capture ‚Äòtrue‚Äô model (likely culprits: low \\(N\\), high noise‚Ä¶)\n\n\n\n\n\nPossible resolution: [See coming slides!]\n\n\n\n\nCode\nsim1k_deltas &lt;- compute_deltas(sim1k_df)\nsim1k_delta_df &lt;- tibble(degree=1:12, delta=sim1k_deltas)\nsim1k_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 1000\")\n\n\n\n\n\nHere, validation error fails to sharply distinguish \\(d \\in \\{5, 6, 7, 8\\}\\)\n\n\n\n\n\nPossible resolution: ‚Äúone standard error rule‚Äù",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#optimizing-over-many-models",
    "href": "w05/index.html#optimizing-over-many-models",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Optimizing over Many Models",
    "text": "Optimizing over Many Models\n\n Let \\(\\mathfrak{M} = (\\mathcal{M}_1, \\ldots, \\mathcal{M}_D)\\) be a set of \\(D\\) different models\n\nEx: \\(\\mathcal{M}_1\\) could be a linear model, \\(\\mathcal{M}_2\\) a quadratic model, \\(\\mathcal{M}_3\\) a cubic model, and so on‚Ä¶\n\n For each \\(\\mathcal{M}_i \\in \\mathfrak{M}\\) (and for given training data \\(\\mathbf{D}_{\\text{Tr}}\\)):\n\nUse [Insert Validation Approach] to derive \\(\\varepsilon_i\\)\n\n Model \\(\\mathcal{M}_i\\) with lowest \\(\\varepsilon_i\\) wins!\nWe are now cooking with gas! We can use CV to optimize over e.g.¬†HYPERPARAMETERS ü§Ø (that we had to just kind of‚Ä¶ guess before)!",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#cooking-with-even-more-gas",
    "href": "w05/index.html#cooking-with-even-more-gas",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Cooking with Even More Gas!",
    "text": "Cooking with Even More Gas!\n(Âä† even more Ê≤π!)\n\nIt turns out that, for the purposes of estimating test error (aka, estimating how well the fitted model will generalize), Validation Set Approach is the worst approach (still good, just, least good!)\nTo see its limitations, consider: is there something special about the 20% of data we selected for the validation set?\nAnswer: No! It was literally randomly selected!\n\\(\\implies\\) Any other 20% ‚Äúchunk‚Äù (which we‚Äôll call a fold from now on) could work just as well",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#fold-cross-validation",
    "href": "w05/index.html#fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "5-Fold Cross-Validation",
    "text": "5-Fold Cross-Validation\n\nSecretly, by choosing a 20% fold to be the validation set earlier, I was priming you for 5-fold cross-validation!\n\n\n\n\nImage Source",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#k-fold-cross-validation",
    "href": "w05/index.html#k-fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "\\(K\\)-Fold Cross-Validation",
    "text": "\\(K\\)-Fold Cross-Validation\n\n5 was an arbitrary choice!\n\n(Though, one with important and desirable statistical properties, which we‚Äôll get to ASAP)\n\nIn general, \\(K\\)-Fold CV Estimator \\(\\varepsilon_{(K)} = \\widehat{\\text{Err}}_{\\text{Test}}\\) given by\n\n\\[\n\\varepsilon_{(K)} = \\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{\\text{Val}}_{i}\n\\]",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#leave-one-out-cross-validation-loocv",
    "href": "w05/index.html#leave-one-out-cross-validation-loocv",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\\(K\\)-Fold CV with \\(K := N\\)\nProduces the maximum possible number of terms in the summation that leads to \\(\\varepsilon_{(K)}\\)\n\ni.e., \\(\\varepsilon_{(N)}\\) is an average over the maximum possible number of models\n\nSo‚Ä¶ does this mean it‚Äôs the best choice?\nHint: How ‚Äúdifferent‚Äù are any two models used in the sum?\n\n\\[\n\\begin{align*}\n\\varepsilon_{(N)} =~ & \\text{Err}(\\mathbf{D}_1 \\mid \\mathbf{D}_{2:100}) + \\text{Err}(\\mathbf{D}_2 \\mid \\mathbf{D}_1, \\mathbf{D}_{3:100}) \\\\\n&+ \\text{Err}(\\mathbf{D}_3 \\mid \\mathbf{D}_{1:2}, \\mathbf{D}_{4:100}) + \\cdots + \\text{Err}(\\mathbf{D}_{100} \\mid \\mathbf{D}_{1:99})\n\\end{align*}\n\\]\n\nThe terms here are highly correlated!\nFun math results around how \\(K = 5\\) or \\(K = 10\\) can best strike a balance between overly-correlated terms (\\(K = N\\)) and not averaging enough models (\\(K = 2\\))!",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "References",
    "text": "References\n\n\nHume, David. 1760. An Enquiry Concerning Human Understanding. Simon and Schuster.\n\n\nPopper, Karl R. 1934. The Logic of Scientific Discovery. Psychology Press.",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/slides.html#reminder-w04-new-goal-generalizability",
    "href": "w05/slides.html#reminder-w04-new-goal-generalizability",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n The Goal of Statistical Learning\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì"
  },
  {
    "objectID": "w05/slides.html#our-working-dgp",
    "href": "w05/slides.html#our-working-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Our Working DGP",
    "text": "Our Working DGP\n\nEach country \\(i\\) has a certain \\(x_i = \\texttt{gdp\\_per\\_capita}_i\\)\nThey spend some portion of it on healthcare each year, which translates (based on the country‚Äôs healthcare system) into health outcomes \\(y_i\\)\nWe operationalize these health outcomes as \\(y_i = \\texttt{DALY}_i\\): Disability Adjusted Life Years, cross-nationally-standardized ‚Äúlost years of minimally-healthy life‚Äù\n\n\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(plotly) |&gt; suppressPackageStartupMessages()\ndaly_df &lt;- read_csv(\"assets/dalys_cleaned.csv\")\ndaly_df &lt;- daly_df |&gt; mutate(\n  gdp_pc_1k=gdp_pc_clean/1000\n)\nmodel_labels &lt;- labs(\n  x=\"GDP per capita ($1K PPP, 2021)\",\n  y=\"Log(DALYs/n)\",\n  title=\"Decrease in DALYs as GDP/n Increases\"\n)\ndaly_plot &lt;- daly_df |&gt; ggplot(aes(x=gdp_pc_1k, y=log_dalys_pc, label=name)) +\n  geom_point() +\n  # geom_smooth(method=\"loess\", formula=y ~ x) +\n  geom_smooth(method=\"lm\", formula=y ~ poly(x,5), se=FALSE) +\n  theme_dsan(base_size=14) +\n  model_labels\nggplotly(daly_plot)\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\leadsto Y = &10.58 - 0.2346 X + 0.01396 X^2 \\\\\n&- 0.0004 X^3 + 0.000005 X^4 \\\\\n&- 0.00000002 X^5 + \\varepsilon\n\\end{align*}\n\\]\n\n\nCode\neval_fitted_poly &lt;- function(x) {\n  coefs &lt;- c(\n    10.58,  -0.2346, 0.01396,\n    -0.0004156, 0.0000053527, -0.0000000244\n  )\n  x_terms &lt;- c(x^0, x^1, x^2, x^3, x^4, x^5)\n  dot_prod &lt;- sum(coefs * x_terms)\n  return(dot_prod)\n}\nN &lt;- 500\nx_vals &lt;- runif(N, min=0, max=90)\ny_vals &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\nsim_df &lt;- tibble(gdpc=x_vals, ldalys=y_vals)\nggplot() +\n  geom_line(data=sim_df, aes(x=gdpc, y=ldalys)) +\n  geom_point(data=daly_df, aes(x=gdp_pc_1k, y=log_dalys_pc)) +\n  theme_dsan() +\n  model_labels"
  },
  {
    "objectID": "w05/slides.html#the-true-model",
    "href": "w05/slides.html#the-true-model",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The ‚ÄúTrue‚Äù Model",
    "text": "The ‚ÄúTrue‚Äù Model\n\nFrom here onwards, we adopt this as our ‚Äútrue‚Äù model, for pedagogical purposes!\nMeaning: we use this model to get a sense for how‚Ä¶\n\nCV can ‚Äúforesee‚Äù test error \\(\\leadsto\\) confidence in CV\nRegularization can penalize overly-complex models \\(\\leadsto\\) confidence in LASSO\n\nIn the real world we don‚Äôt know the DGP!\n\n\\(\\implies\\) We build our confidence here, then take off the training wheels irl: use CV/Regularization in hopes they can help us ‚Äúuncover‚Äù the unknown DGP\n\n\n\n\nCode\nrun_dgp &lt;- function(world_label=\"Sim\", N=60, x_max=90) {\n  x_vals &lt;- runif(N, min=0, max=x_max)\n  y_raw &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\n  y_noise &lt;- rnorm(N, mean=0, sd=0.8)\n  y_vals &lt;- y_raw + y_noise\n  sim_df &lt;- tibble(\n    gdpc=x_vals,\n    ldalys=y_vals,\n    world=world_label\n  )\n  return(sim_df)\n}\ndf1 &lt;- run_dgp(\"World 1\")\ndf2 &lt;- run_dgp(\"World 2\")\ndf3 &lt;- run_dgp(\"World 3\")\ndgp_df &lt;- bind_rows(df1, df2, df3)\ndgp_df |&gt; ggplot(aes(x=gdpc, y=ldalys)) +\n  geom_point(aes(color=world)) +\n  facet_wrap(vars(world)) +\n  theme_dsan(base_size=22) +\n  remove_legend() +\n  model_labels +\n  labs(title=\"Three Possible Realizations of our DGP\")"
  },
  {
    "objectID": "w05/slides.html#training-vs.-test-data",
    "href": "w05/slides.html#training-vs.-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\nWe introduced this as a first step towards tackling the scourge of overfitting!\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%\n\n\n\n\n\n\n\n\n\nTraining Set \\(\\leadsto\\) Training Error, Test Set \\(\\leadsto\\) Test Error\nSo‚Ä¶ what‚Äôs the issue? Why do we need to complicate this picture?"
  },
  {
    "objectID": "w05/slides.html#the-chilling-truth-behind-test-data",
    "href": "w05/slides.html#the-chilling-truth-behind-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Chilling Truth Behind Test Data ü´£",
    "text": "The Chilling Truth Behind Test Data ü´£\n\nScience-wise, technically, once you use the test set, you should stop working\nFull gory details in fancy books (Hume (1760) \\(\\rightarrow\\) Popper (1934)), but the essence is captured by visualizing scientific inference (and statistical learning!) like:\n\n\n\nSo, what do we do? Use \\(\\mathbf{D}_{\\text{Tr}}\\) along with knowledge of issues like overfitting to estimate test error!\nFulfills our goal: find model which best predicts \\(Y\\) from \\(X\\) for unobserved data"
  },
  {
    "objectID": "w05/slides.html#the-validation-set-approach",
    "href": "w05/slides.html#the-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Validation Set Approach",
    "text": "The Validation Set Approach\n\n‚ö†Ô∏è Remember: under our new goal, ‚Äúgood‚Äù models = models that generalize well!\nOptimizing over test set violates scientific inference axioms\nInstead, optimize over training data by ‚Äúholding out‚Äù some portion of it‚Ä¶ Enter the Validation Set:\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "w05/slides.html#evaluating-one-model-validation-set-approach",
    "href": "w05/slides.html#evaluating-one-model-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Evaluating One Model: Validation Set Approach",
    "text": "Evaluating One Model: Validation Set Approach\n\n Randomly pick 20% of \\(\\mathbf{D}_{\\text{train}}\\) as sub-training set \\(\\mathbf{D}_{\\text{SubTr}}\\)\n The other 80% becomes validation set \\(\\mathbf{D}_{\\text{Val}}\\)\n Train model on \\(\\mathbf{D}_{\\text{SubTr}}\\), then evaluate using \\(\\mathbf{D}_{\\text{Val}}\\), to produce validation error \\(\\boxed{\\varepsilon_{\\text{Val}} = \\widehat{\\text{Err}}_{\\text{Test}}}\\)\n\\(\\varepsilon_{\\text{Val}}\\) gives us an estimate of the test error\n\n\\(\\implies\\) this is what we want to optimize, in place of training error, for our new goal üòé!"
  },
  {
    "objectID": "w05/slides.html#how-does-it-do-for-our-dgp",
    "href": "w05/slides.html#how-does-it-do-for-our-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "How Does It Do for Our DGP?",
    "text": "How Does It Do for Our DGP?\n\nRecall that the ‚Äútrue‚Äù degree is 5, but that you‚Äôre not supposed to know that!\n\n\n\n\n\nCode\nlibrary(boot)\nset.seed(5300)\nsim200_df &lt;- run_dgp(\n  world_label=\"N=200\", N=200, x_max=100\n)\nsim1k_df &lt;- run_dgp(\n  world_label=\"N=1000\", N=1000, x_max=100\n)\ncompute_deltas &lt;- function(df, min_deg=1, max_deg=12) {\n  cv_deltas &lt;- c()\n  for (i in min_deg:max_deg) {\n    cur_poly &lt;- glm(ldalys ~ poly(gdpc, i), data=df)\n    cur_poly_cv_result &lt;- cv.glm(data=df, glmfit=cur_poly, K=5)\n    cur_cv_adj &lt;- cur_poly_cv_result$delta[1]\n    cv_deltas &lt;- c(cv_deltas, cur_cv_adj)\n  }\n  return(cv_deltas)\n}\nsim200_deltas &lt;- compute_deltas(sim200_df)\nsim200_delta_df &lt;- tibble(degree=1:12, delta=sim200_deltas)\nsim200_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 200\")\n\n\n\n\n\nHere, validation error fails to capture ‚Äòtrue‚Äô model (likely culprits: low \\(N\\), high noise‚Ä¶)\n\n\n\n\n\nPossible resolution: [See coming slides!]\n\n\n\n\nCode\nsim1k_deltas &lt;- compute_deltas(sim1k_df)\nsim1k_delta_df &lt;- tibble(degree=1:12, delta=sim1k_deltas)\nsim1k_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 1000\")\n\n\n\n\n\nHere, validation error fails to sharply distinguish \\(d \\in \\{5, 6, 7, 8\\}\\)\n\n\n\n\n\nPossible resolution: ‚Äúone standard error rule‚Äù"
  },
  {
    "objectID": "w05/slides.html#optimizing-over-many-models",
    "href": "w05/slides.html#optimizing-over-many-models",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Optimizing over Many Models",
    "text": "Optimizing over Many Models\n\n Let \\(\\mathfrak{M} = (\\mathcal{M}_1, \\ldots, \\mathcal{M}_D)\\) be a set of \\(D\\) different models\n\nEx: \\(\\mathcal{M}_1\\) could be a linear model, \\(\\mathcal{M}_2\\) a quadratic model, \\(\\mathcal{M}_3\\) a cubic model, and so on‚Ä¶\n\n For each \\(\\mathcal{M}_i \\in \\mathfrak{M}\\) (and for given training data \\(\\mathbf{D}_{\\text{Tr}}\\)):\n\nUse [Insert Validation Approach] to derive \\(\\varepsilon_i\\)\n\n Model \\(\\mathcal{M}_i\\) with lowest \\(\\varepsilon_i\\) wins!\nWe are now cooking with gas! We can use CV to optimize over e.g.¬†HYPERPARAMETERS ü§Ø (that we had to just kind of‚Ä¶ guess before)!"
  },
  {
    "objectID": "w05/slides.html#cooking-with-even-more-gas",
    "href": "w05/slides.html#cooking-with-even-more-gas",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Cooking with Even More Gas!",
    "text": "Cooking with Even More Gas!\n(Âä† even more Ê≤π!)\n\nIt turns out that, for the purposes of estimating test error (aka, estimating how well the fitted model will generalize), Validation Set Approach is the worst approach (still good, just, least good!)\nTo see its limitations, consider: is there something special about the 20% of data we selected for the validation set?\nAnswer: No! It was literally randomly selected!\n\\(\\implies\\) Any other 20% ‚Äúchunk‚Äù (which we‚Äôll call a fold from now on) could work just as well"
  },
  {
    "objectID": "w05/slides.html#fold-cross-validation",
    "href": "w05/slides.html#fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "5-Fold Cross-Validation",
    "text": "5-Fold Cross-Validation\n\nSecretly, by choosing a 20% fold to be the validation set earlier, I was priming you for 5-fold cross-validation!\n\n\nImage Source"
  },
  {
    "objectID": "w05/slides.html#k-fold-cross-validation",
    "href": "w05/slides.html#k-fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "\\(K\\)-Fold Cross-Validation",
    "text": "\\(K\\)-Fold Cross-Validation\n\n5 was an arbitrary choice!\n\n(Though, one with important and desirable statistical properties, which we‚Äôll get to ASAP)\n\nIn general, \\(K\\)-Fold CV Estimator \\(\\varepsilon_{(K)} = \\widehat{\\text{Err}}_{\\text{Test}}\\) given by\n\n\\[\n\\varepsilon_{(K)} = \\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{\\text{Val}}_{i}\n\\]"
  },
  {
    "objectID": "w05/slides.html#leave-one-out-cross-validation-loocv",
    "href": "w05/slides.html#leave-one-out-cross-validation-loocv",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\\(K\\)-Fold CV with \\(K := N\\)\nProduces the maximum possible number of terms in the summation that leads to \\(\\varepsilon_{(K)}\\)\n\ni.e., \\(\\varepsilon_{(N)}\\) is an average over the maximum possible number of models\n\nSo‚Ä¶ does this mean it‚Äôs the best choice?\nHint: How ‚Äúdifferent‚Äù are any two models used in the sum?\n\n\\[\n\\begin{align*}\n\\varepsilon_{(N)} =~ & \\text{Err}(\\mathbf{D}_1 \\mid \\mathbf{D}_{2:100}) + \\text{Err}(\\mathbf{D}_2 \\mid \\mathbf{D}_1, \\mathbf{D}_{3:100}) \\\\\n&+ \\text{Err}(\\mathbf{D}_3 \\mid \\mathbf{D}_{1:2}, \\mathbf{D}_{4:100}) + \\cdots + \\text{Err}(\\mathbf{D}_{100} \\mid \\mathbf{D}_{1:99})\n\\end{align*}\n\\]\n\nThe terms here are highly correlated!\nFun math results around how \\(K = 5\\) or \\(K = 10\\) can best strike a balance between overly-correlated terms (\\(K = N\\)) and not averaging enough models (\\(K = 2\\))!"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "References",
    "text": "References\n\n\nHume, David. 1760. An Enquiry Concerning Human Understanding. Simon and Schuster.\n\n\nPopper, Karl R. 1934. The Logic of Scientific Discovery. Psychology Press."
  },
  {
    "objectID": "writeups/quiz-1/index.html",
    "href": "writeups/quiz-1/index.html",
    "title": "Quiz 1 Study Guide",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nPractice problems added 2 February 2025, 8:30pm\nFirst published 26 January 2025, 3:30pm"
  },
  {
    "objectID": "writeups/quiz-1/index.html#overview",
    "href": "writeups/quiz-1/index.html#overview",
    "title": "Quiz 1 Study Guide",
    "section": "Overview",
    "text": "Overview\nHello DSAN 5300 Section 01 friends! First, please take a look at the course-wide study guide that was sent out over email by Prof.¬†James! This guide just contains my additional summarization and some practice problems that I hope might link the concepts to stuff I‚Äôve mentioned specifically in Section 01.\nThe key topics that the Quiz will cover can be organized into the following four subheadings:\n\n(1) ‚ÄúParametric‚Äù Modeling\nWhat does it mean to have a ‚Äúparametric‚Äù model? For example, in a model such as regression, which we write as\n\\[\nY = \\beta_0 + \\beta1_ X + \\varepsilon,\n\\]\nwhich of the things in that equation are parameters of the model and which are not parameters (for example, which are just variables that we plug data into)?\n\n\n(2) Optimization in General\nOnce we‚Äôve identified the parameters in a model, how do we evaluate how ‚Äúgood‚Äù or ‚Äúbad‚Äù a certain setting for the parameters is? (The answer being, a loss function)\n\n\n(3) Gradient Descent\nOnce we have a loss function, how does the gradient allow us to choose a random value for the parameter and then ‚Äúmake our way‚Äù towards the optimal value? The answer to this question is the main content in this previous writeup\nAs a refresher, a gradient is just the vector equivalent of a derivative. For example, in calculus we learn how\n\\[\nf(x) = x^2\n\\]\nhas a derivative\n\\[\n\\frac{\\partial f}{\\partial x} = 2x.\n\\]\nSo in this class, if \\(\\mathbf{x}\\) is now a vector like \\(\\mathbf{x} = (x_1,x_2)\\) instead of just a single number, the gradient or vector-valued derivative of \\(f\\) with respect to \\(\\mathbf{x}\\), \\(\\nabla_{\\mathbf{x}} f\\), is\n\\[\n\\nabla_{\\mathbf{x}} f = \\frac{\\partial f}{\\partial \\mathbf{x}} = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right)\n\\]\nIf it helps, try to notice/keep in mind how the \\(\\mathbf{x}\\) in \\(\\frac{\\partial f}{\\partial \\mathbf{x}}\\) is a vector, whereas the \\(x_1\\) in \\(\\frac{\\partial f}{\\partial x_1}\\) is a scalar. In other words, although the first and second terms in this expression may look scary, each entry within the parentheses on the RHS of that equality (the third term) is just the ‚Äúregular‚Äù univariate derivative that you learn in calculus class!\n\n\n(4) More Efficient Optimization Methods\nHere the idea (or, the way I see these ‚Äúfancier‚Äù methods, at least) is, they use additional information about the loss function above and beyond just \\(L(x)\\) and its derivative \\(L'(x)\\).\nSo, Newton‚Äôs method for example uses the second derivative \\(L''(x)\\) as an additional piece of information about the curvature of the loss function, whereas the secant method is slower than Newton‚Äôs method but doesn‚Äôt require us to know this second derivative \\(L''(x)\\) (since it approximates it)."
  },
  {
    "objectID": "writeups/quiz-1/index.html#a-full-on-lecture-replacement",
    "href": "writeups/quiz-1/index.html#a-full-on-lecture-replacement",
    "title": "Quiz 1 Study Guide",
    "section": "A Full-On Lecture Replacement",
    "text": "A Full-On Lecture Replacement\nTo try and fully ‚Äúfill in‚Äù the missing week here, I can just give you all the resource that is literally a recording of the class I learned this stuff from, and that I later TAed. That way if you have additional questions I‚Äôll be able to refer specifically to the examples/materials that Prof.¬†Ng uses in the following video:"
  },
  {
    "objectID": "writeups/quiz-1/index.html#practice-problems",
    "href": "writeups/quiz-1/index.html#practice-problems",
    "title": "Quiz 1 Study Guide",
    "section": "Practice Problems",
    "text": "Practice Problems\n\nProblem 1: Handling Binary Features\nA team of data scientists working for the Los Angeles Lakers NBA team has developed a basic single linear regression model relating the number of minutes played by LeBron James \\(M\\) to their team‚Äôs total points \\(T\\) in a game:\n\\[\nT = \\beta_0 + \\beta_1 M\n\\]\nThey then estimated \\(\\beta_0\\) and \\(\\beta_1\\) from data on the past two seasons, arriving at estimates\n\\[\n\\widehat{\\beta}_0 = 80 \\text{ and }\\widehat{\\beta}_1 = 0.9\n\\]\nPlotting the resulting regression estimation function \\(\\widehat{t} = 80 + 0.9 m\\) on top of the data, they therefore obtain a figure that looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\ntotal_games &lt;- 164\nminute_vals &lt;- runif(total_games, min=0, max=48)\npts_raw &lt;- 80 + 0.9 * minute_vals\npts_noise &lt;- rnorm(total_games, mean=0, sd=5)\npts_obs &lt;- pts_raw + pts_noise\nlebron_df &lt;- tibble(minutes=minute_vals, points=pts_obs)\nlebron_df |&gt; ggplot(aes(x=minutes, y=points)) +\n  geom_point() +\n  geom_smooth(method='lm', formula='y ~ x', se=FALSE) +\n  labs(\n    title=\"Team Points vs. LeBron James Minutes Played\",\n    x = \"Minutes Played by LeBron (M)\",\n    y = \"Lakers Total Points (T)\"\n  ) +\n  theme_classic(base_size=14)\n\n\n\n\n\n\n\n\n\nHowever, as of this morning, a big trade has been made sending Slovenian basketball phenom Luka Doncic to the Lakers(!)\nSo, the Lakers‚Äô data science team is frantically running simulations to model the potential impact Luka will have on their team‚Äôs performance, and especially how it will affect their model plotted above, of team performance vs.¬†LeBron‚Äôs minutes played. They therefore construct a dummy variable \\(D\\), for Doncic, which has the value 1 for games with Doncic on the Lakers and 0 for games without Doncic on the Lakers.\nIn the next two questions you will consider two hypotheses regarding Luka‚Äôs potential contribution, and the appropriate model for each of the two hypotheses!\n\n\nHypothesis 1: Additive Contribution\nA member of the data science team named Addison hypothesizes that Luka‚Äôs contribution will be ‚Äúadditive‚Äù with respect to LeBron‚Äôs performance, in the sense that:\n\nThe team‚Äôs total points will now shift upwards by some amount, regardless of the number of minutes played by LeBron, but\nThe team‚Äôs total points will still increase by about 0.9 points per additional minute that LeBron plays.\n\nFor example, Doncic scored about 34 points per game last season, while the player he was traded for, Anthony Davis, scored about 25 points per game in that same season, so (by this ‚Äúback of the envelope‚Äù calculation) Addison hypothesizes that the team‚Äôs total points with Doncic may be 9 higher than its total points without Doncic, for any amount of minutes that LeBron plays.\nUsing this information,\n\nWrite out a linear regression model which would allow this hypothesis to be tested (once data from games with Luka comes in),\nUse the ‚Äúsplit into cases‚Äù method from class to specify what the prediction equation will look like when \\(D = 0\\) and when \\(D = 1\\), and then\nWrite the hypothesis explicitly in terms of the parameters (\\(\\beta_0\\), \\(\\beta_1\\), etc.) of the regression model.\n\nSolution:\nSince Addison‚Äôs hypothesis implies that the slope from the above model would remain the same, but that the intercept would increase, we do not need an interaction term between \\(M\\) and \\(T\\) in our model. Therefore, we can model Addison‚Äôs hypothesis using a simple MLR model without any interaction terms:\n\\[\nT = \\beta_0 + \\beta_1 M + \\beta_2 D\n\\]\nNext, we can split this modeling equation into cases, to see what predictions it will generate when \\(D = 1\\) separately from the predictions it will generate when \\(D = 0\\):\n\\[\nT = \\begin{cases}\n\\beta_0 + \\beta_1 M &\\text{if }D = 0 \\\\\n\\beta_0 + \\beta_1 M + \\beta_2 &\\text{if }D = 1\n\\end{cases}\n\\]\nLastly, we can see from these cases that the value of \\(\\beta_2\\) specifcally is what would tell us how much the Lakers‚Äô total points shift in games with Doncic (\\(D = 1\\)) compared to games without Doncic (\\(D = 0\\)). So, Addison‚Äôs hypothesis can be written as\n\\[\n\\mathcal{H}_{\\text{Addison}}: \\beta_2 = 9\n\\]\nWith the hypothesis written out in this way, we could now test Addison‚Äôs hypothesis by collecting data, estimating \\(\\beta_2\\) from this data, and using e.g.¬†the standard error, \\(T\\)-statistic, and \\(p\\)-value information provided in the regression output!\n\n\nHypothesis 2: Interactive Contribution\nA second member of the data science team named Interacthony hypothesizes that Luka‚Äôs contribution will in fact ‚Äúboost‚Äù LeBron‚Äôs performance, in the sense that:\n\nThe team‚Äôs total points will now shift upwards by some amount, even when LeBron doesn‚Äôt play at all, but also\nDoncic and LeBron will complement each other‚Äôs play style, such that LeBron will now ‚Äúconvert‚Äù each additional minute of play into a greater number of points for the team.\n\nFor example, whereas in the model plotted above each additional minute played by LeBron translated into an additional 0.9 points for the Lakers, Interacthony now hypothesizes (in addition to Addison‚Äôs hypothesis from the previous question) that in games with Doncic (games with \\(D = 1\\)) each additional minute played by LeBron will translate into an additional 1.1 points for the Lakers.\nSo, like in the previous part, your task is to:\n\nWrite out a linear regression model which would allow Interacthony‚Äôs two hypotheses to be tested (once data from games with Luka comes in),\nUse the ‚Äúsplit into cases‚Äù method from class to specify what the prediction equation will look like when \\(D = 0\\) and when \\(D = 1\\), and then\nWrite the two hypotheses explicitly in terms of the parameters (\\(\\beta_0\\), \\(\\beta_1\\), etc.) of the regression model.\n\nSolution:\nIn this case, we need a model which will allow both the intercept and the slope of the earlier model to change based on whether or not Doncic is on the Lakers. So, we now need an interaction term, to enable both intercept and slope to vary, as we discussed in class. The base model we should use here is therefore\n\\[\nT = \\beta_0 + \\beta_1 M + \\beta_2 D + \\beta_3 (M \\times D)\n\\]\nWriting out the cases for \\(D = 0\\) and \\(D = 1\\) reveals why this interaction term is necessary to allow both intercept and slope to vary:\n\\[\n\\begin{align*}\nT &= \\begin{cases}\n\\beta_0 + \\beta_1 M &\\text{if }D = 0 \\\\\n\\beta_0 + \\beta_1 M + \\beta_2 + \\beta_3 M &\\text{if }D = 1\n\\end{cases} \\\\\n&= \\begin{cases}\n\\beta_0 + \\beta_1 M &\\text{if }D = 0 \\\\\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) M &\\text{if }D = 1\n\\end{cases}\n\\end{align*}\n\\]\nAnd so we see that \\(\\beta_2\\) represents the amount by which the intercept would change and \\(\\beta_3\\) the amount by which the slope would change for datapoints with \\(D = 1\\). Thus, we can write Interacthony‚Äôs two hypotheses as:\n\\[\n\\begin{align*}\n\\mathcal{H}_{\\text{Inter}}^{1}: \\beta_2 &= 9 \\\\\n\\mathcal{H}_{\\text{Inter}}^{2}: \\beta_3 &= (1.1 - 0.9) = 0.2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/lab-9/index.html",
    "href": "writeups/lab-9/index.html",
    "title": "Getting Started with Lab 9",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nOriginal version posted 1 Apr 2025, 9:00pm"
  },
  {
    "objectID": "writeups/lab-9/index.html#practice-code-for-nnet-in-r",
    "href": "writeups/lab-9/index.html#practice-code-for-nnet-in-r",
    "title": "Getting Started with Lab 9",
    "section": "Practice Code for nnet in R",
    "text": "Practice Code for nnet in R\nSince the ISLR lab within the Deep Learning chapter uses R‚Äôs keras library rather than nnet1, here are some quick examples of how nnet works that may help you get started on the R portion of Lab 9.\n\nLoad Data\nHere we‚Äôll load the Advertising.csv dataset used in the beginning of ISLR:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nad_df &lt;- read_csv(\"https://www.statlearning.com/s/Advertising.csv\", show_col_types = FALSE)\n\n\nNew names:\n‚Ä¢ `` -&gt; `...1`\n\n\nCode\ncolnames(ad_df) &lt;- c(\"id\", colnames(ad_df)[2:5])\nad_df |&gt; head()\n\n\n\n\n\n\nid\nTV\nradio\nnewspaper\nsales\n\n\n\n\n1\n230.1\n37.8\n69.2\n22.1\n\n\n2\n44.5\n39.3\n45.1\n10.4\n\n\n3\n17.2\n45.9\n69.3\n9.3\n\n\n4\n151.5\n41.3\n58.5\n18.5\n\n\n5\n180.8\n10.8\n58.4\n12.9\n\n\n6\n8.7\n48.9\n75.0\n7.2\n\n\n\n\n\n\nA scatterplot of TV vs.¬†sales looks as follows:\n\n\nCode\nad_df |&gt; ggplot(aes(x = TV, y = sales)) +\n  geom_point() +\n  theme_dsan()\n\n\n\n\n\n\n\n\n\n\n\nStandard Linear (Regression) Model\nHere we use lm(), also used near the beginning of ISLR, to obtain OLS estimates of the coefficients relating TV, radio, and newspaper to sales:\n\n\nCode\nreg_model &lt;- lm(\n    sales ~ TV + radio + newspaper,\n    data=ad_df\n)\nprint(summary(reg_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nWhile we can‚Äôt really ‚Äúfully‚Äù visualize the model in 2D or even 3D (since there are 3 features and 1 label, which would require a 4D visualization), we can still obtain a helpful 2D visualization that broadly resembles the above visualization of TV vs.¬†sales.\nTo achieve this, we freeze two of the feature values (radio and newspaper) at their means and then plot what our model says about the relation between TV and sales at these held-constant radio and newspaper values:\n\n\nCode\n# \"Freeze\" radio and newspaper values at their means\nradio_mean &lt;- mean(ad_df$radio)\nnews_mean &lt;- mean(ad_df$newspaper)\n# Define the range of TV values over which we want to plot predictions\nTV_vals &lt;- seq(0, 300, 10)\n# Extract all coefficients from our model\nreg_coefs &lt;- reg_model$coef\n# For every value v in TV_vals, compute prediction\n# yhat(v, radio_mean, news_mean)\nget_prediction &lt;- function(TV_val) {\n    intercept &lt;- reg_coefs['(Intercept)']\n    TV_term &lt;- reg_coefs['TV'] * TV_val\n    radio_term &lt;- reg_coefs['radio'] * radio_mean\n    news_term &lt;- reg_coefs['newspaper'] * news_mean\n    return(intercept + TV_term + radio_term + news_term)\n}\n# Compute predictions for each value of TV_vals\npred_df &lt;- tibble(TV=TV_vals) |&gt; mutate(\n    sales_pred = get_prediction(TV)\n)\nggplot() +\n  geom_point(data=ad_df, aes(x=TV, y=sales)) +\n  geom_line(\n    data=pred_df, aes(x=TV, y=sales_pred),\n    linewidth=1, color=cb_palette[2]\n  ) +\n  theme_dsan()\n\n\n\n\n\n\n\n\n\n\n\nnnet for (Simple) NN Model Weights\nHere, the reason I put ‚Äú(Simple)‚Äù is because, for example, nnet only supports networks with either (a) no hidden layers at all, or (b) a single hidden layer.\nHere, to show you how to fit NN models using nnet (without giving away the full code required for this part of the lab), I am using just the default parameter settings for the nnet() function‚Äîon the Lab itself you‚Äôll need to read the instructions more carefully and think about how to modify this code to achieve the desired result.\n\n\nCode\nlibrary(nnet)\nnn_model &lt;- nnet(\n    sales ~ TV + radio + newspaper,\n    size=10,\n    linout=TRUE,\n    data=ad_df\n)\n\n\n# weights:  51\ninitial  value 52680.989030 \niter  10 value 4003.592232\niter  20 value 3556.990620\niter  30 value 3277.830385\niter  40 value 2693.332152\niter  50 value 1442.629937\niter  60 value 713.350821\niter  70 value 419.466340\niter  80 value 314.564890\niter  90 value 202.253371\niter 100 value 84.217076\nfinal  value 84.217076 \nstopped after 100 iterations\n\n\nCode\nnn_model\n\n\na 3-10-1 network with 51 weights\ninputs: TV radio newspaper \noutput(s): sales \noptions were - linear output units \n\n\nFrom the second part of the output (the output from just the line nn_model), you should think through why it‚Äôs called a ‚Äú3-10-1 network‚Äù, and then why this architecture would require estimating 51 weights.\nTo visualize what‚Äôs happening, we can take the same approach we took in the previous visualization: see what our NN predicts for sales across a range of TV values, with radio and newspaper held constant at their means.\nFirst, note that R‚Äôs predict() function takes in (1) a fitted model and (2) a data.frame where each row is a vector of values you want to generate a prediction for. So, for example, we can obtain a single prediction for a specific set of TV, radio, and newspaper values like:\n\n\nCode\npredict(nn_model, data.frame(TV=10, radio=23, newspaper=30))\n\n\n      [,1]\n1 6.698056\n\n\nSo, for ease-of-use with this predict() functionality, we first construct a tibble where each row represents a tuple (TV_val, radio_mean, news_mean):\n\n\nCode\nnn_input_df &lt;- data.frame(TV=TV_vals, radio=radio_mean, newspaper=news_mean)\nas.data.frame(nn_input_df)\n\n\n\n  \n\n\n\nAnd now, by plugging this tibble into predict(), we obtain our NN‚Äôs prediction for the inputs in each row:\n\n\nCode\nnn_pred_df &lt;- nn_input_df\nnn_pred_df$sales_pred &lt;- predict(nn_model, nn_input_df)\nas.data.frame(nn_pred_df)\n\n\n\n  \n\n\n\nWhich we can visualize using the same approach we used for the linear model above (the non-linearity is subtle, but we can see the line varying in a way that a straight line \\(y = mx + b\\) would not!)\n\n\nCode\nggplot() +\n  geom_point(data=ad_df, aes(x=TV, y=sales)) +\n  geom_line(\n    data=nn_pred_df, aes(x=TV, y=sales_pred),\n    linewidth=1, color=cb_palette[2]\n  ) +\n  theme_dsan()"
  },
  {
    "objectID": "writeups/lab-9/index.html#footnotes",
    "href": "writeups/lab-9/index.html#footnotes",
    "title": "Getting Started with Lab 9",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKeras is a more complex, heavy-duty neural network library, but for the purposes of the lab (showing how models like logistic regression can be ‚Äúreconceptualized‚Äù as simple neural networks) the simpler nnet library has a less-steep learning curve!‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/hw1-guide/index.html",
    "href": "writeups/hw1-guide/index.html",
    "title": "Getting Started with HW 1",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nSection on Least Squares Derivation added 1 Feb 2025, 1:00am\nOriginal version posted 31 Jan 2025, 2:30am\nAs with the Lab 1 getting-started guide, here we don‚Äôt give away the answers but do provide as much background as possible to nudge you up to the starting line!"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#sums-leftrightarrows-linear-algebraic-operations",
    "href": "writeups/hw1-guide/index.html#sums-leftrightarrows-linear-algebraic-operations",
    "title": "Getting Started with HW 1",
    "section": "Sums \\(\\leftrightarrows\\) Linear-Algebraic Operations",
    "text": "Sums \\(\\leftrightarrows\\) Linear-Algebraic Operations\nOne key motivation for this section is for you to see how all of the regression model‚Äôs mathematical details can be represented in two equally valid ways:\n\nFirst, using a ‚Äústandard‚Äù calculus approach, where you will end up with lots of sums (\\(\\Sigma\\) symbols) in your derivations because (for example) the loss function used by regression is itself a sum of squares (specifically, a sum of squared residuals). However, there is also‚Ä¶\nSecond, using a linear algebra-heavy approach, where in place of the sums you will now have lots of vector-vector, matrix-vector, and matrix-matrix products.\n\nTo see how the second approach ‚Äúreplaces‚Äù the sums with linear-algebraic operations, as a basic example you can use to remind yourself of these two different representations (and then think about which one feels more clear to you, and then use that one), consider a scenario where you have a set of datapoints (scalars for now):\n\\[\n\\mathbf{x} = (x_1, x_2, \\ldots, x_n),\n\\]\nand a set of weights, with one weight per datapoint (so that the subscripts ‚Äúmatch up‚Äù: \\(w_1\\) is the weight to be applied to \\(x_1\\), \\(w_2\\) is the weight to be applied to \\(x_2\\), and so on):\n\\[\n\\mathbf{w} = (w_1, w_2, \\ldots, w_n).\n\\]\nNow, if we wanted to write out the weighted sum \\(S\\) of these datapoints \\(\\mathbf{x}\\), with the weights given by \\(\\mathbf{w}\\), the ‚Äústraightforward‚Äù way (at least, in the sense that it‚Äôs probably the approach you learned earlier on in your math-class-taking career) would look like:\n\\[\nS = w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\sum_{i=1}^{n}w_ix_i,\n\\]\nwhich is why that \\(\\Sigma\\) notation will appear all over the place when you are working through the early parts of the homework.\nHowever, now consider the fact that you know about some fancier mathematical objects‚Äîvectors and matrices‚Äîand how they can help us in terms of providing an alternative representation of this same sum! Instead of ‚Äúzooming in‚Äù on the individual elements to write out the weighted sum, we could just as easily treat them as unitary objects‚Äîvectors‚Äîand apply the binary dot product operator from Linear Algebra to achieve the same weighted sum!\nTo see this, recall how the dot product of two vectors is defined, and then work out what (e.g.) \\(\\mathbf{w} \\cdot \\mathbf{x}^{\\top}\\) looks like:\n\\[\n\\begin{align*}\n\\mathbf{w} \\cdot \\mathbf{x} &= (w_1, w_2, \\ldots, w_n) \\cdot (x_1, x_2, \\ldots, x_n) \\\\\n&= w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\sum_{i=1}^{n}w_ix_i = S ~ ‚úÖ\n\\end{align*}\n\\tag{1}\\]\nAnd so we see that, indeed, we can obtain this same sum \\(S\\) by considering the datapoints and weights as vectors and then using the dot product as our key operator for combining these vectors (rather than thinking of \\(S\\) on the level of individual multiplications and additions)."
  },
  {
    "objectID": "writeups/hw1-guide/index.html#the-regression-models-linear-algebraic-representation",
    "href": "writeups/hw1-guide/index.html#the-regression-models-linear-algebraic-representation",
    "title": "Getting Started with HW 1",
    "section": "The Regression Model‚Äôs Linear Algebraic Representation",
    "text": "The Regression Model‚Äôs Linear Algebraic Representation\nThat all may be obvious to you, if you‚Äôve taken Linear Algebra for example, but now to foreshadow some of the later portions of the homework, let‚Äôs look at how this idea can help us when e.g.¬†we‚Äôre working with the math of regression models.\nConsider the non-Linear-Algebraic way we‚Äôve been writing out the basic pieces of the regression model thus far in the class. The Multiple Linear Regression model, for example, is typically written out in a form like this (where here I‚Äôm writing out the model we‚Äôd use to predict one particular label, \\(y_i\\), as a function of one observation‚Äôs features, \\(x_{i,1}\\) through \\(x_{i,m}\\)).\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\tag{2}\\]\nIf we look at this representation, we can see how it almost perfectly matches the type of weighted sum we re-wrote using the dot product in Equation¬†1 above. The only difference, in fact, is that the pesky \\(\\beta_0\\) coefficient has no corresponding \\(x_{i,0}\\) term‚Ä¶\n‚Ä¶But, what if we just defined an \\(x_{i,0}\\), to just always take on the numeric value \\(1\\)? This would provide the ‚Äúmissing piece‚Äù which would allow us to write the Multiple Linear Regression model as a dot product! That is: once we define \\(x_{i,0} \\triangleq 1\\), we can now write the data for this observation as as vector \\(\\mathbf{x}_i\\) like:\n\\[\n\\mathbf{x}_i = (x_{i,0}, x_{i,1}, x_{i,2}, \\ldots, x_{i,M}) = (1, x_{i,1}, x_{i,2}, \\ldots, x_{i,M}),\n\\]\nand we can write the coefficients as a vector \\(\\boldsymbol\\beta\\) like:\n\\[\n\\boldsymbol\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_M),\n\\]\nand we achieve the same type of result as in the previous section‚Äîthat the Multiple Linear Regression model itself (modeling the prediction for one observation, for now) can be written as a dot product:\n\\[\n\\begin{align*}\n\\widehat{y}_i &= \\boldsymbol\\beta \\cdot \\mathbf{x}_i = (\\beta_0, \\beta_1, \\ldots, \\beta_M) \\cdot (1, x_{i,1}, \\ldots, x_{i,M}) \\\\\n&= \\beta_0 + \\beta_1 x_{i,1} + \\cdots + \\beta_M x_{i,M} ~ ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#from-vectors-to-matrices",
    "href": "writeups/hw1-guide/index.html#from-vectors-to-matrices",
    "title": "Getting Started with HW 1",
    "section": "From Vectors to Matrices",
    "text": "From Vectors to Matrices\nThe final step, in terms of representing the full MLR model using objects from Linear Algebra (rather than just its prediction for one particular observation \\(i\\)), requires us to think about matrices rather than just the vectors we‚Äôve used thus far. However, just as we thought of:\n\nVectors as objects allowing us to group individual scalars together into a single object, here we can also retain our sanity by thinking of\nMatrices as objects allowing us to group individual vectors together into a single object!\n\nTo see how this way of thinking can help us here, notice how if we have \\(N\\) labels (\\(y_1\\) through \\(y_n\\)) for \\(N\\) datapoints (\\(\\mathbf{x}_1\\) through \\(\\mathbf{x}_n\\)), then our model is going to generate \\(N\\) predictions, using Equation¬†2 above \\(N\\) times:\n\\[\n\\begin{align*}\n\\widehat{y}_1 &= \\beta_0 + \\beta_1 x_{1,1} + \\cdots + \\beta_m x_{1,m} \\\\\n\\widehat{y}_2 &= \\beta_0 + \\beta_1 x_{2,1} + \\cdots + \\beta_m x_{2,m} \\\\\n\\phantom{y_3} &~~\\vdots \\\\\n\\widehat{y}_n &= \\beta_0 + \\beta_1 x_{n,1} + \\dots + \\beta_m x_{n,m}\n\\end{align*}\n\\]\nSo, if you squint your eyes while looking at this system of equations, and you keep in mind the above point about defining \\(x_{i,0}\\) to just be the value \\(1\\) for every observation \\(i\\), hopefully you can start to see how that whole thing could be re-written as a single matrix equation!\nTo start off, for example, we could gather all of the \\(\\widehat{y}_i\\) terms on the left-hand side of each equation into a single column vector:\n\\[\n\\widehat{\\mathbf{y}} = \\begin{bmatrix}\n\\widehat{y}_1 \\\\\n\\vdots \\\\\n\\widehat{y}_n\n\\end{bmatrix}\n\\]\nAnd, next, we already saw how weighted sums like the ones we see on the right-hand side of each equation can be re-written as vector-vector products (dot products). So, in the same way that we ‚Äústacked‚Äù the individual \\(\\widehat{y}_i\\) terms into a single column vector just now, we can also look at these right-hand side expressions as a ‚Äústack‚Äù of such products, like:\n\\[\n\\begin{bmatrix}\n\\beta_0 x_{1,0} + \\beta_1x_{1,1} + \\cdots + \\beta_m x_{1,m} \\\\\n\\vdots \\\\\n\\beta_0 x_{n,0} + \\beta_1x_{n,1} + \\cdots + \\beta_m x_{n,m}\n\\end{bmatrix} = \\begin{bmatrix}\n\\boldsymbol\\beta \\cdot \\mathbf{x}_1 \\\\\n\\vdots \\\\\n\\boldsymbol\\beta \\cdot \\mathbf{x}_n\n\\end{bmatrix}\n\\tag{3}\\]\nThe final leap, which I think is most helpful if you try to work it out yourself (like, as in, by trying different guesses and multiplying them out to see if you get the desired result), is to take this almost-there representation where we‚Äôve stacked the dot products (\\(\\boldsymbol\\beta \\cdot \\mathbf{x}_1\\), \\(\\boldsymbol\\beta \\cdot \\mathbf{x}_2\\), and so on) into a column vector, and turn it into a product of only ‚Äúbase‚Äù Linear Algebra objects: vectors and/or matrices.\nIn other words, right now we have a column-vector-of-dot-products, which is not exactly what we think of when we think of ‚Äúa vector‚Äù or ‚Äúa matrix‚Äù (it‚Äôs‚Ä¶ a hybrid of the two, in a sense). To make progress, take note of the fact that:\n\n\\(\\boldsymbol\\beta\\) appears in every row, whereas\nFor a given index \\(i\\), \\(\\mathbf{x}_i\\) only appears in one row.\n\nSo (here‚Äôs where you can pause and try to work it out on paper, before reading on!), the second bullet point provides us with a hint that it may be helpful to construct a data matrix \\(\\mathbf{X}\\), where each row \\(i\\) contains all of the terms in \\(\\mathbf{x}_i\\):\n\\[\n\\begin{align*}\n\\mathbf{X} &= \\begin{bmatrix}\n\\mathbf{x}_1 \\\\\n\\mathbf{x}_2 \\\\\n\\vdots \\\\\n\\mathbf{x}_n\n\\end{bmatrix} = \\begin{bmatrix}\nx_{1,0} & x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\nx_{2,0} & x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n,0} & x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\n1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n\\end{bmatrix}\n\\end{align*}\n\\]\nAnd finally, the first bullet point gives us a hint that we don‚Äôt need a full matrix to represent \\(\\boldsymbol\\beta\\), since the same set of parameters \\(\\boldsymbol\\beta\\) is used across all predictions from \\(\\widehat{y}_1\\) to \\(\\widehat{y}_n\\). Instead, to ensure that it ‚Äúcombines with‚Äù our data matrix \\(\\mathbf{X}\\) to produce the desired final system of equations, we can represent it as a column vector:\n\\[\n\\boldsymbol\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_m\n\\end{bmatrix}\n\\]\nThough the choice of a column vector rather than a row vector here might seem arbitrary at first, the point is exactly what I mentioned above, that you can try the two different representations and see which one is more helpful for what we‚Äôre trying to do. It turns out that, with this column vector representation, we can combine \\(\\mathbf{X}\\) with \\(\\boldsymbol\\beta\\) using a simple matrix-vector multiplication to obtain the final result we‚Äôve been looking for:\n\\[\n\\begin{align*}\n\\mathbf{X}\\boldsymbol\\beta &= \\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\n1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n\\end{bmatrix} \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_m\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\beta_0 x_{1,0} + \\beta_1x_{1,1} + \\cdots + \\beta_m x_{1,m} \\\\\n\\vdots \\\\\n\\beta_0 x_{n,0} + \\beta_1x_{n,1} + \\cdots + \\beta_m x_{n,m}\n\\end{bmatrix}\n\\end{align*}\n\\]\nWhich is precisely the stack-of-weighted-sums we were hoping to ‚Äúdecompose‚Äù into Linear Algebraic operations in Equation¬†3 above!\nSo, combining all this together, if we want a representation of our MLR model without any sums (since the sums are all performed implicitly via the vector-vector and matrix-vector operations), we can now just use the following ultra-shorthand Linear Algebraic form!\n\\[\n\\widehat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol\\beta\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#sanity-check-how-do-we-know-these-operations-will-be-well-defined",
    "href": "writeups/hw1-guide/index.html#sanity-check-how-do-we-know-these-operations-will-be-well-defined",
    "title": "Getting Started with HW 1",
    "section": "Sanity Check: How Do We Know These Operations Will Be Well-Defined?",
    "text": "Sanity Check: How Do We Know These Operations Will Be Well-Defined?\nThis is kind of‚Ä¶ shoved in here, since it‚Äôs generally super useful imo, across any situation where you‚Äôre using matrices and/or vectors, but I may as well introduce it here!\nThere is a ‚Äúsanity check‚Äù you can perform, whenever you‚Äôre doing math or writing code that involves matrices/vectors, that instantly gives you two pieces of information:\n\nA verification of whether or not the matrix-matrix or matrix-vector product is well-defined, but also\nThe specific dimensions that the result of the matrix-matrix or matrix-vector product will have!\n\nIn general, this sanity check involves just checking how many rows and columns are in the two objects you‚Äôre hoping to multiply, and writing these two dimensions (number of rows and number of columns) underneath each object. So, for example, if we‚Äôre thinking about right-multiplying a \\(3 \\times 2\\) matrix \\(A\\) by a \\(2 \\times 4\\) matrix \\(B\\), we can write the two matrices out like:\n\\[\nAB =\n\\underbrace{\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{bmatrix}}_{3 \\times 2} \\underbrace{\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & b_{14} \\\\\nb_{21} & b_{22} & b_{23} & b_{24}\n\\end{bmatrix}}_{2 \\times 4}\n\\]\nAnd now, the first piece of information this gives us: how do we know whether or not this multiplication is well-defined?\n\nThe multiplication is well-defined if the two ‚Äúinner‚Äù numbers written under the matrices are equal: in this case, the operation is well-defined because the number of columns in the first matrix \\(A\\) (\\(2\\)) is equal to the number of rows in the second matrix \\(B\\) (\\(2\\))!\n\nI call these the ‚Äúinner‚Äù numbers because, if we simplified the above to just have the shapes written out, we‚Äôd get\n\\[\n[3 \\times 2][2 \\times 4],\n\\]\nin which the ‚Äúwell-defined test‚Äù can now be read off from the two numbers (\\(2]\\) and \\([2\\)) on the ‚Äúinside‚Äù of this 4-number representation.\nNext, we get the second piece of information: what dimensions will the resulting product have?\n\nIf the multiplication is well-defined, then the result will have a shape given by the two outer numbers written underneath the matrices above: In this case, the result have a number of rows equal to the number of rows in \\(A\\) (\\(3\\)) and a number of columns equal to the number of columns in \\(B\\) (\\(4\\)), i.e., the result will be a \\(3 \\times 4\\) matrix.\n\nWe can verify this, for example, if we know the procedure for matrix-matrix multiplication from Linear Algebra:\n\\[\n\\begin{align*}\nAB &= \\underbrace{\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{bmatrix}}_{3 \\times 2} \\underbrace{\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & b_{14} \\\\\nb_{21} & b_{22} & b_{23} & b_{24}\n\\end{bmatrix}}_{2 \\times 4} \\\\\n&= \\underbrace{\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} & a_{11}b_{13} + a_{12}b_{23} & a_{11}b_{14} + a_{12}b_{24} \\\\\na_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} & a_{21}b_{13} + a_{22}b_{23} & a_{21}b_{14} + a_{22}b_{24} \\\\\na_{31}b_{11} + a_{32}b_{21} & a_{31}b_{12} + a_{32}b_{22} & a_{31}b_{13} + a_{32}b_{23} & a_{31}b_{14} + a_{32}b_{24}\n\\end{bmatrix}}_{3 \\times 4}\n\\end{align*}\n\\]\nAnd we can see that the result is a \\(3 \\times 4\\) matrix, as expected given the pair of ‚Äúouter‚Äù numbers \\([3\\) and \\(4]\\) in the shape representation\n\\[\n[3 \\times 2][2 \\times 4].\n\\]\nIn my head, once I got used to it, this boiled down to something like the following ‚Äúrule‚Äù: given a \\(r_A \\times c_A\\) matrix \\(A\\) and a \\(r_B \\times c_B\\) matrix \\(B\\), their matrix-matrix product \\(AB\\) can be sanity-checked using a ‚Äúdiagram‚Äù like:\n\\[\n[\\underline{r_A} \\times \\overset{‚úÖ}{c_A}][\\overset{‚úÖ}{r_B} \\times \\underline{c_B}] = [r_A \\times c_B]\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#least-squares-derivation",
    "href": "writeups/hw1-guide/index.html#least-squares-derivation",
    "title": "Getting Started with HW 1",
    "section": "Least Squares Derivation",
    "text": "Least Squares Derivation\nFor this part, the truth is you‚Äôre mainly going to have to wade through a lot of algebra‚Äîthe idea is to get comfortable with the use/manipulation of the kinds of quantities that will probably come up again and again as you absorb fancier and fancier regression and ML models!\nSo, if your derivation has lots of terms that look like \\(\\sum_{i=1}^{N}x_iy_i\\), \\(\\sum_{i=1}^{N}x_i^2\\), and so on, that is a good sign (and relates to the points in the previous sections about turning sums into Linear Algebraic operations)!\nProbably the most concrete advice I can give, for those of you still wrestling with this part, is that at least for the way my brain works it helps to ‚Äúcompartmentalize‚Äù the derivation into two chunks:\nFirst (again, for me, though this division may not be helpful for everyone), I find it helpful to split the terms we might see into (a) terms which arise from treating it like a ‚Äúpure‚Äù calculus problem that we solve by taking derivatives and setting them equal to zero, and then (b) terms/definitions which we bring in from statistics (the definitions which are given in the problem) which can help us interpret what we find. So, for this derivation, that split would look like:\n\n\n\n\n\n\n\n(a) Calculus/Algebra Thing\n(b) Statistics Thing\n\n\n\n\n\\(\\sum_{i=1}^{N}x_i\\)\n\\(\\overline{x} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\n\\(\\sum_{i=1}^{N}y_i\\)\n\\(\\overline{y} = \\frac{1}{N}\\sum_{i=1}^{N}y_i\\)\n\n\n\\(\\sum_{i=1}^{N}x_iy_i\\)\n\\(\\text{Cov}[X, Y] = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\overline{x})(y_i - \\overline{y})\\)\n\n\n\\(\\sum_{i=1}^{N}x_i^2\\)\n\\(\\text{Var}[X] = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\overline{x})^2\\)\n\n\n\nThen, I like to think of the first actual doing-math step as the step where I treat is as a ‚Äúpure‚Äù calculus problem. Mainly because my brain starts to hurt if I switch back-and-forth between Calculus/Algebra Things and Statistics Things, I find it more comfortable to forget about the right column in the above table and just solve:\n\\[\n\\begin{align*}\n\\min_{m,b}\\left[ L(m,b) \\right] &= \\min_{m,b}\\left[ \\sum_{i=1}^{N} (\\widehat{y}(m,b) - y)^2 \\right] \\\\\n&= \\min_{m,b}\\left[ \\sum_{i=1}^{N}((mx_i + b) - y)^2 \\right]\n\\end{align*}\n\\]\nThe way I learned to solve minimization problems in calculus: by taking the thing inside the square brackets, computing its derivative(s) with respect to the maximand(s) (\\(m\\) and \\(b\\) in this case), and then solving the system of equations which in some classes would be called the ‚ÄúFirst-Order Conditions‚Äù that are necessary (but not sufficient) for some chosen values \\(m^*\\) and \\(b^*\\) to minimize the function:\n\\[\n\\left. \\frac{\\partial L}{\\partial m}\\right|_{\\substack{m=m^* \\\\ b=b^*}} = 0 \\wedge \\left. \\frac{\\partial L}{\\partial b}\\right|_{\\substack{m=m^* \\\\ b=b^*}} = 0\n\\]\nThe goal, once you take these two derivatives and set them equal to zero, is to use algebraic manipulations to eventually arrive at closed-form solutions for \\(m\\) and \\(b\\). In this case, what would a closed-form solution look like?\n\nA closed-form solution for \\(m\\) would be an expression like\n\\[\nm = [\\text{stuff}],\n\\]\nwhere everything on the right-hand side is a function of only \\(x_i\\), \\(y_i\\), and \\(N\\). Meaning, if \\(b\\) or \\(m\\) itself appear on the right-hand side, you have not yet arrived at a closed-form solution for \\(m\\)!\nA closed-form solution for \\(b\\) would be an expression like\n\\[\nb = [\\text{stuff}],\n\\]\nwhere everything on the right-hand side here is a function of only \\(x_i\\), \\(y_i\\), and \\(N\\). So, if \\(m\\) or \\(b\\) itself appear on the right-hand side, you have not yet arrived at a closed-form solution for \\(b\\)!\n\nFinally, once I have these two closed-form solutions for \\(m\\) and \\(b\\), I then take the Statistics Things back out and try to rewrite the terms in this closed form solution in terms of \\(\\overline{x}\\), \\(\\overline{y}\\), \\(\\text{Var}[X]\\), \\(\\text{Var}[Y]\\), and \\(\\text{Cov}[X,Y]\\).\nThis may not help in terms of the particular point at which you may be stuck, in which case I‚Äôm sorry in advance! But, it‚Äôs the‚Ä¶ division-of-tasks that tended to help me when deriving closed-form solutions like this in e.g.¬†econometrics classes!"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nWeek\n\n\nAssignment\n\n\n\n\n\n\nGetting Started with Lab 9\n\n\n11\n\n\nLab 9\n\n\n\n\nGetting Started with HW 3\n\n\n9\n\n\nHW 3\n\n\n\n\nDSAN 5300 Exam 2 Study Guide\n\n\n8\n\n\nQuiz 2\n\n\n\n\nGetting Started with HW 2\n\n\n5\n\n\nHW 2\n\n\n\n\nGetting Started with HW 1\n\n\n3\n\n\nHW 1\n\n\n\n\nUsing Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D\n\n\n3\n\n\nGeneral\n\n\n\n\nQuiz 1 Study Guide\n\n\n2\n\n\nQuiz 1\n\n\n\n\nGetting Started with Lab 1\n\n\n2\n\n\nLab 1\n\n\n\n\nExtra Slides: A Slightly Deeper Dive Into Machine Learning\n\n\n2\n\n\nGeneral\n\n\n\n\nRegression vs.¬†PCA\n\n\n2\n\n\nGeneral\n\n\n\n\nMathematical Optimization\n\n\n1\n\n\nGeneral\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Writeups"
    ]
  },
  {
    "objectID": "writeups/quiz-2/index.html",
    "href": "writeups/quiz-2/index.html",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "",
    "text": "Here the idea is that, we will be evaluating your understanding of different ways to split a dataset: how the split is performed and why we might prefer a particular splitting approach in a particular data-analysis scenario.\n\n\nThis is not a 5300 topic on its own‚Äîyou learned it in DSAN 5000!‚Äîbut is nonetheless important to take note of here since its the basis for why we use Cross-Validation for model evaluation in 5300 (e.g., for tuning the hyperparameters of the models we look at each week).\nIt turns out to be an‚Ä¶ oddly deep topic (hence why Jeff spent too long ranting about it in his section during Week 4), but the key rationale for why we split our full dataset into a Training Set and a Test Set boils down to the fact that our fundamental goal in statistical learning is to‚Ä¶\n\nFind a prediction function \\(\\widehat{f}(x)\\) that‚Ä¶\nDoes a good job predicting labels \\(y_i\\) based on features \\(x_i\\)‚Ä¶\nFor data \\((x_i, y_i)\\) that has not (yet) been observed\n\nThis contrasts with the more ‚Äúna√Øve‚Äù goal of optimizing predictions of \\(y_i\\) relative to already-observed training datapoints.\nSo, under this framing of our goal, we set aside a small proportion (usually 20%) of the data as our Test Set, which we do not look at while training the model, so that it can instead be used to evaluate the model‚Äôs performance on unseen data.\n\n\n\nGiven that setup, then, the different versions of Cross-Validation all fall under the general rubric of: generating ‚Äúmini‚Äù training sets (sub-training sets) and test sets (validation sets), as smaller subsets of the full training data, in order to estimate how well a given model might perform on unseen data before we move to the final test-set-based evaluation once we have settled upon and trained a final version of our model.\nThe key versions of Cross-Validation to know are:\n\nThe Validation Set approach,\nLeave-One-Out Cross-Validation (LOOCV), and\n\\(K\\)-Fold Cross-Validation.\n\nFor example, you should understand the relationship between \\(K\\)-Fold CV and LOOCV: that LOOCV is exactly just \\(K\\)-Fold CV with \\(K\\) set to be equal to the number of datapoints \\(n\\)."
  },
  {
    "objectID": "writeups/quiz-2/index.html#part-1-data-splitting",
    "href": "writeups/quiz-2/index.html#part-1-data-splitting",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "",
    "text": "Here the idea is that, we will be evaluating your understanding of different ways to split a dataset: how the split is performed and why we might prefer a particular splitting approach in a particular data-analysis scenario.\n\n\nThis is not a 5300 topic on its own‚Äîyou learned it in DSAN 5000!‚Äîbut is nonetheless important to take note of here since its the basis for why we use Cross-Validation for model evaluation in 5300 (e.g., for tuning the hyperparameters of the models we look at each week).\nIt turns out to be an‚Ä¶ oddly deep topic (hence why Jeff spent too long ranting about it in his section during Week 4), but the key rationale for why we split our full dataset into a Training Set and a Test Set boils down to the fact that our fundamental goal in statistical learning is to‚Ä¶\n\nFind a prediction function \\(\\widehat{f}(x)\\) that‚Ä¶\nDoes a good job predicting labels \\(y_i\\) based on features \\(x_i\\)‚Ä¶\nFor data \\((x_i, y_i)\\) that has not (yet) been observed\n\nThis contrasts with the more ‚Äúna√Øve‚Äù goal of optimizing predictions of \\(y_i\\) relative to already-observed training datapoints.\nSo, under this framing of our goal, we set aside a small proportion (usually 20%) of the data as our Test Set, which we do not look at while training the model, so that it can instead be used to evaluate the model‚Äôs performance on unseen data.\n\n\n\nGiven that setup, then, the different versions of Cross-Validation all fall under the general rubric of: generating ‚Äúmini‚Äù training sets (sub-training sets) and test sets (validation sets), as smaller subsets of the full training data, in order to estimate how well a given model might perform on unseen data before we move to the final test-set-based evaluation once we have settled upon and trained a final version of our model.\nThe key versions of Cross-Validation to know are:\n\nThe Validation Set approach,\nLeave-One-Out Cross-Validation (LOOCV), and\n\\(K\\)-Fold Cross-Validation.\n\nFor example, you should understand the relationship between \\(K\\)-Fold CV and LOOCV: that LOOCV is exactly just \\(K\\)-Fold CV with \\(K\\) set to be equal to the number of datapoints \\(n\\)."
  },
  {
    "objectID": "writeups/quiz-2/index.html#part-2-stepwise-model-selection",
    "href": "writeups/quiz-2/index.html#part-2-stepwise-model-selection",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "Part 2: Stepwise Model Selection",
    "text": "Part 2: Stepwise Model Selection\n\nBest Subset Selection\nForward Stepwise Selection\nBackward Stepwise Selection\n\nHere you can focus on, e.g., what are the relative strengths and weakness of these approaches? In particular, you should understand how:\n\nBest Subset Selection is guaranteed to find an optimal subset of features, but is usually prohibitively slow/inefficient, since it searches over all \\(\\binom{p}{1} + \\binom{p}{2} + \\cdots + \\binom{p}{p}\\) possible subsets of features, whereas\nForward and Backward Stepwise Selection are computationally tractable but at the cost of not being guaranteed to find an optimal subset of features."
  },
  {
    "objectID": "writeups/quiz-2/index.html#part-3-regularization-and-lp-norms",
    "href": "writeups/quiz-2/index.html#part-3-regularization-and-lp-norms",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "Part 3: Regularization and \\(L^p\\)-Norms",
    "text": "Part 3: Regularization and \\(L^p\\)-Norms\nHere the most helpful things to study are as follows:\nWhich particular norms are used in which particular regularization methods?\n\nLasso penalizes model complexity via a penalty \\(\\lambda\\) on \\(\\|\\boldsymbol{\\beta}\\|_1\\), the \\(L^1\\) norm of \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)\\)\nRidge Regression penalizes model complexity via a penalty \\(\\lambda\\) on \\(\\|\\boldsymbol{\\beta}\\|_2^2\\), the squared \\(L^2\\) norm of \\(\\boldsymbol{\\beta}\\)\nElastic Net Regularization ‚Äúaverages‚Äù these approaches, in a sense, by penalizing model complexity via two penalty values:\n\n\\(\\lambda_1\\) as a penalty on \\(\\|\\boldsymbol{\\beta}\\|_1\\) and $ \\(\\lambda_2\\) as a penalty on \\(\\|\\boldsymbol{\\beta}\\|_2^2\\)\n\n\nWhat do the different \\(L^p\\) constraints ‚Äúlook like‚Äù, in terms of how they constrain the full space of possible model parameters \\(\\boldsymbol\\beta\\)?\nHere, as a helpful/fun way to prepare, is a practice problem for this part:\nAssume we are estimating a regression with two parameters \\(\\beta_1\\) and \\(\\beta_2\\), both real numbers so that the space of estimates is \\(\\mathbb{R}^2\\) (we can imagine \\(\\beta_1\\) as the \\(x\\)-axis in this space and \\(\\beta_2\\) as the \\(y\\)-axis). Each \\(L^p\\)-norm induces a ‚Äúunit circle‚Äù within this space, defined to be the set of points \\(\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)\\) for which \\(\\|\\boldsymbol{\\beta}\\|_p = 1\\).\nSince the \\(L^2\\) norm corresponds to ‚Äústandard‚Äù Euclidean distance, for example, the shape of the unit circle it induces is the shape we typically already call ‚Äúthe unit circle‚Äù in geometry:\n\n\n\nFrom this slide\n\n\nThe \\(L^1\\) norm, however, induces a different unit circle: the set of all points exactly 1 unit away from the origin in \\(L^1\\) space looks like a diamond:\n\n\n\nFrom this slide\n\n\nPractice Problem 3A:\n\nWhat do the unit circles look like for values of \\(p\\) where \\(0 &lt; p &lt; 1\\)? (This was discussed in the Google Space, but it can be helpful to think of why the unit circle looks this way)\nThe unit disks (the spaces enclosed by the unit circles) in \\(L^1\\) and \\(L^2\\) space are convex, meaning that if you pick any two points \\(\\boldsymbol\\beta_A\\) and \\(\\boldsymbol\\beta_B\\) within the space and draw a line between them, any point along this line is also within the space.1 Are the unit circles produced when \\(0 &lt; p &lt; 1\\) also convex?\n\nPractice Problem 3B:\nThe definition of the \\(L^p\\) norm,\n\\[\n\\|\\boldsymbol\\beta\\|_p = \\left( \\sum_{j=1}^{J}|\\beta_j|^p \\right)^{\\frac{1}{p}},\n\\]\nworks fine for deriving (e.g.) the \\(L^2\\), \\(L^1\\), and \\(L^{2/3}\\) norms. But there is one additional widely-used norm, the \\(L^\\infty\\) norm, that we can‚Äôt exactly derive by ‚Äúplugging in‚Äù \\(\\infty\\), but can easily derive by taking the limit as \\(p \\rightarrow \\infty\\):\n\\[\n\\|\\boldsymbol\\beta\\|_\\infty \\overset{\\small \\text{def}}{=} \\lim_{p \\rightarrow \\infty} \\|\\boldsymbol\\beta\\|_p\n\\]\n\nShow (or, just, try your best to show! Or look up!) that this limit can be evaluated to arrive at the closed-form expression \\(\\|\\boldsymbol\\beta\\|_\\infty = \\max\\{\\beta_0, \\beta_1, \\ldots, \\beta_J\\}\\)\nMore importantly: what does the unit circle induced by this norm look like? Try to draw it on an \\(xy\\)-plane, the same way you would draw a unit circle or the diamond induced by the \\(L^1\\) norm."
  },
  {
    "objectID": "writeups/quiz-2/index.html#part-4-basis-functions",
    "href": "writeups/quiz-2/index.html#part-4-basis-functions",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "Part 4: Basis Functions",
    "text": "Part 4: Basis Functions\nIn Week 7 we introduced the notion of a set of basis functions \\(\\{b_0(X), b_1(X), \\ldots, b_D(X)\\}\\) as a method for writing different types of regressions (e.g., linear regression, polynomial regression, piecewise regression, and regression splines) in a single form.\nA degree-\\(d\\) polynomial regression, for example, can be thought of as a simple linear regression on \\(d\\) basis functions \\(\\{b_0(X) = 1, b_1(X) = X, b_2(X) = X^2, \\ldots, b_d(X) = X^d\\}\\), so that estimating parameters \\(\\beta_0\\) through \\(\\beta_d\\) gives us\n\\[\nY = \\beta_0 b_0(X) + \\beta_1 b_1(X) + \\cdots + \\beta_d b_d(X) = \\beta_0 + \\beta_1 X + \\cdots + \\beta_d X^d,\n\\]\nthe standard form for a degree-\\(d\\) polynomial regression.\nFor this portion, if we provide you with a ‚Äútrue‚Äù Data-Generating Process \\(f(x)\\), you should be able to identify whether or not performing regression on a particular set of basis functions \\(\\mathcal{B} = \\{b_0(X), b_1(X), \\ldots\\}\\) would allow you to learn the true DGP \\(f(x)\\).\nFor example, if the true DGP underlying a given dataset was\n\\[\nf(x) = 2^x,\n\\]\nbut the set of basis functions used for a regression was \\(\\mathcal{B} = \\{x, x^2\\}\\), then this regression would not be able to learn the true DGP, since there are no numeric constants \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) that it could learn which would produce\n\\[\n2^x = \\beta_0 + \\beta_1 x + \\beta_2 x^2.\n\\]\nHowever, if the true DGP was\n\\[\nf(x) = \\log_2(x) + 3,\n\\]\nand we employed the basis functions \\(\\mathcal{B}' = \\{x, x^2, \\ln(x)\\}\\), we could learn the true DGP using regression, since learning the numeric constants \\(\\widehat{\\beta}_0 = 3\\), \\(\\widehat{\\beta}_1 = 0\\), \\(\\widehat{\\beta}_2 = 0\\), and \\(\\widehat{\\beta}_3 = \\frac{1}{\\ln(2)}\\) would recover \\(f(x)\\):\n\\[\n\\log_2(x) + 3 = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x + \\widehat{\\beta}_2 x^2 + \\widehat{\\beta}_3 \\ln(x) = 3 + 0 + 0 + \\frac{\\ln(x)}{\\ln(2)} ‚úÖ\n\\]"
  },
  {
    "objectID": "writeups/quiz-2/index.html#part-5-splines",
    "href": "writeups/quiz-2/index.html#part-5-splines",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "Part 5: Splines",
    "text": "Part 5: Splines\nThe basic approach to data modeling underlying the use of splines is as follows:\n\n‚ÄúChop‚Äù the feature space (the domain of \\(\\mathbf{x}\\)) at \\(K\\) points \\(\\Xi = \\{\\xi_1, \\xi_2, \\ldots, \\xi_k\\}\\)2, called ‚Äúknot points‚Äù, thus producing \\(K + 1\\) separate pieces of the original domain of \\(\\mathbf{x}\\)\nModel each of these \\(K + 1\\) pieces individually, using tools from Weeks 1-6\nJoin the \\(K + 1\\) resulting prediction functions together in a smooth way\n\nHere the term ‚Äúsmooth‚Äù in Step 3 is typically (99.9% of the time) operationalized to mean that the final joined-together prediction function should have a well-defined second derivative at each knot point \\(\\xi_k\\). This is why, when a data scientist says ‚Äúspline‚Äù in general, they usually are referring specifially to cubic splines:\n\n\n\nTable¬†1: First and second derivatives of polynomials evaluated at an arbitrary point \\(\\xi_k\\)\n\n\n\n\n\n\n\n\n\n\nDegree\nPolynomial\nDerivatives at \\(x = \\xi_k\\)\n\n\n\n\n0\n\\(f(x) = c_0 x^0 = c_0\\)\n\\(f'(\\xi_k) = 0\\)\\(f''(\\xi_k) = 0\\)\n\n\n1\n\\(f(\\xi_k) = c_0 x^0 + c_1 x^1 = c_0 + c_1x\\)\n\\(f'(\\xi_k) = c_1\\)\\(f''(\\xi_k) = 0\\)\n\n\n2\n\\(f(x) = c_0 x^0 + c_1 x^1 + c_2 x^2\\)\n\\(f'(\\xi_k) = c_1 + 2c_2 \\xi_k\\)\\(f''(\\xi_k) = 2c_2\\)\n\n\n3\n\\(f(x) = c_0 x^0 + c_1 x^1 + c_2 x^2 + c_3 x^3\\)\n\\(f'(\\xi_k) = c_1 + 2c_2 \\xi_k + 3c_3 \\xi_k^2\\)\\(f''(\\xi_k) = 2c_2 + 6c_3 \\xi_k\\)\n\n\n\n\n\n\nFrom this table we can see how degree-3 cubic polynomials are the ‚Äúsimplest‚Äù (in terms of lowest degree) polynomials for which we could learn coefficients \\(c_0\\) through \\(c_3\\) that would enable joining different pieces together to have a well-defined second derivative at all knot points \\(\\xi_k\\)3. And, in fact, we don‚Äôt even need to learn 4 separate coefficients \\(c_0\\), \\(c_1\\), \\(c_2\\), and \\(c_3\\) to achieve this: by looking at the form of \\(f''(x)\\) in the final row of Table¬†1, we see that by just learning a coefficient \\(c_3\\) on the cubic term (\\(x^3\\)) at each knot point, we can ensure that the regression solution \\(\\widehat{c_3}\\) (for example, the OLS estimate \\(\\widehat{\\beta}_j\\) for a term \\(\\beta_j \\xi_k^3\\) on the RHS of a regression model)\nThis tells us (for reasons that we talked about in more depth during lecture) that we can join our \\(K + 1\\) separately-modeled ‚Äúpieces‚Äù together in a smooth way by adding the truncated power basis functions\n\\[\n\\mathcal{B}_{\\text{TP}} = \\{(x - \\xi_1)^3_+, (x - \\xi_2)^3_+, \\ldots, (x - \\xi_K)^3_+\\}\n\\]\nto any existing regression model with existing basis functions \\(\\mathcal{B}\\), where the \\(+\\) in the subscript after the parentheses denotes the truncated power function:\n\\[\n(x - \\xi)^3_+ \\overset{\\small \\text{def}}{=} \\begin{cases}\n0 &\\text{if }x - \\xi \\leq 0 \\\\\n(x - \\xi)^3 &\\text{if }x - \\xi &gt; 0\n\\end{cases}\n\\]\nThus, for example, if we were fitting a regression model using one of the example bases given in the previous part:\n\\[\n\\mathcal{B}' = \\{x, x^2, \\ln(x)\\},\n\\]\nbut decided that in fact we want to fit this model separately for datapoints with \\(x_i \\leq 100^\\circ \\mathrm{C}\\) and \\(x_i &gt; 100^\\circ \\mathrm{C}\\), we can ‚Äúautomatically‚Äù achieve this by performing a new regression with basis functions\n\\[\n\\mathcal{B}'' = \\{x, x^2, \\ln(x), (x-100)^3_+\\}.\n\\]\nThis ‚Äúforces‚Äù the regression to now estimate some parameter \\(\\beta_4\\) as a coefficient for \\((x-100)^3_+\\), ensuring that resulting prediction function joins the two ‚Äúpieces‚Äù at \\(x = 100^\\circ \\textrm{C}\\) in a smooth way.\nGiven these pieces, some takeaways for yall in terms of what you can study is as follows:\nHow should the knot points \\(\\xi_k\\) themselves be chosen?\n\nIf we have a pre-existing reason (with respect to our theory or hypothesis that we‚Äôre bringing to the data) to expect different behavior in different regions of the domain of some feature \\(\\mathbf{x}\\), we can opt to ‚Äúmanually‚Äù place knot points to separate the full domain into these regions\nIn the above example, \\(\\xi_1 = 100^\\circ \\mathrm{C}\\) may be chosen because we‚Äôre studying water, which (our theory tells us) undergoes a phase change at that temperature.\nIn the absence of this type of pre-existing reason, however, we more-commonly choose both the number of knot points \\(K\\) and their locations \\(\\{\\xi_1, \\ldots, \\xi_k\\}\\) via Cross-Validation\n\nHow exactly does adding the truncated power function \\((x - \\xi_k)^3_+\\) as a new basis function allow us to ‚Äúsplit‚Äù the original regression into pieces (constrained to join together smoothly)?\n\nThe parenthetical part we already discussed above (that smoothness is ‚Äúenforced‚Äù by the \\(3\\) in the exponent), but the splitting is a separate ‚Äúfeature‚Äù of these truncated power bases.\nFor intuition, rather than thinking of these as separate/new features, it can help to think of them as ‚Äúslope modifiers‚Äù: if our prediction function is\n\\[\n\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_i + \\widehat{\\beta}_2(x_i - \\xi_1)^3_+,\n\\]\nthen the slope of this prediction function will be \\(\\widehat{\\beta}_1\\) up until the point \\(x = \\xi_1\\), after which the slope will become \\(\\widehat{\\beta}_1 + \\widehat{\\beta}_2\\)."
  },
  {
    "objectID": "writeups/quiz-2/index.html#footnotes",
    "href": "writeups/quiz-2/index.html#footnotes",
    "title": "DSAN 5300 Exam 2 Study Guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe haven‚Äôt talked about convex optimization in this class, but it turns out to be a very important feature in terms of whether or not we can efficiently optimize a given function within a given space‚Ä¶‚Ü©Ô∏é\nThe symbol \\(\\Xi\\) is the capitalized form, and \\(\\xi\\) the lowercased form, of the Greek letter ‚Äúxi‚Äù, pronounced like ‚Äúksy‚Äù‚Äîlike saying ‚Äúsigh‚Äù immediately after a quick ‚Äúk‚Äù sound.‚Ü©Ô∏é\nIn a Calculus class, we learn (as basically a helpful rule-of-thumb definition) that a function \\(f(x)\\) has a ‚Äúwell-defined‚Äù second derivative at a given point \\(\\xi\\) if \\(\\lim_{x \\rightarrow \\xi^+}f''(x) = \\lim_{x \\rightarrow \\xi^-}f''(x)\\), i.e., if the second derivative has the same value approaching \\(\\xi\\) from below and approaching \\(\\xi\\) from above. If we go on to take a Real Analysis class this picture gets complicated a bit to arrive at a fully-workable definition (by bringing in infima and suprema), but the intuition from Calculus class should be fine for this class!‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/machine-learning/index.html",
    "href": "writeups/machine-learning/index.html",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "writeups/machine-learning/index.html#supervised-vs.-unsupervised-learning",
    "href": "writeups/machine-learning/index.html#supervised-vs.-unsupervised-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Supervised vs.¬†Unsupervised Learning",
    "text": "Supervised vs.¬†Unsupervised Learning\n\n\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\nSupervised Learning: You want the computer to learn the existing pattern of how you are classifying1 observations\n\nDiscovering the relationship between properties of data and outcomes\nExample (Binary Classification): I look at homes on Zillow, saving those I like to folder A and don‚Äôt like to folder B\nExample (Regression): I assign a rating of 0-100 to each home\nIn both cases: I ask the computer to learn my schema (how I classify)\n\n\nUnsupervised Learning: You want the computer to find patterns in a dataset, without any prior classification info\n\nTypically: grouping or clustering observations based on shared properties\nExample (Clustering): I save all the used car listings I can find, and ask the computer to ‚Äúfind a pattern‚Äù in this data, by clustering similar cars together"
  },
  {
    "objectID": "writeups/machine-learning/index.html#dataset-structures",
    "href": "writeups/machine-learning/index.html#dataset-structures",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures",
    "text": "Dataset Structures\n\n\nSupervised Learning: Dataset has both explanatory variables (‚Äúfeatures‚Äù) and response variables (‚Äúlabels‚Äù)\n\n\nsup_data &lt;- tibble::tribble(\n  ~home_id, ~sqft, ~bedrooms, ~rating,\n  0, 1000, 1, \"Disliked\",\n  1, 2000, 2, \"Liked\",\n  2, 2500, 1, \"Liked\",\n  3, 1500, 2, \"Disliked\",\n  4, 2200, 1, \"Liked\"\n)\nsup_data\n\n# A tibble: 5 √ó 4\n  home_id  sqft bedrooms rating  \n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1       0  1000        1 Disliked\n2       1  2000        2 Liked   \n3       2  2500        1 Liked   \n4       3  1500        2 Disliked\n5       4  2200        1 Liked   \n\n\n\n\nUnsupervised Learning: Dataset has only explanatory variables (‚Äúfeatures‚Äù)\n\n\nunsup_data &lt;- tibble::tribble(\n  ~home_id, ~sqft, ~bedrooms,\n  0, 1000, 1,\n  1, 2000, 2,\n  2, 2500, 1,\n  3, 1500, 2,\n  4, 2200, 1\n)\nunsup_data\n\n# A tibble: 5 √ó 3\n  home_id  sqft bedrooms\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1       0  1000        1\n2       1  2000        2\n3       2  2500        1\n4       3  1500        2\n5       4  2200        1"
  },
  {
    "objectID": "writeups/machine-learning/index.html#dataset-structures-visualized",
    "href": "writeups/machine-learning/index.html#dataset-structures-visualized",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures: Visualized",
    "text": "Dataset Structures: Visualized\n\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    title = \"Supervised Data: House Listings\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  dsan_theme(\"half\")\n\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# To force a legend\nunsup_grouped &lt;- unsup_data |&gt; mutate(big=bedrooms &gt; 1)\nunsup_grouped[['big']] &lt;- factor(unsup_grouped[['big']], labels=c(\"?1\",\"?2\"))\nggplot(unsup_grouped, aes(x=sqft, y=bedrooms, fill=big)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    fill = \"?\"\n  ) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  ggtitle(\"Unsupervised Data: House Listings\") +\n  theme(legend.background = element_rect(fill=\"white\", color=\"white\"), legend.box.background = element_rect(fill=\"white\"), legend.text = element_text(color=\"white\"), legend.title = element_text(color=\"white\"), legend.position = \"right\") +\n  scale_fill_discrete(labels=c(\"?\",\"?\")) +\n  #scale_color_discrete(values=c(\"white\",\"white\"))\n  scale_color_manual(name=NULL, values=c(\"white\",\"white\")) +\n  #scale_color_manual(values=c(\"?1\"=\"white\",\"?2\"=\"white\"))\n  guides(fill = guide_legend(override.aes = list(shape = NA)))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#different-goals",
    "href": "writeups/machine-learning/index.html#different-goals",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Different Goals",
    "text": "Different Goals\n\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    title = \"Supervised Data: House Listings\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  geom_vline(xintercept = 1750, linetype=\"dashed\", color = \"black\", size=1) +\n  annotate('rect', xmin=-Inf, xmax=1750, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[1]) +\n  annotate('rect', xmin=1750, xmax=Inf, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[2])\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n  #geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf, alpha=.2, fill='red'))\n\n\n\nlibrary(ggforce)\nggplot(unsup_grouped, aes(x=sqft, y=bedrooms)) +\n  #scale_color_brewer(palette = \"PuOr\") +\n  geom_mark_ellipse(expand=0.1, aes(fill=big), size = 1) +\n  geom_point(size=g_pointsize * 2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    fill = \"?\"\n  ) +\n  dsan_theme(\"half\") +\n  ggtitle(\"Unsupervised Data: House Listings\") +\n  #theme(legend.position = \"none\") +\n  #theme(legend.title = text_element(\"?\"))\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(cbPalette[3],cbPalette[4]), labels=c(\"?\",\"?\"))\n\n\n\n\n\n\n\n  #scale_fill_manual(labels=c(\"?\",\"?\"))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#the-learning-in-machine-learning",
    "href": "writeups/machine-learning/index.html#the-learning-in-machine-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The ‚ÄúLearning‚Äù in Machine Learning",
    "text": "The ‚ÄúLearning‚Äù in Machine Learning\n\nGiven these datasets, how do we learn the patterns?\nNa√Øve idea: Try random lines (each forming a decision boundary), pick ‚Äúbest‚Äù one\n\n\nx_min &lt;- 0\nx_max &lt;- 3000\ny_min &lt;- -1\ny_max &lt;- 3\nrand_y0 &lt;- runif(50, min=y_min, max=y_max)\nrand_y1 &lt;- runif(50, min=y_min, max=y_max)\nrand_slope &lt;- (rand_y1 - rand_y0)/(x_max - x_min)\nrand_intercept &lt;- rand_y0\nrand_lines &lt;- tibble::tibble(id=1:50, slope=rand_slope, intercept=rand_intercept)\n#ggplot() +\n#  geom_abline(data=rand_lines, aes(slope=slope, #intercept=intercept)) +\n#  xlim(0,3000) +\n#  ylim(0,2) +\n#  dsan_theme()\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size=g_pointsize) +\n  labs(\n    title = \"The Like vs. Dislike Boundary: 50 Guesses\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  geom_abline(data=rand_lines, aes(slope=slope, intercept=intercept), linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\nWhat parameters are we choosing when we draw a random line? Random curve?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#what-makes-a-goodbest-guess",
    "href": "writeups/machine-learning/index.html#what-makes-a-goodbest-guess",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?",
    "text": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?\n\nWhat‚Äôs your intuition? How about accuracy‚Ä¶ ü§î\n\n\nline_data &lt;- tibble::tribble(\n  ~id, ~slope, ~intercept,\n  0, 0, 0.75,\n  1, 0.00065, 0.5\n)\ndata_range &lt;- 800:2700\nribbon_range &lt;- c(-Inf,data_range,Inf)\nf1 &lt;- function(x) { return(0*x + 0.75) }\nf1_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3100)))\ng1_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==0))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 1: 60% Accuracy\") +\n  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Disliked\"), alpha=0.2) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Liked\"), alpha=0.2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\n\nlibrary(patchwork)\nf2 &lt;- function(x) { return(0.00065*x + 0.5) }\nf2_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f2(710),f2(data_range),f2(Inf)))\ng2_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==1))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 2: 60% Accuracy\") +\n  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\"\n  ) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Liked\"), alpha=0.2) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Disliked\"), alpha=0.2) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\ng1_plot + g2_plot\n\n\n\n\n\n\n\n\nSo‚Ä¶ what‚Äôs wrong here?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#whats-wrong-with-accuracy",
    "href": "writeups/machine-learning/index.html#whats-wrong-with-accuracy",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What‚Äôs Wrong with Accuracy?",
    "text": "What‚Äôs Wrong with Accuracy?\n\ngen_homes &lt;- function(n) {\n  rand_sqft &lt;- runif(n, min=2000, max=3000)\n  rand_bedrooms &lt;- sample(c(1,2), size=n, prob=c(0.5,0.5), replace=TRUE)\n  rand_ids &lt;- 1:n\n  rand_rating &lt;- \"Liked\"\n  rand_tibble &lt;- tibble::tibble(home_id=rand_ids, sqft=rand_sqft, bedrooms=rand_bedrooms, rating=rand_rating)\n  return(rand_tibble)\n}\nfake_homes &lt;- gen_homes(18)\nfake_sup_data &lt;- dplyr::bind_rows(sup_data, fake_homes)\nline_data &lt;- tibble::tribble(\n  ~id, ~slope, ~intercept,\n  0, 0, 0.75,\n  1, 0.00065, 0.5\n)\nf1 &lt;- function(x) { return(0*x + 0.75) }\nf2 &lt;- function(x) { return(0.00065*x + 0.5) }\n# And check accuracy\nfake_sup_data &lt;- fake_sup_data %&gt;% mutate(boundary1=f1(sqft)) %&gt;% mutate(guessDislike1 = bedrooms &lt; boundary1) %&gt;% mutate(correct1 = ((rating==\"Disliked\") & (guessDislike1)) | (rating==\"Liked\") & (!guessDislike1))\nfake_sup_data &lt;- fake_sup_data %&gt;% mutate(boundary2=f2(sqft)) %&gt;% mutate(guessDislike2 = bedrooms &gt; boundary2) %&gt;% mutate(correct2 = ((rating==\"Disliked\") & (guessDislike2)) | (rating==\"Liked\") & (!guessDislike2))\n\ndata_range &lt;- 800:2700\nribbon_range &lt;- c(-Inf,data_range,Inf)\n\nf1_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3200)))\ng1_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==0))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 1: 91.3% Accuracy\") +\n  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct1, levels=c(TRUE,FALSE))), size=g_pointsize) +\n  scale_shape_manual(values=c(24, 25)) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Disliked\"), alpha=0.2) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Liked\"), alpha=0.2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\",\n    shape = \"Correct Guess\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\n\nf2_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f2(700),f2(data_range),f2(3100)))\ng2_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==1))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 2: 73.9% Accuracy\") +\n  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct2, levels=c(TRUE,FALSE))), size=g_pointsize) +\n  scale_shape_manual(values=c(24, 25)) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\",\n    shape = \"Correct Guess\"\n  ) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Liked\"), alpha=0.2) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Disliked\"), alpha=0.2) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\ng1_plot + g2_plot"
  },
  {
    "objectID": "writeups/machine-learning/index.html#the-oversimplified-big-picture",
    "href": "writeups/machine-learning/index.html#the-oversimplified-big-picture",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The (Oversimplified) Big Picture",
    "text": "The (Oversimplified) Big Picture\n\nA model: some representation of something in the world\n\n\n\n\n\nHow well does our model represent the world?2 \\(\\mathsf{Correspondence}(y_{obs}, \\theta)\\)\n\\(P\\left(y_{obs}, \\theta\\right)\\), \\(P\\left(\\theta \\; | \\; y_{obs}\\right)\\), \\(P\\left(y_{obs} \\; | \\; \\theta\\right)\\)3\nMaximum Likelihood Estimation?\n\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\n\"Nature\"\n\n\ncluster_02\n\n\"Science\"\n\n\n\nObs\n\nThing(s) we can see\n\n\n\nUnd\n\nUnderlying process\n\n\n\nUnd-&gt;Obs\n\n\n\n\n\nModel\n\nModel\n\n\n\nUnd-&gt;Model\n\n\n\n\n\nModel-&gt;Obs\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathsf{Correspondence}(y_{obs}, \\theta) &\\equiv P(y = y_{obs}, \\theta) \\\\\nP(y = y_{obs}, \\theta) &= P(y=y_{obs} \\; | \\; \\theta)P(\\theta) \\\\\n&\\propto P\\left(y = y_{obs} \\; | \\; \\theta\\right)\\ldots \\implies \\text{(maximize this!)}  \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#measuring-errors-f1-score",
    "href": "writeups/machine-learning/index.html#measuring-errors-f1-score",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: F1 Score",
    "text": "Measuring Errors: F1 Score\n\nHow can we reward guesses which best discriminate between classes?\n\n\\[\n\\begin{align*}\n\\mathsf{Precision} &= \\frac{\\# \\text{true positives}}{\\# \\text{predicted positive}} = \\frac{tp}{tp+fp} \\\\[1.5em]\n\\mathsf{Recall} &= \\frac{\\# \\text{true positives}}{\\# \\text{positives in data}} = \\frac{tp}{tp+fn} \\\\[1.5em]\nF_1 &= 2\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision} + \\mathsf{Recall}} = \\mathsf{HMean}(\\mathsf{Precision}, \\mathsf{Recall})\n\\end{align*}\n\\]\n\nThink about: How does this address/fix issue with accuracy?\n\n\n\nHere \\(\\mathsf{HMean}\\) is the Harmonic Mean function: see appendix slide or Wikipedia."
  },
  {
    "objectID": "writeups/machine-learning/index.html#measuring-errors-the-loss-function",
    "href": "writeups/machine-learning/index.html#measuring-errors-the-loss-function",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: The Loss Function",
    "text": "Measuring Errors: The Loss Function\n\nWhat about regression?\n\nNo longer just ‚Äútrue prediction good, false prediction bad‚Äù\n\nWe have to quantify how bad the guess is! Then we can scale the penalty accordingly: \\(\\text{penalty} \\propto \\text{badness}\\)\nEnter Loss Functions! Just distances (using distance metrics you‚Äôve already seen) between the true value and our guess:\n\nSquared Error \\(L^2(y_{obs}, y_{pred}) = (y_{obs} - y_{pred})^2\\)\nKullback-Leibler Divergence if guessing distributions"
  },
  {
    "objectID": "writeups/machine-learning/index.html#calculus-rears-its-ugly-head",
    "href": "writeups/machine-learning/index.html#calculus-rears-its-ugly-head",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Rears its Ugly Head",
    "text": "Calculus Rears its Ugly Head\n\nNeural networks use derivatives/gradients to improve their predictions given a particular loss function.\n\n\n\n\nbase &lt;-\n  ggplot() +\n  xlim(-5, 5) +\n  ylim(0, 25) +\n  labs(\n    x = \"Y[obs] - Y[pred]\",\n    y = \"Prediction Badness (Loss)\"\n  ) +\n  dsan_theme()\n\nmy_fn &lt;- function(x) { return(x^2) }\nmy_deriv2 &lt;- function(x) { return(4*x - 4) }\nmy_derivN4 &lt;- function(x) { return(-8*x - 16) }\nbase + geom_function(fun = my_fn, color=cbPalette[1], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_deriv2, color=cbPalette[2], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-4,y=16)), aes(x=x,y=y), color=cbPalette[3], size=g_pointsize/2) + \n  geom_function(fun = my_derivN4, color=cbPalette[3], linewidth=1)\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 70 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\n\nmy_fake_deriv &lt;- function(x) { return(-x) }\nmy_fake_deriv2 &lt;- function(x) { return(-(1/2)*x + 1/2) }\nmy_fake_deriv3 &lt;- function(x) { return(-(1/4)*x + 3/4) }\n\nd=data.frame(x=c(-2,-1,0,1,2), y=c(1,0,0,1,1))\nbase &lt;- ggplot() +\n  xlim(-5,5) +\n  ylim(0,2) +\n  labs(\n    x=\"Y[obs] - Y[pred]\",\n    y=\"Prediction Badness (Loss)\"\n  ) +\n  geom_step(data=d, mapping=aes(x=x, y=y), linewidth=1) +\n  dsan_theme()\n\nbase + geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_fake_deriv, color=cbPalette[2], linewidth=1) +\n  geom_function(fun = my_fake_deriv2, color=cbPalette[3], linewidth=1) +\n  geom_function(fun = my_fake_deriv3, color=cbPalette[4], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-1,y=1)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) +\n  annotate(\"text\", x = -0.7, y = 1.1, label = \"?\", size=6)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 80 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCan we just use the \\(F_1\\) score?\n\n\\[\n\\frac{\\partial F_1(weights)}{\\partial weights} = \\ldots \\; ? \\; \\ldots üíÄ\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#quantifying-discrete-loss",
    "href": "writeups/machine-learning/index.html#quantifying-discrete-loss",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Quantifying Discrete Loss",
    "text": "Quantifying Discrete Loss\n\nWe can quantify a differentiable discrete loss by asking the algorithm how confident it is\n\nCloser to 0 \\(\\implies\\) more confident that the true label is 0\nCloser to 1 \\(\\implies\\) more confident that the true label is 1\n\n\n\\[\n\\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\\log(y_{pred}) + (1-y_{obs})\\log(1-y_{pred}))\n\\]\n\ny_pred &lt;- seq(from = 0, to = 1, by = 0.001)\ncompute_ce &lt;- function(y_p, y_o) { return(-(y_o * log(y_p) + (1-y_o)*log(1-y_p))) }\nce0 &lt;- compute_ce(y_pred, 0)\nce1 &lt;- compute_ce(y_pred, 1)\nce0_data &lt;- tibble::tibble(y_pred=y_pred, y_obs=0, ce=ce0)\nce1_data &lt;- tibble::tibble(y_pred=y_pred, y_obs=1, ce=ce1)\nce_data &lt;- dplyr::bind_rows(ce0_data, ce1_data)\nggplot(ce_data, aes(x=y_pred, y=ce, color=factor(y_obs))) +\n  geom_line(linewidth=1) +\n  labs(\n    title=\"Binary Cross-Entropy Loss\",\n    x = \"Predicted Value\",\n    y = \"Loss\",\n    color = \"Actual Value\"\n  ) +\n  dsan_theme() +\n  ylim(0,6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "writeups/machine-learning/index.html#loss-function-implies-ready-to-learn",
    "href": "writeups/machine-learning/index.html#loss-function-implies-ready-to-learn",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Loss Function \\(\\implies\\) Ready to Learn!",
    "text": "Loss Function \\(\\implies\\) Ready to Learn!\n\nOnce we‚Äôve chosen a loss function, the learning algorithm has what it needs to proceed with the actual learning\nNotation: Bundle all the model‚Äôs parameters together into \\(\\theta\\)\nThe goal: \\[\n\\min_{\\theta} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nWhat would this look like for the random-lines approach?\nIs there a more efficient way?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#calculus-strikes-again",
    "href": "writeups/machine-learning/index.html#calculus-strikes-again",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Strikes Again",
    "text": "Calculus Strikes Again\n\ntldr: The slope of a function tells us how to get to a minimum (why a minimum rather than the minimum?)\nDerivative (gradient) = ‚Äúdirection of sharpest decrease‚Äù\nThink of hill climbing! Let \\(\\ell_t \\in L\\) be your location at time \\(t\\), and \\(Alt(\\ell)\\) be the altitude at a location \\(\\ell\\)\nGradient descent for \\(\\ell^* = \\max_{\\ell \\in L} Alt(\\ell)\\): \\[\n\\ell_{t+1} = \\ell_t + \\gamma\\nabla Alt(\\ell_t),\\ t\\geq 0.\n\\]\nWhile top of mountain = good, Loss is bad: we want to find the bottom of the ‚Äúloss crater‚Äù\n\n\\(\\implies\\) we do the opposite: subtract \\(\\gamma\\nabla Alt(\\ell_t)\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#good-and-bad-news",
    "href": "writeups/machine-learning/index.html#good-and-bad-news",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Good and Bad News",
    "text": "Good and Bad News\n\n\n\nUniversal Approximation Theorem\nNeural networks can represent any function mapping one Euclidean space to another\n(Neural Turing Machines:)\n\n\n\n\n\n\nFigure from @schmidinger_exploring_2019\n\n\nWeierstrass Approximation Theorem\n(Polynomials could already represent any function)\n\n\\[\nf \\in C([a,b],[a,b])\n\\] \\[\n\\implies \\forall \\epsilon &gt; 0, \\exists p \\in \\mathbb{R}[x] :\n\\] \\[\n\\forall x \\in [a, b] \\; \\left|f(x) ‚àí p(x)\\right| &lt; \\epsilon\n\\]\n\nImplications for machine learning?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#so-whats-the-issue",
    "href": "writeups/machine-learning/index.html#so-whats-the-issue",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "So What‚Äôs the Issue?",
    "text": "So What‚Äôs the Issue?\n\n\n\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\n\nHigher \\(R^2\\) = Better Model? Lower \\(RSS\\)?\nLinear Model:\n\n\nsummary(lin_model)$r.squared\n\n[1] 0.5269359\n\nget_rss(lin_model)\n\n[1] 0.5075188\n\n\n\nPolynomial Model:\n\n\nsummary(poly_model)$r.squared\n\n[1] 1\n\nget_rss(poly_model)\n\n[1] 0"
  },
  {
    "objectID": "writeups/machine-learning/index.html#generalization",
    "href": "writeups/machine-learning/index.html#generalization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Generalization",
    "text": "Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  dsan_theme() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Performance on Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\nlin_r2_test\n\n[1] 0.8366753\n\nlin_rss_test\n\n[1] 0.1749389\n\n\n\nPolynomial Model:\n\n\npoly_r2_test\n\n[1] 0.470513\n\npoly_rss_test\n\n[1] 0.5671394"
  },
  {
    "objectID": "writeups/machine-learning/index.html#how-to-avoid-overfitting",
    "href": "writeups/machine-learning/index.html#how-to-avoid-overfitting",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "How to Avoid Overfitting?",
    "text": "How to Avoid Overfitting?\n\nThe gist: penalize model complexity\n\nOriginal optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nNew optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y_{obs}, y_{pred}(\\theta)) + \\mathsf{Complexity}(\\theta) \\right]\n\\]\n\nSo how do we measure, and penalize, ‚Äúcomplexity‚Äù?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#regularization-measuring-and-penalizing-complexity",
    "href": "writeups/machine-learning/index.html#regularization-measuring-and-penalizing-complexity",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#lasso-and-elastic-net-regularization",
    "href": "writeups/machine-learning/index.html#lasso-and-elastic-net-regularization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO4 [@tibshirani_regression_1996] is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\n(Ensures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#training-vs.-test-data",
    "href": "writeups/machine-learning/index.html#training-vs.-test-data",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\n\n\n\n\n\ngrid\n\n\ncluster_02\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/index.html#cross-validation",
    "href": "writeups/machine-learning/index.html#cross-validation",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_04\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/index.html#hyperparameters",
    "href": "writeups/machine-learning/index.html#hyperparameters",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several we‚Äôve already seen ‚Äì can you name them?\n\n\n\nUnsupervised Clustering: The number of clusters we want (\\(K\\))\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "writeups/machine-learning/index.html#hyperparameter-selection",
    "href": "writeups/machine-learning/index.html#hyperparameter-selection",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, number of nodes per layer\nDecision Trees: Maximum tree depth, max number of features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM [@kingma_adam_2017] for Neural Network learning rates)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#appendix-harmonic-mean",
    "href": "writeups/machine-learning/index.html#appendix-harmonic-mean",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Appendix: Harmonic Mean",
    "text": "Appendix: Harmonic Mean\n\n\\(\\mathsf{HMean}\\) is the harmonic mean, an alternative to the standard (arithmetic) mean\nPenalizes greater ‚Äúgaps‚Äù between precision and recall: if precision is 0 and recall is 1, for example, their arithmetic mean is 0.5 while their harmonic mean is 0.\nFor the curious: given numbers \\(X = \\{x_1, \\ldots, x_n\\}\\), \\(\\mathsf{HMean}(X) = \\frac{n}{\\sum_{i=1}^nx_i^{-1}}\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#footnotes",
    "href": "writeups/machine-learning/index.html#footnotes",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhether standard classification (sorting observations into bins) or regression (assigning a real number to each observation)‚Ü©Ô∏é\nComputer scientists implicitly assume a Correspondence Theory of Truth, hence the choice of name‚Ü©Ô∏é\nThanks to Bayes‚Äô Rule, mathematically we can always convert between the two: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)‚Ü©Ô∏é\nLeast Absolute Shrinkage and Selection Operator‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#supervised-vs.-unsupervised-learning",
    "href": "writeups/machine-learning/slides.html#supervised-vs.-unsupervised-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Supervised vs.¬†Unsupervised Learning",
    "text": "Supervised vs.¬†Unsupervised Learning\n\n\n\n\n\nSupervised Learning: You want the computer to learn the existing pattern of how you are classifying1 observations\n\nDiscovering the relationship between properties of data and outcomes\nExample (Binary Classification): I look at homes on Zillow, saving those I like to folder A and don‚Äôt like to folder B\nExample (Regression): I assign a rating of 0-100 to each home\nIn both cases: I ask the computer to learn my schema (how I classify)\n\n\nUnsupervised Learning: You want the computer to find patterns in a dataset, without any prior classification info\n\nTypically: grouping or clustering observations based on shared properties\nExample (Clustering): I save all the used car listings I can find, and ask the computer to ‚Äúfind a pattern‚Äù in this data, by clustering similar cars together\n\n\nWhether standard classification (sorting observations into bins) or regression (assigning a real number to each observation)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#dataset-structures",
    "href": "writeups/machine-learning/slides.html#dataset-structures",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures",
    "text": "Dataset Structures\n\n\nSupervised Learning: Dataset has both explanatory variables (‚Äúfeatures‚Äù) and response variables (‚Äúlabels‚Äù)\n\n\n\n\n\n\n\nhome_id\nsqft\nbedrooms\nrating\n\n\n\n\n0\n1000\n1\nDisliked\n\n\n1\n2000\n2\nLiked\n\n\n2\n2500\n1\nLiked\n\n\n3\n1500\n2\nDisliked\n\n\n4\n2200\n1\nLiked\n\n\n\n\n\n\n\n\nUnsupervised Learning: Dataset has only explanatory variables (‚Äúfeatures‚Äù)\n\n\n\n\n\n\n\nhome_id\nsqft\nbedrooms\n\n\n\n\n0\n1000\n1\n\n\n1\n2000\n2\n\n\n2\n2500\n1\n\n\n3\n1500\n2\n\n\n4\n2200\n1"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#dataset-structures-visualized",
    "href": "writeups/machine-learning/slides.html#dataset-structures-visualized",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures: Visualized",
    "text": "Dataset Structures: Visualized"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#different-goals",
    "href": "writeups/machine-learning/slides.html#different-goals",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Different Goals",
    "text": "Different Goals"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#the-learning-in-machine-learning",
    "href": "writeups/machine-learning/slides.html#the-learning-in-machine-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The ‚ÄúLearning‚Äù in Machine Learning",
    "text": "The ‚ÄúLearning‚Äù in Machine Learning\n\nGiven these datasets, how do we learn the patterns?\nNa√Øve idea: Try random lines (each forming a decision boundary), pick ‚Äúbest‚Äù one\n\n\n\nWhat parameters are we choosing when we draw a random line? Random curve?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#what-makes-a-goodbest-guess",
    "href": "writeups/machine-learning/slides.html#what-makes-a-goodbest-guess",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?",
    "text": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?\n\nWhat‚Äôs your intuition? How about accuracy‚Ä¶ ü§î\n\n\nSo‚Ä¶ what‚Äôs wrong here?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#whats-wrong-with-accuracy",
    "href": "writeups/machine-learning/slides.html#whats-wrong-with-accuracy",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What‚Äôs Wrong with Accuracy?",
    "text": "What‚Äôs Wrong with Accuracy?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#the-oversimplified-big-picture",
    "href": "writeups/machine-learning/slides.html#the-oversimplified-big-picture",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The (Oversimplified) Big Picture",
    "text": "The (Oversimplified) Big Picture\n\nA model: some representation of something in the world\n\n\n\n\n\nHow well does our model represent the world?1 \\(\\mathsf{Correspondence}(y_{obs}, \\theta)\\)\n\\(P\\left(y_{obs}, \\theta\\right)\\), \\(P\\left(\\theta \\; | \\; y_{obs}\\right)\\), \\(P\\left(y_{obs} \\; | \\; \\theta\\right)\\)2\nMaximum Likelihood Estimation?\n\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\n\"Nature\"\n\n\ncluster_02\n\n\"Science\"\n\n\n\nObs\n\nThing(s) we can see\n\n\n\nUnd\n\nUnderlying process\n\n\n\nUnd-&gt;Obs\n\n\n\n\n\nModel\n\nModel\n\n\n\nUnd-&gt;Model\n\n\n\n\n\nModel-&gt;Obs\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathsf{Correspondence}(y_{obs}, \\theta) &\\equiv P(y = y_{obs}, \\theta) \\\\\nP(y = y_{obs}, \\theta) &= P(y=y_{obs} \\; | \\; \\theta)P(\\theta) \\\\\n&\\propto P\\left(y = y_{obs} \\; | \\; \\theta\\right)\\ldots \\implies \\text{(maximize this!)}  \\\\\n\\end{align*}\n\\]\nComputer scientists implicitly assume a Correspondence Theory of Truth, hence the choice of nameThanks to Bayes‚Äô Rule, mathematically we can always convert between the two: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#measuring-errors-f1-score",
    "href": "writeups/machine-learning/slides.html#measuring-errors-f1-score",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: F1 Score",
    "text": "Measuring Errors: F1 Score\n\nHow can we reward guesses which best discriminate between classes?\n\n\\[\n\\begin{align*}\n\\mathsf{Precision} &= \\frac{\\# \\text{true positives}}{\\# \\text{predicted positive}} = \\frac{tp}{tp+fp} \\\\[1.5em]\n\\mathsf{Recall} &= \\frac{\\# \\text{true positives}}{\\# \\text{positives in data}} = \\frac{tp}{tp+fn} \\\\[1.5em]\nF_1 &= 2\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision} + \\mathsf{Recall}} = \\mathsf{HMean}(\\mathsf{Precision}, \\mathsf{Recall})\n\\end{align*}\n\\]\n\nThink about: How does this address/fix issue with accuracy?\n\n\n\nHere \\(\\mathsf{HMean}\\) is the Harmonic Mean function: see appendix slide or Wikipedia."
  },
  {
    "objectID": "writeups/machine-learning/slides.html#measuring-errors-the-loss-function",
    "href": "writeups/machine-learning/slides.html#measuring-errors-the-loss-function",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: The Loss Function",
    "text": "Measuring Errors: The Loss Function\n\nWhat about regression?\n\nNo longer just ‚Äútrue prediction good, false prediction bad‚Äù\n\nWe have to quantify how bad the guess is! Then we can scale the penalty accordingly: \\(\\text{penalty} \\propto \\text{badness}\\)\nEnter Loss Functions! Just distances (using distance metrics you‚Äôve already seen) between the true value and our guess:\n\nSquared Error \\(L^2(y_{obs}, y_{pred}) = (y_{obs} - y_{pred})^2\\)\nKullback-Leibler Divergence if guessing distributions"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#calculus-rears-its-ugly-head",
    "href": "writeups/machine-learning/slides.html#calculus-rears-its-ugly-head",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Rears its Ugly Head",
    "text": "Calculus Rears its Ugly Head\n\nNeural networks use derivatives/gradients to improve their predictions given a particular loss function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan we just use the \\(F_1\\) score?\n\n\\[\n\\frac{\\partial F_1(weights)}{\\partial weights} = \\ldots \\; ? \\; \\ldots üíÄ\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#quantifying-discrete-loss",
    "href": "writeups/machine-learning/slides.html#quantifying-discrete-loss",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Quantifying Discrete Loss",
    "text": "Quantifying Discrete Loss\n\nWe can quantify a differentiable discrete loss by asking the algorithm how confident it is\n\nCloser to 0 \\(\\implies\\) more confident that the true label is 0\nCloser to 1 \\(\\implies\\) more confident that the true label is 1\n\n\n\\[\n\\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\\log(y_{pred}) + (1-y_{obs})\\log(1-y_{pred}))\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#loss-function-implies-ready-to-learn",
    "href": "writeups/machine-learning/slides.html#loss-function-implies-ready-to-learn",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Loss Function \\(\\implies\\) Ready to Learn!",
    "text": "Loss Function \\(\\implies\\) Ready to Learn!\n\nOnce we‚Äôve chosen a loss function, the learning algorithm has what it needs to proceed with the actual learning\nNotation: Bundle all the model‚Äôs parameters together into \\(\\theta\\)\nThe goal: \\[\n\\min_{\\theta} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nWhat would this look like for the random-lines approach?\nIs there a more efficient way?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#calculus-strikes-again",
    "href": "writeups/machine-learning/slides.html#calculus-strikes-again",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Strikes Again",
    "text": "Calculus Strikes Again\n\ntldr: The slope of a function tells us how to get to a minimum (why a minimum rather than the minimum?)\nDerivative (gradient) = ‚Äúdirection of sharpest decrease‚Äù\nThink of hill climbing! Let \\(\\ell_t \\in L\\) be your location at time \\(t\\), and \\(Alt(\\ell)\\) be the altitude at a location \\(\\ell\\)\nGradient descent for \\(\\ell^* = \\max_{\\ell \\in L} Alt(\\ell)\\): \\[\n\\ell_{t+1} = \\ell_t + \\gamma\\nabla Alt(\\ell_t),\\ t\\geq 0.\n\\]\nWhile top of mountain = good, Loss is bad: we want to find the bottom of the ‚Äúloss crater‚Äù\n\n\\(\\implies\\) we do the opposite: subtract \\(\\gamma\\nabla Alt(\\ell_t)\\)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#good-and-bad-news",
    "href": "writeups/machine-learning/slides.html#good-and-bad-news",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Good and Bad News",
    "text": "Good and Bad News\n\n\n\nUniversal Approximation Theorem\nNeural networks can represent any function mapping one Euclidean space to another\n(Neural Turing Machines:)\n\n\n\n\n\n\n\nWeierstrass Approximation Theorem\n(Polynomials could already represent any function)\n\n\\[\nf \\in C([a,b],[a,b])\n\\] \\[\n\\implies \\forall \\epsilon &gt; 0, \\exists p \\in \\mathbb{R}[x] :\n\\] \\[\n\\forall x \\in [a, b] \\; \\left|f(x) ‚àí p(x)\\right| &lt; \\epsilon\n\\]\n\nImplications for machine learning?\n\n\n\nFigure from @schmidinger_exploring_2019"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#so-whats-the-issue",
    "href": "writeups/machine-learning/slides.html#so-whats-the-issue",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "So What‚Äôs the Issue?",
    "text": "So What‚Äôs the Issue?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher \\(R^2\\) = Better Model? Lower \\(RSS\\)?\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5639018\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.537882\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#generalization",
    "href": "writeups/machine-learning/slides.html#generalization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Generalization",
    "text": "Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8870637\n\n\nCode\nlin_rss_test\n\n\n[1] 0.1317542\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.3334695\n\n\nCode\npoly_rss_test\n\n\n[1] 0.7775903"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#how-to-avoid-overfitting",
    "href": "writeups/machine-learning/slides.html#how-to-avoid-overfitting",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "How to Avoid Overfitting?",
    "text": "How to Avoid Overfitting?\n\nThe gist: penalize model complexity\n\nOriginal optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nNew optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y_{obs}, y_{pred}(\\theta)) + \\mathsf{Complexity}(\\theta) \\right]\n\\]\n\nSo how do we measure, and penalize, ‚Äúcomplexity‚Äù?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#regularization-measuring-and-penalizing-complexity",
    "href": "writeups/machine-learning/slides.html#regularization-measuring-and-penalizing-complexity",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#lasso-and-elastic-net-regularization",
    "href": "writeups/machine-learning/slides.html#lasso-and-elastic-net-regularization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 [@tibshirani_regression_1996] is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\n(Ensures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\))\n\nLeast Absolute Shrinkage and Selection Operator"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#training-vs.-test-data",
    "href": "writeups/machine-learning/slides.html#training-vs.-test-data",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#cross-validation",
    "href": "writeups/machine-learning/slides.html#cross-validation",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#hyperparameters",
    "href": "writeups/machine-learning/slides.html#hyperparameters",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several we‚Äôve already seen ‚Äì can you name them?\n\n\n\nUnsupervised Clustering: The number of clusters we want (\\(K\\))\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#hyperparameter-selection",
    "href": "writeups/machine-learning/slides.html#hyperparameter-selection",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, number of nodes per layer\nDecision Trees: Maximum tree depth, max number of features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM [@kingma_adam_2017] for Neural Network learning rates)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#appendix-harmonic-mean",
    "href": "writeups/machine-learning/slides.html#appendix-harmonic-mean",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Appendix: Harmonic Mean",
    "text": "Appendix: Harmonic Mean\n\n\\(\\mathsf{HMean}\\) is the harmonic mean, an alternative to the standard (arithmetic) mean\nPenalizes greater ‚Äúgaps‚Äù between precision and recall: if precision is 0 and recall is 1, for example, their arithmetic mean is 0.5 while their harmonic mean is 0.\nFor the curious: given numbers \\(X = \\{x_1, \\ldots, x_n\\}\\), \\(\\mathsf{HMean}(X) = \\frac{n}{\\sum_{i=1}^nx_i^{-1}}\\)"
  },
  {
    "objectID": "w11/index.html",
    "href": "w11/index.html",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#quick-roadmap",
    "href": "w11/index.html#quick-roadmap",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nLast week: Examples of how NNs are capable of learning‚Ä¶\nThe types of features that let us learn fancy non-linear DGPs: \\(Y = {\\color{#e69f00} X_1 X_2 }\\) ‚úÖ, \\(Y = {\\color{#56b4e9} X_1^2 + X_2^2 }\\) ‚úÖ, \\(Y = {\\color{#009E73} X_1 \\underset{\\mathclap{\\small \\text{XOR}}}{\\oplus} X_2}\\) ‚úÖ\nMulti-layer networks like CNNs for ‚Äúpooling‚Äù low-level/fine-grained information into high-level/coarse-grained information\n\nEx: Early layers detect lines, later layers figure out whether they‚Äôre brows or smiles\n\nThis week: How do we actually learn the weights/biases which enable these capabilities?\n\nThe answer is (üôà) calculus (chain rule)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#step-by-step",
    "href": "w11/index.html#step-by-step",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Step-by-Step",
    "text": "Step-by-Step\n\n\n\n\n\n\nNeural Network Training Procedure\n\n\n\nFor each training observation \\((\\mathbf{x}_i, y_i)\\)‚Ä¶\n Predict \\(\\widehat{y}_i\\) from \\(\\mathbf{x}_i\\)\n Evaluate loss \\(\\mathcal{L}(\\widehat{y}_i, y_i)\\): Cross-Entropy Loss\n Update parameters (weights/biases): Backpropagation\n\n\nKey for success of NNs: Non-linear but differentiable\n\n\\(\\Rightarrow\\) parameters \\(w^*\\) most responsible for the loss value can be\n\nidentified: \\(w^* = \\argmax_w\\left[ \\frac{\\partial \\mathcal{L}}{\\partial w} \\right]\\), then\nchanged the most \\(w^*_t \\rightarrow w^*_{t+1}\\)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#how-do-we-evaluate-output",
    "href": "w11/index.html#how-do-we-evaluate-output",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "How Do We Evaluate Output?",
    "text": "How Do We Evaluate Output?\n\n\n\nMultilayer NN for MNIST Handwritten Digit Recognition, Adapted from ISLR Fig 10.4",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#entropy-in-general",
    "href": "w11/index.html#entropy-in-general",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Entropy in General",
    "text": "Entropy in General",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#entropy-loss-output-layer-uncertainty",
    "href": "w11/index.html#entropy-loss-output-layer-uncertainty",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "‚ÄúEntropy Loss‚Äù: Output Layer Uncertainty",
    "text": "‚ÄúEntropy Loss‚Äù: Output Layer Uncertainty\n\n\n\n\n\n\n\nMax entropy = max uncertainty\n\n\n\n\nStep 1: NN has no idea, guesses (via softmax) \\(\\widehat{y}_d = \\Pr(y = d) = 0.1 \\; \\forall d\\)\n\n\n\n\n\nLess entropy = less uncertainty\n\n\n\n\nStep 2: NN starting to converge: \\(\\Pr(Y = 9)\\) high, \\(\\Pr(Y = 3)\\) medium, \\(\\Pr(Y = d)\\) low for all other \\(d\\)\n\n\n\n\n\nMin entropy = no uncertainty\n\n\n\n\nStep 3: NN has converged to predicting ultra-high \\(\\widehat{y}_9 = \\Pr(y = 9 \\mid X)\\)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#the-problem-with-entropy-loss",
    "href": "w11/index.html#the-problem-with-entropy-loss",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "The Problem With Entropy Loss",
    "text": "The Problem With Entropy Loss\n\n\n\n\n\n\n\nMax entropy = max uncertainty\n\n\n\n\nStep 1: NN has no idea, guesses (via softmax) \\(\\Pr(y = d) = 0.1\\) for every \\(d\\)\n\n\n\n\n\nLess entropy = less uncertainty\n\n\n\n\nStep 2: NN starting to converge: probably \\(d = 3\\), maybe \\(d = 9\\), low probability on all other values\n\n\n\n\n\nMin entropy = no uncertainty\n\n\n\n\nStep 3: NN has converged to predicting ultra-high \\(\\Pr(y = 3)\\)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#cross-entropy-loss-output-layer-vs.-truth",
    "href": "w11/index.html#cross-entropy-loss-output-layer-vs.-truth",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Cross-Entropy Loss: Output Layer vs.¬†Truth",
    "text": "Cross-Entropy Loss: Output Layer vs.¬†Truth\n\n\n\n\n\n\n\nMax entropy = max uncertainty\n\n\n\n\nStep 1: \\(H(y, \\widehat{y}) = -1\\cdot \\log_2(0.1) \\approx 3.32\\)\n\n\n\n\n\nLess entropy = less uncertainty\n\n\n\n\nStep 2: \\(H(y,\\widehat{y}) = -1\\cdot \\log_2(0.4) \\approx 1.32\\)\n\n\n\n\n\nMin entropy = no uncertainty\n\n\n\n\nStep 3: \\(H(y,\\widehat{y}) = -1\\cdot \\log_2(1) = 0\\)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#its-not-as-silly-as-you-think",
    "href": "w11/index.html#its-not-as-silly-as-you-think",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "It‚Äôs Not as Silly as You Think!",
    "text": "It‚Äôs Not as Silly as You Think!\n\nIn our example, we know the true digit‚Ä¶ But remember the origin of the dataset: postal workers trying to figure out handwritten digits\nMay not know with certainty, but may be able to say, e.g., ‚ÄúIt‚Äôs either a 1 or a 7‚Äù\n\n\n\n\nFrom Perceptions of Probability Dataset",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#backpropagation-simple-example",
    "href": "w11/index.html#backpropagation-simple-example",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Backpropagation: Simple Example",
    "text": "Backpropagation: Simple Example\n\nLiterally just one neuron (which is the output layer), \\(\\mathcal{L}(\\widehat{y},y) = (\\widehat{y} - y)^2\\)\nConsider a training datapoint \\((x,y) = (2,10)\\)\nAnd say our current parameters are \\(\\beta_0 = 1, \\beta_1 = 3\\)\nPredicted output: \\(\\widehat{y} = \\beta_0 + \\beta_1 x = 1 + 3\\cdot 2 = 7\\)\nSince true output is \\(y = 10\\), we have loss \\(\\mathcal{L} = (10-7)^2 = 9\\)\nNow, let‚Äôs backpropagate to update \\(\\beta_1\\) (on the board!)\n\n(Using learning rate of \\(0.1\\))",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#top-secret-answer-slide",
    "href": "w11/index.html#top-secret-answer-slide",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Top Secret Answer Slide",
    "text": "Top Secret Answer Slide\n\nWeight \\(\\beta_1\\) becomes 4.2‚Ä¶\nNew prediction: \\(\\widehat{y} = 1 + 4.2\\cdot 2 = 9.4\\)\nNew loss: \\((10-9.4)^2 = 0.36\\) ü•≥",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#backpropagation-deeper-dive",
    "href": "w11/index.html#backpropagation-deeper-dive",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Backpropagation Deeper Dive",
    "text": "Backpropagation Deeper Dive\n\n\n\nBackpropagation! (3Blue1Brown Again!)\n\n\n(Full NN playlist here)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#simplest-possible-backprop",
    "href": "w11/index.html#simplest-possible-backprop",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Simplest Possible Backprop",
    "text": "Simplest Possible Backprop\n\nOne input unit, one hidden unit, one output unit",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#references",
    "href": "w11/index.html#references",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/slides.html#quick-roadmap",
    "href": "w11/slides.html#quick-roadmap",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nLast week: Examples of how NNs are capable of learning‚Ä¶\nThe types of features that let us learn fancy non-linear DGPs: \\(Y = {\\color{#e69f00} X_1 X_2 }\\) ‚úÖ, \\(Y = {\\color{#56b4e9} X_1^2 + X_2^2 }\\) ‚úÖ, \\(Y = {\\color{#009E73} X_1 \\underset{\\mathclap{\\small \\text{XOR}}}{\\oplus} X_2}\\) ‚úÖ\nMulti-layer networks like CNNs for ‚Äúpooling‚Äù low-level/fine-grained information into high-level/coarse-grained information\n\nEx: Early layers detect lines, later layers figure out whether they‚Äôre brows or smiles\n\nThis week: How do we actually learn the weights/biases which enable these capabilities?\n\nThe answer is (üôà) calculus (chain rule)"
  },
  {
    "objectID": "w11/slides.html#step-by-step",
    "href": "w11/slides.html#step-by-step",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Step-by-Step",
    "text": "Step-by-Step\n\n\n\n\n\n\n\nNeural Network Training Procedure\n\n\nFor each training observation \\((\\mathbf{x}_i, y_i)\\)‚Ä¶\n Predict \\(\\widehat{y}_i\\) from \\(\\mathbf{x}_i\\)\n Evaluate loss \\(\\mathcal{L}(\\widehat{y}_i, y_i)\\): Cross-Entropy Loss\n Update parameters (weights/biases): Backpropagation\n\n\n\n\nKey for success of NNs: Non-linear but differentiable\n\n\\(\\Rightarrow\\) parameters \\(w^*\\) most responsible for the loss value can be\n\nidentified: \\(w^* = \\argmax_w\\left[ \\frac{\\partial \\mathcal{L}}{\\partial w} \\right]\\), then\nchanged the most \\(w^*_t \\rightarrow w^*_{t+1}\\)"
  },
  {
    "objectID": "w11/slides.html#how-do-we-evaluate-output",
    "href": "w11/slides.html#how-do-we-evaluate-output",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "How Do We Evaluate Output?",
    "text": "How Do We Evaluate Output?\n\nMultilayer NN for MNIST Handwritten Digit Recognition, Adapted from ISLR Fig 10.4"
  },
  {
    "objectID": "w11/slides.html#entropy-in-general",
    "href": "w11/slides.html#entropy-in-general",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Entropy in General",
    "text": "Entropy in General"
  },
  {
    "objectID": "w11/slides.html#entropy-loss-output-layer-uncertainty",
    "href": "w11/slides.html#entropy-loss-output-layer-uncertainty",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "‚ÄúEntropy Loss‚Äù: Output Layer Uncertainty",
    "text": "‚ÄúEntropy Loss‚Äù: Output Layer Uncertainty\n\n\n\n\n\n\n\nMax entropy = max uncertainty\n\n\n\n\nStep 1: NN has no idea, guesses (via softmax) \\(\\widehat{y}_d = \\Pr(y = d) = 0.1 \\; \\forall d\\)\n\n\n\n\n\nLess entropy = less uncertainty\n\n\n\n\nStep 2: NN starting to converge: \\(\\Pr(Y = 9)\\) high, \\(\\Pr(Y = 3)\\) medium, \\(\\Pr(Y = d)\\) low for all other \\(d\\)\n\n\n\n\n\nMin entropy = no uncertainty\n\n\n\n\nStep 3: NN has converged to predicting ultra-high \\(\\widehat{y}_9 = \\Pr(y = 9 \\mid X)\\)"
  },
  {
    "objectID": "w11/slides.html#the-problem-with-entropy-loss",
    "href": "w11/slides.html#the-problem-with-entropy-loss",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "The Problem With Entropy Loss",
    "text": "The Problem With Entropy Loss\n\n\n\n\n\n\n\nMax entropy = max uncertainty\n\n\n\n\nStep 1: NN has no idea, guesses (via softmax) \\(\\Pr(y = d) = 0.1\\) for every \\(d\\)\n\n\n\n\n\nLess entropy = less uncertainty\n\n\n\n\nStep 2: NN starting to converge: probably \\(d = 3\\), maybe \\(d = 9\\), low probability on all other values\n\n\n\n\n\nMin entropy = no uncertainty\n\n\n\n\nStep 3: NN has converged to predicting ultra-high \\(\\Pr(y = 3)\\)"
  },
  {
    "objectID": "w11/slides.html#cross-entropy-loss-output-layer-vs.-truth",
    "href": "w11/slides.html#cross-entropy-loss-output-layer-vs.-truth",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Cross-Entropy Loss: Output Layer vs.¬†Truth",
    "text": "Cross-Entropy Loss: Output Layer vs.¬†Truth\n\n\n\n\n\n\n\nMax entropy = max uncertainty\n\n\n\n\nStep 1: \\(H(y, \\widehat{y}) = -1\\cdot \\log_2(0.1) \\approx 3.32\\)\n\n\n\n\n\nLess entropy = less uncertainty\n\n\n\n\nStep 2: \\(H(y,\\widehat{y}) = -1\\cdot \\log_2(0.4) \\approx 1.32\\)\n\n\n\n\n\nMin entropy = no uncertainty\n\n\n\n\nStep 3: \\(H(y,\\widehat{y}) = -1\\cdot \\log_2(1) = 0\\)"
  },
  {
    "objectID": "w11/slides.html#its-not-as-silly-as-you-think",
    "href": "w11/slides.html#its-not-as-silly-as-you-think",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "It‚Äôs Not as Silly as You Think!",
    "text": "It‚Äôs Not as Silly as You Think!\n\nIn our example, we know the true digit‚Ä¶ But remember the origin of the dataset: postal workers trying to figure out handwritten digits\nMay not know with certainty, but may be able to say, e.g., ‚ÄúIt‚Äôs either a 1 or a 7‚Äù\n\n\nFrom Perceptions of Probability Dataset"
  },
  {
    "objectID": "w11/slides.html#backpropagation-simple-example",
    "href": "w11/slides.html#backpropagation-simple-example",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Backpropagation: Simple Example",
    "text": "Backpropagation: Simple Example\n\nLiterally just one neuron (which is the output layer), \\(\\mathcal{L}(\\widehat{y},y) = (\\widehat{y} - y)^2\\)\nConsider a training datapoint \\((x,y) = (2,10)\\)\nAnd say our current parameters are \\(\\beta_0 = 1, \\beta_1 = 3\\)\nPredicted output: \\(\\widehat{y} = \\beta_0 + \\beta_1 x = 1 + 3\\cdot 2 = 7\\)\nSince true output is \\(y = 10\\), we have loss \\(\\mathcal{L} = (10-7)^2 = 9\\)\nNow, let‚Äôs backpropagate to update \\(\\beta_1\\) (on the board!)\n\n(Using learning rate of \\(0.1\\))"
  },
  {
    "objectID": "w11/slides.html#top-secret-answer-slide",
    "href": "w11/slides.html#top-secret-answer-slide",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Top Secret Answer Slide",
    "text": "Top Secret Answer Slide\n\nWeight \\(\\beta_1\\) becomes 4.2‚Ä¶\nNew prediction: \\(\\widehat{y} = 1 + 4.2\\cdot 2 = 9.4\\)\nNew loss: \\((10-9.4)^2 = 0.36\\) ü•≥"
  },
  {
    "objectID": "w11/slides.html#backpropagation-deeper-dive",
    "href": "w11/slides.html#backpropagation-deeper-dive",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Backpropagation Deeper Dive",
    "text": "Backpropagation Deeper Dive\n\nBackpropagation! (3Blue1Brown Again!)(Full NN playlist here)"
  },
  {
    "objectID": "w11/slides.html#simplest-possible-backprop",
    "href": "w11/slides.html#simplest-possible-backprop",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "Simplest Possible Backprop",
    "text": "Simplest Possible Backprop\n\nOne input unit, one hidden unit, one output unit"
  },
  {
    "objectID": "w11/slides.html#references",
    "href": "w11/slides.html#references",
    "title": "Week 11: How Do Neural Networks Deep-Learn?",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5300-01: Statistical Learning",
    "section": "",
    "text": "Welcome to the homepage for Section 01 (Mondays 6:30-9pm in Car Barn 203) of DSAN 5300: Statistical Learning at Georgetown University, for the Spring 2025 semester!\nIf you‚Äôre looking to book an office hour with Jeff, you can use the link in the sidebar, or this direct link: the office hour blocks for Spring 2025 are held from 3:30-6:30pm every Tuesday.\nUse the following links to view notes and lecture slides for individual weeks:\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Introduction to the Course\n\n\nJanuary 8\n\n\n\n\nWeek 2: Linear Regression\n\n\nJanuary 13\n\n\n\n\nWeek 3: Getting Fancy with Regression\n\n\nJanuary 27\n\n\n\n\nWeek 4: The Scourge of Overfitting\n\n\nFebruary 3\n\n\n\n\nWeek 5: Cross-Validation for Model Assessment\n\n\nFebruary 10\n\n\n\n\nWeek 6: Regularization for Model Selection\n\n\nFebruary 18\n\n\n\n\nWeek 7: Basis Functions and Splines\n\n\nFebruary 24\n\n\n\n\nWeek 8: Support Vector Machines\n\n\nMarch 10\n\n\n\n\nWeek 9: Generative vs.¬†Discriminative Models\n\n\nMarch 17\n\n\n\n\nWeek 10: Deep Learning\n\n\nMarch 24\n\n\n\n\nWeek 11: How Do Neural Networks Deep-Learn?\n\n\nMarch 31\n\n\n\n\nWeek 12: Survival Analysis\n\n\nApril 7\n\n\n\n\nWeek 13: Machine Learning for Causal Inference\n\n\nApril 14\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i> Home"
    ]
  },
  {
    "objectID": "w09/index.html",
    "href": "w09/index.html",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#quick-roadmap",
    "href": "w09/index.html#quick-roadmap",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nWeeks 8-9: Shift from focus on regression to focus on classification (Though we use lessons from regression!)\nLast Week (W08): SVMs as new method with this focus\n\nEmphasis on boundary between classes \\(\\leadsto\\) 2.5hrs on separating hyperplanes: in original feature space (Max-Margin, SVCs) or derived feature spaces (SVMs)\n\nNow: Wait, didn‚Äôt we discuss a classification method before, though its name confusingly had ‚Äúregression‚Äù in it? ü§î\n\nTake logistic regression but use Bayes rule to ‚Äúflip‚Äù from regression[+thresholding] task to class-separation task (think of SVM‚Äôs max-width-of-‚Äúslab‚Äù objective!)",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#logistic-regression-refresher",
    "href": "w09/index.html#logistic-regression-refresher",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Logistic Regression Refresher",
    "text": "Logistic Regression Refresher\n\nWe don‚Äôt have time for full refresher, but just remember how it involves learning \\(\\beta_j\\) values to minimize loss w.r.t.\n\n\\[\n\\begin{align*}\n&\\log\\left[ \\frac{\\Pr(Y = 1 \\mid X)}{1 - \\Pr(Y = 1 \\mid X)} \\right] = \\beta_0 + \\beta_1 X \\\\\n&\\iff \\Pr(Y = 1 \\mid X = x_i) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{align*}\n\\]\n\nAnd then, if we want to classify \\(x\\) rather than just predict \\(\\Pr(Y = 1 \\mid X = x)\\), we apply a threshold \\(t \\in [0,1]\\):\n\n\\[\n\\widehat{y} = \\begin{cases}\n1 &\\text{if }\\Pr(Y = 1 \\mid X = x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} &gt; t \\\\\n0 &\\text{otherwise}\n\\end{cases}\n\\]",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#intuition",
    "href": "w09/index.html#intuition",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Intuition",
    "text": "Intuition\n\nLogistic regression is called a discriminative model, since we are learning parameters \\(\\beta_j\\) that best produce a predicted class \\(\\widehat{y_i}\\) from features \\(\\mathbf{x}_i\\)‚Ä¶\nWe‚Äôre modeling \\(\\Pr(Y = k \\mid X)\\) (for two classes, \\(k = 0\\) and \\(k = 1\\)), hence the LHS of the Logistic Regression formula\nBut there are cases where we can do better1 by instead modeling (learning parameters for) \\(\\Pr(X \\mid Y = k)\\), for each \\(k\\), then using Bayes rule to ‚Äúflip‚Äù back to \\(\\Pr(Y = k \\mid X)\\)!\\(\\leadsto\\) LDA, QDA, and Na√Øve Bayes classifiers",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#bayes-rule",
    "href": "w09/index.html#bayes-rule",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Bayes‚Äô Rule",
    "text": "Bayes‚Äô Rule\n\nFirst things first, we generalize from \\(Y \\in \\{0, 1\\}\\) to \\(K\\) possible classes (labels), since the notation for \\(K\\) classes here is not much more complex than 2 classes!\nWe label the pieces using ISLR‚Äôs notation to make our lives easier:\n\n\\[\n\\underbrace{\\Pr(Y = k \\mid X = x)}_{p_k(x)} = \\frac{\n  \\overbrace{\\Pr(X = x \\mid Y = k)}^{f_k(x)} \\overbrace{\\Pr(Y = k)}^{\\pi_k}\n}{\n  \\sum_{\\ell = 1}^{K} \\underbrace{\\Pr(X = x \\mid Y = \\ell)}_{f_{\\ell}(x)} \\underbrace{\\Pr(Y = \\ell)}_{\\pi_{\\ell}}\n} = \\frac{f_k(x) \\overbrace{\\pi_k}^{\\mathclap{\\text{Prior}(k)}}}{\\sum_{\\ell = 1}^{K}f_{\\ell}(x) \\underbrace{\\pi_\\ell}_{\\mathclap{\\text{Prior}(\\ell)}}}\n\\]\n\nSo if we do have only two classes, \\(K = 2\\) and \\(p_1(x) = \\frac{f_1(x)\\pi_1}{f_1(x)\\pi_1 + f_0(x)\\pi_0}\\)\nPriors can be estimated as \\(n_k / n\\). The hard work is in modeling \\(f_k(x)\\)! With estimates of these two ‚Äúpieces‚Äù for each \\(k\\), we can derive a classifier \\(\\widehat{y}(x) = \\argmax_k p_k(x)\\)",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#the-lda-assumption-one-feature-x",
    "href": "w09/index.html#the-lda-assumption-one-feature-x",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "The LDA Assumption (One Feature \\(x\\))",
    "text": "The LDA Assumption (One Feature \\(x\\))\n\nWithin each class \\(k\\), values of \\(x\\) are normally distributed:\n\n\\[\n(X \\mid Y = k) \\sim \\mathcal{N}(\\param{\\mu_k}, \\param{\\sigma^2}) \\iff f_k(x) = \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2\\right]\n\\]\n\nPlugging back into (notationally-simplified) classifier, we get\n\n\\[\n\\widehat{y}(x) = \\argmax_{k}\\left[ \\frac{\n  \\pi_k \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2\\right]\n}{\n  \\sum_{\\ell = 1}^{K}\\pi_{\\ell} \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2\\right]\n}\\right],\n\\]\n\nGross, BUT \\(\\argmax_k p_k(x) = \\argmax_k \\log(p_k(x)) \\leadsto\\) ‚Äúlinear‚Äù discriminant \\(\\delta_k(x)\\):\n\n\\[\n\\widehat{y}(x) = \\argmax_k[\\delta_k(x)] = \\argmax_{k}\\left[ \\overbrace{\\frac{\\mu_k}{\\sigma^2}}^{\\smash{m}} x ~ \\overbrace{- \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)}^{\\smash{b}} \\right]\n\\]",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#decision-boundaries",
    "href": "w09/index.html#decision-boundaries",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nThe boundary between two classes \\(k\\) and \\(k'\\) will be the point at which \\(\\delta_k(x) = \\delta_{k'}(x)\\)\nFor two classes, can solve \\(\\delta_0(x) = \\delta_1(x)\\) for \\(x\\) to obtain \\(x = \\frac{\\mu_0 + \\mu_1}{2}\\)\nTo derive a boundary from data: \\(x = \\frac{\\widehat{\\mu}_0 + \\widehat{\\mu}_1}{2}\\) \\(\\Rightarrow\\) Predict \\(1\\) if \\(x &gt; \\frac{\\widehat{\\mu}_0 + \\widehat{\\mu}_1}{2}\\), \\(0\\) otherwise\n\n\n\n\nISLR Figure 4.4: Estimating the Decision Boundary from data. The dashed line is the ‚Äútrue‚Äù boundary \\(x = \\frac{\\mu_0 + \\mu_1}{2}\\), while the solid line in the right panel is the boundary estimated from data as \\(x = \\frac{\\widehat{\\mu}_0 + \\widehat{\\mu}_1}{2}\\).",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#number-of-parameters",
    "href": "w09/index.html#number-of-parameters",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Number of Parameters",
    "text": "Number of Parameters\n\n\\(K = 2\\) is special case, since lots of things cancel out, but in general need to estimate:\n\n\n\n\n\n\n\n\n\n\nEnglish\nNotation\nHow Many\nFormula\n\n\n\n\nPrior for class \\(k\\)\n\\(\\widehat{\\pi}_k\\)\n\\(K - 1\\)\n\\(\\widehat{\\pi}_k = n_k / n\\)\n\n\nEstimated mean for class \\(k\\)\n\\(\\widehat{\\mu}_k\\)\n\\(K\\)\n\\(\\widehat{\\mu}_k = \\displaystyle \\frac{1}{n_k}\\sum_{\\{i \\mid y_i = k\\}}x_i\\)\n\n\nEstimated (shared) variance\n\\(\\widehat{\\sigma}^2\\)\n1\n\\(\\widehat{\\sigma}^2 = \\displaystyle \\frac{1}{n - K}\\sum_{k = 1}^{K}\\sum_{i:y_i = k}(x_i - \\widehat{\\mu}_k)^2\\)\n\n\n\nTotal:\n\\(2K\\)\n\n\n\n\n\n(Keep in mind for fancier methods! This may blow up to be much larger than \\(n\\))",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#lda-with-multiple-features-here-p-2",
    "href": "w09/index.html#lda-with-multiple-features-here-p-2",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "LDA with Multiple Features (Here \\(p = 2\\))",
    "text": "LDA with Multiple Features (Here \\(p = 2\\))\n\nWithin each class \\(k\\), values of \\(\\mathbf{x}\\) are (multivariate) normally distributed:\n\n\\[\n\\left( \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix} \\middle| ~ Y = k \\right) \\sim \\mathbf{\\mathcal{N}}_2(\\param{\\boldsymbol\\mu_k}, \\param{\\mathbf{\\Sigma}})\n\\]\n\nIncreasing \\(p\\) to 2 and \\(K\\) to 3 means more parameters, but still linear boundaries. It turns out: shared variance (\\(\\sigma^2\\) or \\(\\mathbf{\\Sigma}\\)) will always produce linear boundaries ü§î\n\n\n\n\nISLR Figure 4.6: Like before, dashed lines are ‚Äútrue‚Äù boundaries while solid lines are boundaries estimated from data",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#quadratic-class-boundaries",
    "href": "w09/index.html#quadratic-class-boundaries",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Quadratic Class Boundaries",
    "text": "Quadratic Class Boundaries\n\nTo achieve non-linear boundaries, estimate covariance matrix \\(\\mathbf{\\Sigma}_k\\) for each class \\(k\\):\n\n\\[\n\\left( \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix} \\middle| ~ Y = k \\right) \\sim \\mathbf{\\mathcal{N}}_2(\\param{\\boldsymbol\\mu_k}, \\param{\\mathbf{\\Sigma}_k})\n\\]\n\nPros: Non-linear class boundaries! Cons: More parameters to estimate, does worse than LDA if data linearly-separable (or nearly linearly-separable).\nDeciding factor: do you think DGP produces normal classes with same variance?\n\n\n\n\nISLR Figure 4.9: Dashed purple line is ‚Äútrue‚Äù boundary (Bayes decision boundary), dotted black line is LDA boundary, solid green line is QDA boundary",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#key-advantage-of-generative-model",
    "href": "w09/index.html#key-advantage-of-generative-model",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Key Advantage of Generative Model",
    "text": "Key Advantage of Generative Model\n\nYou get an actual ‚Äúpicture‚Äù of what the data looks like!\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{f}_k(X_1)\\)\n\\(\\widehat{f}_k(X_2)\\)\n\\(\\widehat{f}_k(X_3)\\)\n\n\n\\(k = 1\\)\n\n\n\n\n\n\\(k = 2\\)",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#classifying-new-points",
    "href": "w09/index.html#classifying-new-points",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Classifying New Points",
    "text": "Classifying New Points\n\nFor new feature values \\(x_{ij}\\), compare how likely this value is under \\(k = 1\\) vs.¬†\\(k = 2\\)\nExample: \\(\\mathbf{x} = (0.4, 1.5, 1)^{\\top}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{f}_k(X_1)\\)\n\\(\\widehat{f}_k(X_2)\\)\n\\(\\widehat{f}_k(X_3)\\)\n\n\n\\(k = 1\\)\n\n\n\n\n\n\n\\(f_1(0.4) = 0.368\\)\n\\(f_1(1.5) = 0.484\\)\n\\(f_1(1) = 0.226\\)\n\n\n\\(k = 2\\)\n\n\n\n\n\n\n\\(f_2(0.4) = 0.030\\)\n\\(f_2(1.5) = 0.130\\)\n\\(f_2(1) = 0.616\\)",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#appendix-fuller-logistic-derivation",
    "href": "w09/index.html#appendix-fuller-logistic-derivation",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Appendix: Fuller Logistic Derivation",
    "text": "Appendix: Fuller Logistic Derivation\n\\[\n\\begin{align*}\n&\\log\\left[ \\frac{\\Pr(Y = 1 \\mid X)}{1 - \\Pr(Y = 1 \\mid X)} \\right] = \\beta_0 + \\beta_1 X \\\\\n&\\iff \\frac{\\Pr(Y = 1 \\mid X = x_i)}{1 - \\Pr(Y = 1\\ \\mid X = x_i)} = e^{\\beta_0 + \\beta_1 X} \\\\\n&\\iff \\Pr(Y = 1 \\mid X) = e^{\\beta_0 + \\beta_1 X}(1 - \\Pr(Y = 1 \\mid X)) \\\\\n&\\iff \\Pr(Y = 1 \\mid X) = e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}\\Pr(Y = 1 \\mid X) \\\\\n&\\iff \\Pr(Y = 1 \\mid X) + e^{\\beta_0 + \\beta_1 X}\\Pr(Y = 1 \\mid X) = e^{\\beta_0 + \\beta_1 X} \\\\\n&\\iff \\Pr(Y = 1 \\mid X)(1 + e^{\\beta_0 + \\beta_1 X}) = e^{\\beta_0 + \\beta_1 X} \\\\\n&\\iff \\Pr(Y = 1 \\mid X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#references",
    "href": "w09/index.html#references",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#footnotes",
    "href": "w09/index.html#footnotes",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(More normally-distributed \\(X\\) \\(\\implies\\) more likely to ‚Äúbeat‚Äù Logistic Regression)‚Ü©Ô∏é",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/slides.html#quick-roadmap",
    "href": "w09/slides.html#quick-roadmap",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Quick Roadmap",
    "text": "Quick Roadmap\n\nWeeks 8-9: Shift from focus on regression to focus on classification (Though we use lessons from regression!)\nLast Week (W08): SVMs as new method with this focus\n\nEmphasis on boundary between classes \\(\\leadsto\\) 2.5hrs on separating hyperplanes: in original feature space (Max-Margin, SVCs) or derived feature spaces (SVMs)\n\nNow: Wait, didn‚Äôt we discuss a classification method before, though its name confusingly had ‚Äúregression‚Äù in it? ü§î\n\nTake logistic regression but use Bayes rule to ‚Äúflip‚Äù from regression[+thresholding] task to class-separation task (think of SVM‚Äôs max-width-of-‚Äúslab‚Äù objective!)"
  },
  {
    "objectID": "w09/slides.html#logistic-regression-refresher",
    "href": "w09/slides.html#logistic-regression-refresher",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Logistic Regression Refresher",
    "text": "Logistic Regression Refresher\n\nWe don‚Äôt have time for full refresher, but just remember how it involves learning \\(\\beta_j\\) values to minimize loss w.r.t.\n\n\\[\n\\begin{align*}\n&\\log\\left[ \\frac{\\Pr(Y = 1 \\mid X)}{1 - \\Pr(Y = 1 \\mid X)} \\right] = \\beta_0 + \\beta_1 X \\\\\n&\\iff \\Pr(Y = 1 \\mid X = x_i) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{align*}\n\\]\n\nAnd then, if we want to classify \\(x\\) rather than just predict \\(\\Pr(Y = 1 \\mid X = x)\\), we apply a threshold \\(t \\in [0,1]\\):\n\n\\[\n\\widehat{y} = \\begin{cases}\n1 &\\text{if }\\Pr(Y = 1 \\mid X = x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} &gt; t \\\\\n0 &\\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "w09/slides.html#intuition",
    "href": "w09/slides.html#intuition",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Intuition",
    "text": "Intuition\n\nLogistic regression is called a discriminative model, since we are learning parameters \\(\\beta_j\\) that best produce a predicted class \\(\\widehat{y_i}\\) from features \\(\\mathbf{x}_i\\)‚Ä¶\nWe‚Äôre modeling \\(\\Pr(Y = k \\mid X)\\) (for two classes, \\(k = 0\\) and \\(k = 1\\)), hence the LHS of the Logistic Regression formula\nBut there are cases where we can do better1 by instead modeling (learning parameters for) \\(\\Pr(X \\mid Y = k)\\), for each \\(k\\), then using Bayes rule to ‚Äúflip‚Äù back to \\(\\Pr(Y = k \\mid X)\\)!\\(\\leadsto\\) LDA, QDA, and Na√Øve Bayes classifiers\n\n(More normally-distributed \\(X\\) \\(\\implies\\) more likely to ‚Äúbeat‚Äù Logistic Regression)"
  },
  {
    "objectID": "w09/slides.html#bayes-rule",
    "href": "w09/slides.html#bayes-rule",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Bayes‚Äô Rule",
    "text": "Bayes‚Äô Rule\n\nFirst things first, we generalize from \\(Y \\in \\{0, 1\\}\\) to \\(K\\) possible classes (labels), since the notation for \\(K\\) classes here is not much more complex than 2 classes!\nWe label the pieces using ISLR‚Äôs notation to make our lives easier:\n\n\\[\n\\underbrace{\\Pr(Y = k \\mid X = x)}_{p_k(x)} = \\frac{\n  \\overbrace{\\Pr(X = x \\mid Y = k)}^{f_k(x)} \\overbrace{\\Pr(Y = k)}^{\\pi_k}\n}{\n  \\sum_{\\ell = 1}^{K} \\underbrace{\\Pr(X = x \\mid Y = \\ell)}_{f_{\\ell}(x)} \\underbrace{\\Pr(Y = \\ell)}_{\\pi_{\\ell}}\n} = \\frac{f_k(x) \\overbrace{\\pi_k}^{\\mathclap{\\text{Prior}(k)}}}{\\sum_{\\ell = 1}^{K}f_{\\ell}(x) \\underbrace{\\pi_\\ell}_{\\mathclap{\\text{Prior}(\\ell)}}}\n\\]\n\nSo if we do have only two classes, \\(K = 2\\) and \\(p_1(x) = \\frac{f_1(x)\\pi_1}{f_1(x)\\pi_1 + f_0(x)\\pi_0}\\)\nPriors can be estimated as \\(n_k / n\\). The hard work is in modeling \\(f_k(x)\\)! With estimates of these two ‚Äúpieces‚Äù for each \\(k\\), we can derive a classifier \\(\\widehat{y}(x) = \\argmax_k p_k(x)\\)"
  },
  {
    "objectID": "w09/slides.html#the-lda-assumption-one-feature-x",
    "href": "w09/slides.html#the-lda-assumption-one-feature-x",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "The LDA Assumption (One Feature \\(x\\))",
    "text": "The LDA Assumption (One Feature \\(x\\))\n\nWithin each class \\(k\\), values of \\(x\\) are normally distributed:\n\n\\[\n(X \\mid Y = k) \\sim \\mathcal{N}(\\param{\\mu_k}, \\param{\\sigma^2}) \\iff f_k(x) = \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2\\right]\n\\]\n\nPlugging back into (notationally-simplified) classifier, we get\n\n\\[\n\\widehat{y}(x) = \\argmax_{k}\\left[ \\frac{\n  \\pi_k \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2\\right]\n}{\n  \\sum_{\\ell = 1}^{K}\\pi_{\\ell} \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2\\right]\n}\\right],\n\\]\n\nGross, BUT \\(\\argmax_k p_k(x) = \\argmax_k \\log(p_k(x)) \\leadsto\\) ‚Äúlinear‚Äù discriminant \\(\\delta_k(x)\\):\n\n\\[\n\\widehat{y}(x) = \\argmax_k[\\delta_k(x)] = \\argmax_{k}\\left[ \\overbrace{\\frac{\\mu_k}{\\sigma^2}}^{\\smash{m}} x ~ \\overbrace{- \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)}^{\\smash{b}} \\right]\n\\]"
  },
  {
    "objectID": "w09/slides.html#decision-boundaries",
    "href": "w09/slides.html#decision-boundaries",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nThe boundary between two classes \\(k\\) and \\(k'\\) will be the point at which \\(\\delta_k(x) = \\delta_{k'}(x)\\)\nFor two classes, can solve \\(\\delta_0(x) = \\delta_1(x)\\) for \\(x\\) to obtain \\(x = \\frac{\\mu_0 + \\mu_1}{2}\\)\nTo derive a boundary from data: \\(x = \\frac{\\widehat{\\mu}_0 + \\widehat{\\mu}_1}{2}\\) \\(\\Rightarrow\\) Predict \\(1\\) if \\(x &gt; \\frac{\\widehat{\\mu}_0 + \\widehat{\\mu}_1}{2}\\), \\(0\\) otherwise\n\n\nISLR Figure 4.4: Estimating the Decision Boundary from data. The dashed line is the ‚Äútrue‚Äù boundary \\(x = \\frac{\\mu_0 + \\mu_1}{2}\\), while the solid line in the right panel is the boundary estimated from data as \\(x = \\frac{\\widehat{\\mu}_0 + \\widehat{\\mu}_1}{2}\\)."
  },
  {
    "objectID": "w09/slides.html#number-of-parameters",
    "href": "w09/slides.html#number-of-parameters",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Number of Parameters",
    "text": "Number of Parameters\n\n\\(K = 2\\) is special case, since lots of things cancel out, but in general need to estimate:\n\n\n\n\n\n\n\n\n\n\nEnglish\nNotation\nHow Many\nFormula\n\n\n\n\nPrior for class \\(k\\)\n\\(\\widehat{\\pi}_k\\)\n\\(K - 1\\)\n\\(\\widehat{\\pi}_k = n_k / n\\)\n\n\nEstimated mean for class \\(k\\)\n\\(\\widehat{\\mu}_k\\)\n\\(K\\)\n\\(\\widehat{\\mu}_k = \\displaystyle \\frac{1}{n_k}\\sum_{\\{i \\mid y_i = k\\}}x_i\\)\n\n\nEstimated (shared) variance\n\\(\\widehat{\\sigma}^2\\)\n1\n\\(\\widehat{\\sigma}^2 = \\displaystyle \\frac{1}{n - K}\\sum_{k = 1}^{K}\\sum_{i:y_i = k}(x_i - \\widehat{\\mu}_k)^2\\)\n\n\n\nTotal:\n\\(2K\\)\n\n\n\n\n\n(Keep in mind for fancier methods! This may blow up to be much larger than \\(n\\))"
  },
  {
    "objectID": "w09/slides.html#lda-with-multiple-features-here-p-2",
    "href": "w09/slides.html#lda-with-multiple-features-here-p-2",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "LDA with Multiple Features (Here \\(p = 2\\))",
    "text": "LDA with Multiple Features (Here \\(p = 2\\))\n\nWithin each class \\(k\\), values of \\(\\mathbf{x}\\) are (multivariate) normally distributed:\n\n\\[\n\\left( \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix} \\middle| ~ Y = k \\right) \\sim \\mathbf{\\mathcal{N}}_2(\\param{\\boldsymbol\\mu_k}, \\param{\\mathbf{\\Sigma}})\n\\]\n\nIncreasing \\(p\\) to 2 and \\(K\\) to 3 means more parameters, but still linear boundaries. It turns out: shared variance (\\(\\sigma^2\\) or \\(\\mathbf{\\Sigma}\\)) will always produce linear boundaries ü§î\n\n\nISLR Figure 4.6: Like before, dashed lines are ‚Äútrue‚Äù boundaries while solid lines are boundaries estimated from data"
  },
  {
    "objectID": "w09/slides.html#quadratic-class-boundaries",
    "href": "w09/slides.html#quadratic-class-boundaries",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Quadratic Class Boundaries",
    "text": "Quadratic Class Boundaries\n\nTo achieve non-linear boundaries, estimate covariance matrix \\(\\mathbf{\\Sigma}_k\\) for each class \\(k\\):\n\n\\[\n\\left( \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix} \\middle| ~ Y = k \\right) \\sim \\mathbf{\\mathcal{N}}_2(\\param{\\boldsymbol\\mu_k}, \\param{\\mathbf{\\Sigma}_k})\n\\]\n\nPros: Non-linear class boundaries! Cons: More parameters to estimate, does worse than LDA if data linearly-separable (or nearly linearly-separable).\nDeciding factor: do you think DGP produces normal classes with same variance?\n\n\nISLR Figure 4.9: Dashed purple line is ‚Äútrue‚Äù boundary (Bayes decision boundary), dotted black line is LDA boundary, solid green line is QDA boundary"
  },
  {
    "objectID": "w09/slides.html#key-advantage-of-generative-model",
    "href": "w09/slides.html#key-advantage-of-generative-model",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Key Advantage of Generative Model",
    "text": "Key Advantage of Generative Model\n\nYou get an actual ‚Äúpicture‚Äù of what the data looks like!\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{f}_k(X_1)\\)\n\\(\\widehat{f}_k(X_2)\\)\n\\(\\widehat{f}_k(X_3)\\)\n\n\n\\(k = 1\\)\n\n\n\n\n\n\\(k = 2\\)"
  },
  {
    "objectID": "w09/slides.html#classifying-new-points",
    "href": "w09/slides.html#classifying-new-points",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Classifying New Points",
    "text": "Classifying New Points\n\nFor new feature values \\(x_{ij}\\), compare how likely this value is under \\(k = 1\\) vs.¬†\\(k = 2\\)\nExample: \\(\\mathbf{x} = (0.4, 1.5, 1)^{\\top}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{f}_k(X_1)\\)\n\\(\\widehat{f}_k(X_2)\\)\n\\(\\widehat{f}_k(X_3)\\)\n\n\n\\(k = 1\\)\n\n\n\n\n\n\n\\(f_1(0.4) = 0.368\\)\n\\(f_1(1.5) = 0.484\\)\n\\(f_1(1) = 0.226\\)\n\n\n\\(k = 2\\)\n\n\n\n\n\n\n\\(f_2(0.4) = 0.030\\)\n\\(f_2(1.5) = 0.130\\)\n\\(f_2(1) = 0.616\\)"
  },
  {
    "objectID": "w09/slides.html#appendix-fuller-logistic-derivation",
    "href": "w09/slides.html#appendix-fuller-logistic-derivation",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "Appendix: Fuller Logistic Derivation",
    "text": "Appendix: Fuller Logistic Derivation\n\\[\n\\begin{align*}\n&\\log\\left[ \\frac{\\Pr(Y = 1 \\mid X)}{1 - \\Pr(Y = 1 \\mid X)} \\right] = \\beta_0 + \\beta_1 X \\\\\n&\\iff \\frac{\\Pr(Y = 1 \\mid X = x_i)}{1 - \\Pr(Y = 1\\ \\mid X = x_i)} = e^{\\beta_0 + \\beta_1 X} \\\\\n&\\iff \\Pr(Y = 1 \\mid X) = e^{\\beta_0 + \\beta_1 X}(1 - \\Pr(Y = 1 \\mid X)) \\\\\n&\\iff \\Pr(Y = 1 \\mid X) = e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}\\Pr(Y = 1 \\mid X) \\\\\n&\\iff \\Pr(Y = 1 \\mid X) + e^{\\beta_0 + \\beta_1 X}\\Pr(Y = 1 \\mid X) = e^{\\beta_0 + \\beta_1 X} \\\\\n&\\iff \\Pr(Y = 1 \\mid X)(1 + e^{\\beta_0 + \\beta_1 X}) = e^{\\beta_0 + \\beta_1 X} \\\\\n&\\iff \\Pr(Y = 1 \\mid X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w09/slides.html#references",
    "href": "w09/slides.html#references",
    "title": "Week 9: Generative vs.¬†Discriminative Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Introduction to the Course",
    "section": "",
    "text": "Week 1 was held on Zoom as a combined session of the three sections of DSAN 5300\nThe recording can be found here",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html",
    "href": "w12/index.html",
    "title": "Week 12: Survival Analysis",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#roadmap",
    "href": "w12/index.html#roadmap",
    "title": "Week 12: Survival Analysis",
    "section": "Roadmap",
    "text": "Roadmap\n\n Basic tool: survival curve \\(S(t)\\) = probability of surviving past period \\(t\\)\nComplicating factor: Censored obs (e.g., drop out of study)\n\\(\\Rightarrow\\) Need to think about DGP: Why is observation \\(i\\) censored while \\(j\\) is observed?\n\nBasic estimators only valid if censoring \\(\\perp\\) survival time\n\n How to compare survival between two groups\n Regression: Effect of features \\(X_1, \\ldots, X_p\\) on survival",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#read-left-to-right-rightarrow-sequence-of-events",
    "href": "w12/index.html#read-left-to-right-rightarrow-sequence-of-events",
    "title": "Week 12: Survival Analysis",
    "section": "Read Left to Right \\(\\Rightarrow\\) Sequence of Events",
    "text": "Read Left to Right \\(\\Rightarrow\\) Sequence of Events\n\n\n\nModified ISLR Figure 11.1",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#slice-at-deaths-rightarrow-at-risk-observations",
    "href": "w12/index.html#slice-at-deaths-rightarrow-at-risk-observations",
    "title": "Week 12: Survival Analysis",
    "section": "‚ÄúSlice‚Äù at Deaths \\(\\Rightarrow\\) At-Risk Observations",
    "text": "‚ÄúSlice‚Äù at Deaths \\(\\Rightarrow\\) At-Risk Observations\n\n\n\nModified ISLR Figure 11.1",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#the-actual-dataset-we-get",
    "href": "w12/index.html#the-actual-dataset-we-get",
    "title": "Week 12: Survival Analysis",
    "section": "The Actual Dataset We Get",
    "text": "The Actual Dataset We Get\n\n\n\nPatient (\\(i\\))\nObserved Outcome (\\(Y_i\\))\nObserved? (\\(\\delta_i\\))\n\n\n\n\n1\n300\n1\n\n\n2\n365\n0\n\n\n3\n150\n1\n\n\n4\n250\n0",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#the-dataset-we-can-infer",
    "href": "w12/index.html#the-dataset-we-can-infer",
    "title": "Week 12: Survival Analysis",
    "section": "The Dataset We Can Infer",
    "text": "The Dataset We Can Infer\n\n\n\n\n\n\n\n\n\n\nPatient (\\(i\\))\n\\(Y_i\\)\n\\(\\delta_i\\)\nSurvival Time (\\(T_i\\))\nCensor Point (\\(C_i\\))\n\n\n\n\n1\n300\n1\n300\nNA\n\n\n2\n365\n0\nNA\n365\n\n\n3\n150\n1\n150\nNA\n\n\n4\n250\n0\nNA\n250\n\n\n\n‚Ä¶If we‚Äôre testing effect of treatment, which column do we most care about?",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#measuring-effect-of-treatment",
    "href": "w12/index.html#measuring-effect-of-treatment",
    "title": "Week 12: Survival Analysis",
    "section": "Measuring Effect of Treatment!",
    "text": "Measuring Effect of Treatment!\n\n\n\n\n\n\n\n\n\n\n\nPatient (\\(i\\))\n\\(Y_i\\)\n\\(\\delta_i\\)\nSurvival Time (\\(T_i\\))\nCensor Point (\\(C_i\\))\n\n\n\n\n1\n300\n1\n300\nNA\n\n\n2\n365\n0\nNA\n365\n\n\n3\n150\n1\n150\nNA\n\n\n4\n250\n0\nNA\n250\n\n\n\n‚Ä¶If we‚Äôre testing effect of treatment, \\(T_i\\) is what we care about!",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#basic-question-1-survival",
    "href": "w12/index.html#basic-question-1-survival",
    "title": "Week 12: Survival Analysis",
    "section": "Basic Question 1: Survival",
    "text": "Basic Question 1: Survival\n\nLet \\(T\\) be a RV representing time of death for a patient\nWhat is probability that patient survives past given time \\(t\\)?\n\n\\[\nS_T(t) = \\Pr(T &gt; t)\n\\]\n\nNote relationship to something you saw in 5100!\n\\(S_T(t)\\) defined to be \\(1 - F_T(t)\\), where \\(F_T(t)\\) is CDF of \\(T\\):\n\n\\[\nF_T(t) = \\Pr(T \\leq t)\n\\]",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#kaplan-meier-estimator-intuition",
    "href": "w12/index.html#kaplan-meier-estimator-intuition",
    "title": "Week 12: Survival Analysis",
    "section": "Kaplan-Meier Estimator: Intuition",
    "text": "Kaplan-Meier Estimator: Intuition\nEach death event \\(d_k\\) gives us info that survival probability lower by some amount\n Break \\(S(t)\\) into sequence of stepwise changes at \\(d_1, \\ldots, d_K\\):\n\\[\n\\begin{align*}\nS(d_k) = \\Pr(T &gt; d_k) &\\overset{\\mathclap{\\small\\text{LTP}}}{=} \\Pr(T &gt; d_k \\mid T &gt; d_{k-1})\\overbrace{\\Pr(T &gt; d_{k-1})}^{S(d_{k-1})} \\\\\n&\\phantom{=} \\; \\; + \\underbrace{\\Pr(T &gt; d_k \\mid T \\leq d_{k-1})}_{\\text{Contradiction} \\implies \\Pr = 0}\\Pr(T \\leq d_{k-1})\n\\end{align*}\n\\]\n\n\n Gives us a recurrence relation:\n\\[\n\\begin{align*}\nS(d_k) &= \\Pr(T &gt; d_k \\mid T &gt; d_{k-1})S(d_{k-1}) \\\\\nS(d_{k-1}) &= \\Pr(T &gt; d_{k-1} \\mid T &gt; d_{k-2})S(d_{k-2}) \\\\\n%S(d_{k-2}) &= \\Pr(T &gt; d_{k-2} \\mid T &gt; d_{k-3})S(d_{k-3}) \\\\\n&\\vdots \\\\\nS(d_2) &= \\Pr(T &gt; d_2 \\mid T &gt; d_1)S(d_1) \\\\\nS(d_1) &= \\Pr(T &gt; d_1 \\mid T &gt; d_0)S(d_0) = \\Pr(T &gt; d_1)\n\\end{align*}\n\\]\n\n Plug each eq into eq above it to derive:\n\\[\n\\begin{align*}\nS(d_k) = &\\Pr(\\underbrace{T &gt; d_k}_{\\mathclap{\\small\\text{Survives past }d_k}} \\; \\mid \\; \\underbrace{T &gt; d_{k-1}}_{\\mathclap{\\small\\text{Survives past }d_{k-1}}}) \\\\\n&\\times \\Pr(\\underbrace{T &gt; d_{k-1}}_{\\small\\text{Survives past }d_{k-1}} \\mid \\underbrace{T &gt; d_{k-2}}_{\\small\\text{Survives past }d_{k-2}}) \\\\\n&\\times \\cdots \\times \\Pr(\\underbrace{T &gt; d_2}_{\\mathclap{\\small\\text{Survives past }d_2}} \\; \\mid \\; \\underbrace{T &gt; d_1}_{\\mathclap{\\small\\text{Survives past }d_1}}) \\\\\n&\\times \\Pr(\\underbrace{T &gt; d_1}_{\\mathclap{\\small\\text{Survives past }d_1}})\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#kaplan-meier-estimator",
    "href": "w12/index.html#kaplan-meier-estimator",
    "title": "Week 12: Survival Analysis",
    "section": "Kaplan-Meier Estimator",
    "text": "Kaplan-Meier Estimator\n\nDefined at death points \\(d_k\\) as\n\n\\[\n\\widehat{S}(d_k) = \\prod_{j=1}^{k} \\Bigl(\n    \\overbrace{\n        \\frac{\n            r_j - q_j\n        }{\n            \\underbrace{r_j}_{\\mathclap{\\small\\text{Num At Risk}}}\n        }\n    }^{\\mathclap{\\small\\text{Num Survived}}}\n\\Bigr)\n\\]\n\nThen, for \\(t \\in (d_k, d_{k+1})\\), \\(\\widehat{S}(t) = \\widehat{S}(d_k)\\), producing stepwise survival function:\n\n\n\n\nISLR Figure 11.2",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#kaplan-meier-estimator-for-our-example",
    "href": "w12/index.html#kaplan-meier-estimator-for-our-example",
    "title": "Week 12: Survival Analysis",
    "section": "Kaplan-Meier Estimator for our Example",
    "text": "Kaplan-Meier Estimator for our Example\n\n\nTwo death points: \\(d_1 = 150, d_2 = 300\\) (plus start point \\(d_0 = 0\\))\n\\[\n\\begin{align*}\n{\\color{#e69f00}\\widehat{S}(d_0)} &= \\prod_{j=0}^{0}\\left( \\frac{r_k - q_k}{r_k} \\right) = \\left( \\frac{4 - 0}{4} \\right) = {\\color{#e69f00}\\boxed{1}} \\\\\n{\\color{#56b4e9}\\widehat{S}(d_1)} &= \\prod_{j=0}^{1}\\left( \\frac{r_k - q_k}{r_k} \\right) = {\\color{#e69f00}\\boxed{1}} \\cdot \\left( \\frac{r_1-q_1}{r_1} \\right) \\\\\n&= {\\color{#e69f00}\\boxed{1}} \\cdot \\left( \\frac{4 - 1}{4} \\right) = {\\color{#56B4E9}\\boxed{\\frac{3}{4}}} \\\\\n{\\color{#009e73}\\widehat{S}(d_2)} &= \\prod_{j=0}^{2}\\left( \\frac{r_k - q_k}{r_k} \\right) = {\\color{#e69f00}\\boxed{1}} \\cdot {\\color{#56b4e9}\\boxed{\\frac{3}{4}}} \\cdot \\left( \\frac{r_2 - q_2}{r_2} \\right) \\\\\n&= {\\color{#e69f00}\\boxed{1}} \\cdot {\\color{#56b4e9}\\boxed{\\frac{3}{4}}} \\cdot \\left( \\frac{2-1}{2} \\right) = \\frac{3}{4}\\cdot \\frac{1}{2} = {\\color{#009e73}\\boxed{\\frac{3}{8}}}\n\\end{align*}\n\\]\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(survival) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nsurv_df &lt;- tribble(\n  ~id, ~y, ~delta,\n  1, 300, 1,\n  2, 365, 0,\n  3, 150, 1,\n  4, 250, 0\n)\nsurv_obj &lt;- Surv(surv_df$y, event = surv_df$delta)\nsurv_model &lt;- survfit(surv_obj ~ 1)\n# Plot options\npar(mar=c(2,4,1.25,1.0)) # bltr\ny_label &lt;- TeX(\"$\\\\Pr(T &gt; t)$\")\nplot(\n  surv_model,\n  ylab=y_label,\n  lwd=1,\n  main=\"Survival Curve for 4-Patient Example\"\n) # conf.int=FALSE\n# Add colors\n# lines(c(0, 150), c(1.0, 1.0), type='l', col='#E69F00', lwd=2)\nrect(xleft = 0, xright = 150, ybottom = 0, ytop = 1.0, col=\"#E69F0040\", lwd=0)\n# lines(c(150, 300), c(3/4, 3/4), type='l', col='#56B4E9', lwd=2)\nrect(xleft = 150, xright = 300, ybottom = 0, ytop = 1.0, col=\"#56B4E940\", lwd=0)\n# lines(c(300, 365), c(3/8, 3/8), type='l', col='#009E73', lwd=2)\nrect(xleft = 300, xright = 365, ybottom = 0, ytop = 1.0, col=\"#009E7340\", lwd=0)",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#log-rank-test",
    "href": "w12/index.html#log-rank-test",
    "title": "Week 12: Survival Analysis",
    "section": "Log-Rank Test",
    "text": "Log-Rank Test\n\nAt each death event \\(d_k\\), construct a table like:\n\n\n\n\n\nGroup 1\nGroup 2\nTotal\n\n\n\n\nDied\n\\(q_{1k}\\)\n\\(q_{2k}\\)\n\\(q_k\\)\n\n\nSurvived\n\\(r_{1k} - q_{1k}\\)\n\\(r_{2k} - q_{2k}\\)\n\\(r_k - q_k\\)\n\n\nTotal\n\\(r_{1k}\\)\n\\(r_{2k}\\)\n\\(r_k\\)\n\n\n\n\nFocus on \\(q_{1k}\\)! Null hypothesis: across all \\(k \\in \\{1, \\ldots, K\\}\\), \\(q_{1k}\\) not systematically lower or higher than RHS:\n\n\\[\n\\mathbb{E}[ \\underbrace{q_{1k}}_{\\mathclap{\\small\\substack{\\text{Group 1 deaths} \\\\ \\text{at }d_k}}} ] =\n\\overbrace{r_{1k}}^{\\mathclap{\\small\\substack{\\text{At risk in} \\\\[0.2em] \\text{Group 1}}}} \\cdot \\underbrace{\\left( \\frac{q_k}{r_k} \\right)}_{\\mathclap{\\small\\text{Overall death rate at }d_k}}\n\\]",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#log-rank-test-statistic",
    "href": "w12/index.html#log-rank-test-statistic",
    "title": "Week 12: Survival Analysis",
    "section": "Log-Rank Test Statistic",
    "text": "Log-Rank Test Statistic\n\nTest statistic \\(W\\) should ‚Äúdetect‚Äù the alternative hypothesis, on basis of information \\(X\\)‚Ä¶ We can use \\(\\boxed{\\textstyle X = \\sum_{k=1}^{K}q_{1k}}\\)!\nHere, log-rank test statistic \\(W\\) detects how much \\(q_{1k}\\) deviates from expected value from prev slide:\n\n\\[\n\\begin{align*}\nW &= \\frac{X - \\mathbb{E}[X]}{\\sqrt{\\text{Var}[X]}} = \\frac{\n  \\sum_{k=1}^{K}q_{1k} - \\mathbb{E}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]\n}{\n  \\sqrt{\\text{Var}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]}\n} = \\frac{\n  \\sum_{k=1}^{K} \\left( q_{1k} - \\mathbb{E}[q_{1k}] \\right)\n}{\n  \\sqrt{\\text{Var}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]}\n} \\\\\n&= \\frac{\n  \\sum_{k=1}^{K}\\left( q_{1k} - r_{1k}\\cdot \\frac{q_k}{r_k} \\right)\n}{\n  \\sqrt{\\text{Var}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]}\n} \\underset{\\small\\text{Ex 11.7}}{\\overset{\\small\\text{ISLR}}{=}}\n\\frac{\n  \\sum_{k=1}^{K}\\left( q_{1k} - r_{1k}\\cdot \\frac{q_k}{r_k} \\right)\n}{\n  \\sqrt{\\sum_{k=1}^{K} \\frac{\n    q_k(r_{1k}/r_k)(1 - r_{1k}/r_k)(r_k - q_k)\n  }{\n    r_k - 1\n  }}\n}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#the-hazard-function",
    "href": "w12/index.html#the-hazard-function",
    "title": "Week 12: Survival Analysis",
    "section": "The Hazard Function",
    "text": "The Hazard Function\n\nDeath rate at tiny instant after \\(t\\) (between \\(t\\) and \\(t + \\Delta t\\)), given survival past \\(t\\):\n\n\\[\nh(t) \\definedas \\lim_{\\Delta t \\rightarrow 0}\\frac{\\Pr(t &lt; T \\leq t + \\Delta t)}{\\Delta t}\n\\]\n\nIf we define a RV \\(T_{&gt;t} \\definedas [T \\mid T &gt; t]\\), \\(h(t)\\) is the pdf of \\(T_{&gt;t}\\)!\nCan relate \\(h(t)\\) to quantities we know (e.g., from 5100):\n\n\\[\n\\underbrace{h(t)}_{\\small\\text{pdf of }T_{&gt;t}} = \\frac{\\overbrace{f(t)}^{\\small\\text{pdf of }T}}{\\underbrace{S(t)}_{\\small \\Pr(T &gt; t)}}\n\\]",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#proportional-hazard-assumption",
    "href": "w12/index.html#proportional-hazard-assumption",
    "title": "Week 12: Survival Analysis",
    "section": "Proportional Hazard Assumption",
    "text": "Proportional Hazard Assumption\n\\[\nh(t \\mid x_i) = h_0(t)\\exp\\left[ \\sum_{j=1}^{p}\\beta_j x_{ij} \\right] \\iff \\underbrace{\\log[h(t \\mid x_i)]}_{\\hbar(t \\mid x_i)} = \\underbrace{\\log[h_0(t)]}_{\\hbar_0(t)} + \\sum_{j=1}^{p}\\beta_j x_{ij}\n\\]\nBasically: Features \\(X_{j}\\) shift [log] baseline hazard function \\(\\hbar_0(t)\\) up and down by constant amounts, via multiplication by \\(e^{\\beta_j}\\)\n\n\n\n\nTop row: \\(\\hbar_0(t)\\) in black, \\(X_j = 1\\) shifts it down via multiplication by \\(e^{\\beta_j}\\) to form \\(\\hbar(t \\mid X_j)\\) in green\nBottom row: Proportional hazard violated, since \\(X_j = 1\\) associated with different changes to \\(\\hbar_0(t)\\) at different \\(t\\) values",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#cox-proportional-hazard-model",
    "href": "w12/index.html#cox-proportional-hazard-model",
    "title": "Week 12: Survival Analysis",
    "section": "Cox Proportional Hazard Model",
    "text": "Cox Proportional Hazard Model\n\nIntuition: Best \\(\\beta\\)s are those which best predict \\(i\\)‚Äôs death among all at risk at same time. Called ‚ÄúPartial Likelihood‚Äù \\(\\text{PL}(\\boldsymbol\\beta)\\) since we don‚Äôt need to estimate \\(h_0(t)\\)!\n\n\\[\n\\prod_{i : \\, \\delta_i = 1} \\; \\frac{\n  {\\color{red}\\cancel{\\color{black}h_0(t)}}\\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{ij} \\right]\n}{\n  {\\displaystyle \\sum\\limits_{\\mathclap{i': \\, y_{i'} \\geq y_i}}} {\\color{red}\\cancel{\\color{black}h_0(t)}}\\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{i'j} \\right]\n} = \\prod_{i : \\, \\delta_i = 1} \\; \\frac{\n  \\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{ij} \\right]\n}{\n  {\\displaystyle \\sum\\limits_{\\mathclap{i': \\, y_{i'} \\geq y_i}}} \\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{i'j} \\right]\n}\n\\]\n\nAlso note the missing intercept \\(\\beta_0\\): handled by the baseline hazard function \\(h_0(t)\\) (which we cancel out anyways!)",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#in-code",
    "href": "w12/index.html#in-code",
    "title": "Week 12: Survival Analysis",
    "section": "In Code",
    "text": "In Code\n\nWe‚Äôll use the survival library in R, very similar syntax to lm(), glm(), etc.!\n\n\n\nCode\nlibrary(survival) |&gt; suppressPackageStartupMessages()\nlibrary(ISLR2) |&gt; suppressPackageStartupMessages()\nbc_df &lt;- BrainCancer |&gt; filter(diagnosis != \"Other\")\nbc_df$diagnosis = factor(bc_df$diagnosis)\nbc_df$sex &lt;- factor(substr(bc_df$sex, 1, 1))\nbc_df$loc &lt;- factor(substr(bc_df$loc, 1, 5))\noptions(width=130)\nsummary(bc_df)\n\n\n sex         diagnosis     loc           ki              gtv         stereo       status           time      \n F:39   Meningioma:42   Infra:10   Min.   : 40.00   Min.   : 0.040   SRS:19   Min.   :0.000   Min.   : 0.07  \n M:34   LG glioma : 9   Supra:63   1st Qu.: 80.00   1st Qu.: 2.500   SRT:54   1st Qu.:0.000   1st Qu.: 9.77  \n        HG glioma :22              Median : 80.00   Median : 6.480            Median :0.000   Median :26.46  \n                                   Mean   : 81.37   Mean   : 8.297            Mean   :0.411   Mean   :27.83  \n                                   3rd Qu.: 90.00   3rd Qu.:11.380            3rd Qu.:1.000   3rd Qu.:41.44  \n                                   Max.   :100.00   Max.   :33.690            Max.   :1.000   Max.   :82.56  \n\n\n\n\nCode\nbc_df |&gt; head(5)\n\n\n\n\n\n\nsex\ndiagnosis\nloc\nki\ngtv\nstereo\nstatus\ntime\n\n\n\n\nF\nMeningioma\nInfra\n90\n6.11\nSRS\n0\n57.64\n\n\nM\nHG glioma\nSupra\n90\n19.35\nSRT\n1\n8.98\n\n\nF\nMeningioma\nInfra\n70\n7.95\nSRS\n0\n26.46\n\n\nF\nLG glioma\nSupra\n80\n7.61\nSRT\n1\n47.80\n\n\nM\nHG glioma\nSupra\n90\n5.06\nSRT\n1\n6.30",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#coxph-estimation",
    "href": "w12/index.html#coxph-estimation",
    "title": "Week 12: Survival Analysis",
    "section": "coxph() Estimation",
    "text": "coxph() Estimation\n\n\n\nCode\nlibrary(broom) |&gt; suppressPackageStartupMessages()\nfull_cox_model &lt;- coxph(\n  Surv(time, status) ~ sex + diagnosis + loc + ki + gtv + stereo,\n  data=bc_df\n)\nbroom::tidy(full_cox_model) |&gt; mutate_if(is.numeric, round, 3)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nsexM\n0.456\n0.396\n1.154\n0.249\n\n\ndiagnosisLG glioma\n0.864\n0.641\n1.347\n0.178\n\n\ndiagnosisHG glioma\n2.116\n0.470\n4.504\n0.000\n\n\nlocSupra\n1.482\n1.105\n1.342\n0.180\n\n\nki\n-0.048\n0.020\n-2.406\n0.016\n\n\ngtv\n0.036\n0.026\n1.361\n0.173\n\n\nstereoSRT\n-0.475\n0.591\n-0.803\n0.422\n\n\n\n\n\n\n\nDiagnosis: Relative to baseline of Meningioma, HG (High-Grade) glioma associated with \\(e^{2.155} \\approx 8.628\\) times greater hazard\nKarnofsky Index (ki): 1-unit increase associated with reduction of hazard to \\(e^{-0.055} \\approx 94.65\\)% of previous value [0-100 scale of self-functioning abilities]",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#survival-curves-for-each-diagnosis",
    "href": "w12/index.html#survival-curves-for-each-diagnosis",
    "title": "Week 12: Survival Analysis",
    "section": "Survival Curves for Each Diagnosis",
    "text": "Survival Curves for Each Diagnosis\n\n\nCode\nlibrary(extrafont) |&gt; suppressPackageStartupMessages()\npar(cex=1.2, family=\"CMU Sans Serif\")\ndiag_levels &lt;- c(\"Meningioma\", \"LG glioma\", \"HG glioma\")\ndiag_df &lt;- tibble(\n  diagnosis = diag_levels,\n  sex = rep(\"F\", 3),\n  loc = rep(\"Supra\", 3),\n  ki = rep(mean(bc_df$ki), 3),\n  gtv = rep(mean(bc_df$gtv), 3),\n  stereo = rep(\"SRT\", 3)\n)\nsurvplots &lt;- survfit(full_cox_model, newdata = diag_df)\nplot(\n  survplots,\n  main = \"Survival Curves by Diagnosis\",\n  xlab = \"Months\", ylab = \"Survival Probability\",\n  col = cb_palette, lwd=1.5\n)\nlegend(\n  \"bottomleft\",\n  diag_levels,\n  col = cb_palette, lty = 1, lwd=1.5\n)",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#less-straightforward-curves-for-each-ki-gtv-val",
    "href": "w12/index.html#less-straightforward-curves-for-each-ki-gtv-val",
    "title": "Week 12: Survival Analysis",
    "section": "Less Straightforward: Curves for Each ki, gtv Val",
    "text": "Less Straightforward: Curves for Each ki, gtv Val\n\nTechnically a different survival curve for each value of ki \\(\\in [0, 100]\\), gtv \\(\\in (0, \\infty)\\)\n\n\n\n\n\nCode\nbc_df |&gt; ggplot(aes(x=ki)) +\n  geom_density(\n    linewidth=g_linewidth,\n    fill=cb_palette[1], alpha=0.333\n  ) +\n  theme_dsan(base_size=28) +\n  labs(title = \"Karnofsky Index Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nbc_df |&gt; ggplot(aes(x=gtv)) +\n  geom_density(\n    linewidth=g_linewidth,\n    fill=cb_palette[1], alpha=0.333\n  ) +\n  theme_dsan(base_size=28) +\n  labs(title = \"Gross Tumor Volume (GTV) Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\nOne approach: bin into (low, medium, high) via terciles, one curve per bin median:\n\n\n\n\n\nCode\nki_terciles &lt;- quantile(bc_df$ki, c(1/3, 2/3))\nbc_df &lt;- bc_df |&gt; mutate(\n  tercile = ifelse(ki &lt; ki_terciles[1], 1, ifelse(ki &lt; ki_terciles[2], 2, 3))\n)\n(terc_df &lt;- bc_df |&gt;\n  group_by(tercile) |&gt;\n  summarize(med_ki=median(ki)))\n\n\n\n\n\n\ntercile\nmed_ki\n\n\n\n\n1\n70\n\n\n2\n80\n\n\n3\n90\n\n\n\n\n\n\n\n\n\nCode\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nki_df &lt;- tibble(\n  diagnosis = rep(\"Meningioma\", 3),\n  sex = rep(\"F\", 3),\n  loc = rep(\"Supra\", 3),\n  ki = terc_df$med_ki,\n  gtv = rep(mean(bc_df$gtv), 3),\n  stereo = rep(\"SRT\", 3)\n)\nki_plots &lt;- survfit(full_cox_model, newdata = ki_df)\npar(\n  mar=c(4.0,4.0,1.2,0.5),\n  cex=1.2,\n  family=\"CMU Sans Serif\"\n) # bltr\nplot(\n  ki_plots,\n  main = \"Survival Curves by KI Tercile\",\n  xlab = \"Months\",\n  ylab = TeX(\"$\\\\Pr(T &gt; t)$\"),\n  lwd = 1,\n  col = cb_palette\n)\nki_labs &lt;- c(\n  TeX(\"$h( t \\\\, | \\\\, KI = 70 )$\"),\n  TeX(\"$h( t \\\\, | \\\\, KI = 80 )$\"),\n  TeX(\"$h( t \\\\, | \\\\, KI = 90 )$\")\n)\nlegend(\n  \"bottomleft\",\n  ki_labs, lwd=1,\n  col = cb_palette, lty = 1, cex=0.8\n)",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/slides.html#roadmap",
    "href": "w12/slides.html#roadmap",
    "title": "Week 12: Survival Analysis",
    "section": "Roadmap",
    "text": "Roadmap\n\n Basic tool: survival curve \\(S(t)\\) = probability of surviving past period \\(t\\)\nComplicating factor: Censored obs (e.g., drop out of study)\n\\(\\Rightarrow\\) Need to think about DGP: Why is observation \\(i\\) censored while \\(j\\) is observed?\n\nBasic estimators only valid if censoring \\(\\perp\\) survival time\n\n How to compare survival between two groups\n Regression: Effect of features \\(X_1, \\ldots, X_p\\) on survival"
  },
  {
    "objectID": "w12/slides.html#read-left-to-right-rightarrow-sequence-of-events",
    "href": "w12/slides.html#read-left-to-right-rightarrow-sequence-of-events",
    "title": "Week 12: Survival Analysis",
    "section": "Read Left to Right \\(\\Rightarrow\\) Sequence of Events",
    "text": "Read Left to Right \\(\\Rightarrow\\) Sequence of Events\n\nModified ISLR Figure 11.1"
  },
  {
    "objectID": "w12/slides.html#slice-at-deaths-rightarrow-at-risk-observations",
    "href": "w12/slides.html#slice-at-deaths-rightarrow-at-risk-observations",
    "title": "Week 12: Survival Analysis",
    "section": "‚ÄúSlice‚Äù at Deaths \\(\\Rightarrow\\) At-Risk Observations",
    "text": "‚ÄúSlice‚Äù at Deaths \\(\\Rightarrow\\) At-Risk Observations\n\nModified ISLR Figure 11.1"
  },
  {
    "objectID": "w12/slides.html#the-actual-dataset-we-get",
    "href": "w12/slides.html#the-actual-dataset-we-get",
    "title": "Week 12: Survival Analysis",
    "section": "The Actual Dataset We Get",
    "text": "The Actual Dataset We Get\n\n\n\nPatient (\\(i\\))\nObserved Outcome (\\(Y_i\\))\nObserved? (\\(\\delta_i\\))\n\n\n\n\n1\n300\n1\n\n\n2\n365\n0\n\n\n3\n150\n1\n\n\n4\n250\n0"
  },
  {
    "objectID": "w12/slides.html#the-dataset-we-can-infer",
    "href": "w12/slides.html#the-dataset-we-can-infer",
    "title": "Week 12: Survival Analysis",
    "section": "The Dataset We Can Infer",
    "text": "The Dataset We Can Infer\n\n\n\n\n\n\n\n\n\n\nPatient (\\(i\\))\n\\(Y_i\\)\n\\(\\delta_i\\)\nSurvival Time (\\(T_i\\))\nCensor Point (\\(C_i\\))\n\n\n\n\n1\n300\n1\n300\nNA\n\n\n2\n365\n0\nNA\n365\n\n\n3\n150\n1\n150\nNA\n\n\n4\n250\n0\nNA\n250\n\n\n\n‚Ä¶If we‚Äôre testing effect of treatment, which column do we most care about?"
  },
  {
    "objectID": "w12/slides.html#measuring-effect-of-treatment",
    "href": "w12/slides.html#measuring-effect-of-treatment",
    "title": "Week 12: Survival Analysis",
    "section": "Measuring Effect of Treatment!",
    "text": "Measuring Effect of Treatment!\n\n\n\n\n\n\n\n\n\n\n\nPatient (\\(i\\))\n\\(Y_i\\)\n\\(\\delta_i\\)\nSurvival Time (\\(T_i\\))\nCensor Point (\\(C_i\\))\n\n\n\n\n1\n300\n1\n300\nNA\n\n\n2\n365\n0\nNA\n365\n\n\n3\n150\n1\n150\nNA\n\n\n4\n250\n0\nNA\n250\n\n\n\n‚Ä¶If we‚Äôre testing effect of treatment, \\(T_i\\) is what we care about!"
  },
  {
    "objectID": "w12/slides.html#basic-question-1-survival",
    "href": "w12/slides.html#basic-question-1-survival",
    "title": "Week 12: Survival Analysis",
    "section": "Basic Question 1: Survival",
    "text": "Basic Question 1: Survival\n\nLet \\(T\\) be a RV representing time of death for a patient\nWhat is probability that patient survives past given time \\(t\\)?\n\n\\[\nS_T(t) = \\Pr(T &gt; t)\n\\]\n\nNote relationship to something you saw in 5100!\n\\(S_T(t)\\) defined to be \\(1 - F_T(t)\\), where \\(F_T(t)\\) is CDF of \\(T\\):\n\n\\[\nF_T(t) = \\Pr(T \\leq t)\n\\]"
  },
  {
    "objectID": "w12/slides.html#kaplan-meier-estimator-intuition",
    "href": "w12/slides.html#kaplan-meier-estimator-intuition",
    "title": "Week 12: Survival Analysis",
    "section": "Kaplan-Meier Estimator: Intuition",
    "text": "Kaplan-Meier Estimator: Intuition\nEach death event \\(d_k\\) gives us info that survival probability lower by some amount\n Break \\(S(t)\\) into sequence of stepwise changes at \\(d_1, \\ldots, d_K\\):\n\\[\n\\begin{align*}\nS(d_k) = \\Pr(T &gt; d_k) &\\overset{\\mathclap{\\small\\text{LTP}}}{=} \\Pr(T &gt; d_k \\mid T &gt; d_{k-1})\\overbrace{\\Pr(T &gt; d_{k-1})}^{S(d_{k-1})} \\\\\n&\\phantom{=} \\; \\; + \\underbrace{\\Pr(T &gt; d_k \\mid T \\leq d_{k-1})}_{\\text{Contradiction} \\implies \\Pr = 0}\\Pr(T \\leq d_{k-1})\n\\end{align*}\n\\]\n\n\n Gives us a recurrence relation:\n\\[\n\\begin{align*}\nS(d_k) &= \\Pr(T &gt; d_k \\mid T &gt; d_{k-1})S(d_{k-1}) \\\\\nS(d_{k-1}) &= \\Pr(T &gt; d_{k-1} \\mid T &gt; d_{k-2})S(d_{k-2}) \\\\\n%S(d_{k-2}) &= \\Pr(T &gt; d_{k-2} \\mid T &gt; d_{k-3})S(d_{k-3}) \\\\\n&\\vdots \\\\\nS(d_2) &= \\Pr(T &gt; d_2 \\mid T &gt; d_1)S(d_1) \\\\\nS(d_1) &= \\Pr(T &gt; d_1 \\mid T &gt; d_0)S(d_0) = \\Pr(T &gt; d_1)\n\\end{align*}\n\\]\n\n Plug each eq into eq above it to derive:\n\\[\n\\begin{align*}\nS(d_k) = &\\Pr(\\underbrace{T &gt; d_k}_{\\mathclap{\\small\\text{Survives past }d_k}} \\; \\mid \\; \\underbrace{T &gt; d_{k-1}}_{\\mathclap{\\small\\text{Survives past }d_{k-1}}}) \\\\\n&\\times \\Pr(\\underbrace{T &gt; d_{k-1}}_{\\small\\text{Survives past }d_{k-1}} \\mid \\underbrace{T &gt; d_{k-2}}_{\\small\\text{Survives past }d_{k-2}}) \\\\\n&\\times \\cdots \\times \\Pr(\\underbrace{T &gt; d_2}_{\\mathclap{\\small\\text{Survives past }d_2}} \\; \\mid \\; \\underbrace{T &gt; d_1}_{\\mathclap{\\small\\text{Survives past }d_1}}) \\\\\n&\\times \\Pr(\\underbrace{T &gt; d_1}_{\\mathclap{\\small\\text{Survives past }d_1}})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w12/slides.html#kaplan-meier-estimator",
    "href": "w12/slides.html#kaplan-meier-estimator",
    "title": "Week 12: Survival Analysis",
    "section": "Kaplan-Meier Estimator",
    "text": "Kaplan-Meier Estimator\n\nDefined at death points \\(d_k\\) as\n\n\\[\n\\widehat{S}(d_k) = \\prod_{j=1}^{k} \\Bigl(\n    \\overbrace{\n        \\frac{\n            r_j - q_j\n        }{\n            \\underbrace{r_j}_{\\mathclap{\\small\\text{Num At Risk}}}\n        }\n    }^{\\mathclap{\\small\\text{Num Survived}}}\n\\Bigr)\n\\]\n\nThen, for \\(t \\in (d_k, d_{k+1})\\), \\(\\widehat{S}(t) = \\widehat{S}(d_k)\\), producing stepwise survival function:\n\n\nISLR Figure 11.2"
  },
  {
    "objectID": "w12/slides.html#kaplan-meier-estimator-for-our-example",
    "href": "w12/slides.html#kaplan-meier-estimator-for-our-example",
    "title": "Week 12: Survival Analysis",
    "section": "Kaplan-Meier Estimator for our Example",
    "text": "Kaplan-Meier Estimator for our Example\n\n\nTwo death points: \\(d_1 = 150, d_2 = 300\\) (plus start point \\(d_0 = 0\\))\n\\[\n\\begin{align*}\n{\\color{#e69f00}\\widehat{S}(d_0)} &= \\prod_{j=0}^{0}\\left( \\frac{r_k - q_k}{r_k} \\right) = \\left( \\frac{4 - 0}{4} \\right) = {\\color{#e69f00}\\boxed{1}} \\\\\n{\\color{#56b4e9}\\widehat{S}(d_1)} &= \\prod_{j=0}^{1}\\left( \\frac{r_k - q_k}{r_k} \\right) = {\\color{#e69f00}\\boxed{1}} \\cdot \\left( \\frac{r_1-q_1}{r_1} \\right) \\\\\n&= {\\color{#e69f00}\\boxed{1}} \\cdot \\left( \\frac{4 - 1}{4} \\right) = {\\color{#56B4E9}\\boxed{\\frac{3}{4}}} \\\\\n{\\color{#009e73}\\widehat{S}(d_2)} &= \\prod_{j=0}^{2}\\left( \\frac{r_k - q_k}{r_k} \\right) = {\\color{#e69f00}\\boxed{1}} \\cdot {\\color{#56b4e9}\\boxed{\\frac{3}{4}}} \\cdot \\left( \\frac{r_2 - q_2}{r_2} \\right) \\\\\n&= {\\color{#e69f00}\\boxed{1}} \\cdot {\\color{#56b4e9}\\boxed{\\frac{3}{4}}} \\cdot \\left( \\frac{2-1}{2} \\right) = \\frac{3}{4}\\cdot \\frac{1}{2} = {\\color{#009e73}\\boxed{\\frac{3}{8}}}\n\\end{align*}\n\\]\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(survival) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nsurv_df &lt;- tribble(\n  ~id, ~y, ~delta,\n  1, 300, 1,\n  2, 365, 0,\n  3, 150, 1,\n  4, 250, 0\n)\nsurv_obj &lt;- Surv(surv_df$y, event = surv_df$delta)\nsurv_model &lt;- survfit(surv_obj ~ 1)\n# Plot options\npar(mar=c(2,4,1.25,1.0)) # bltr\ny_label &lt;- TeX(\"$\\\\Pr(T &gt; t)$\")\nplot(\n  surv_model,\n  ylab=y_label,\n  lwd=1,\n  main=\"Survival Curve for 4-Patient Example\"\n) # conf.int=FALSE\n# Add colors\n# lines(c(0, 150), c(1.0, 1.0), type='l', col='#E69F00', lwd=2)\nrect(xleft = 0, xright = 150, ybottom = 0, ytop = 1.0, col=\"#E69F0040\", lwd=0)\n# lines(c(150, 300), c(3/4, 3/4), type='l', col='#56B4E9', lwd=2)\nrect(xleft = 150, xright = 300, ybottom = 0, ytop = 1.0, col=\"#56B4E940\", lwd=0)\n# lines(c(300, 365), c(3/8, 3/8), type='l', col='#009E73', lwd=2)\nrect(xleft = 300, xright = 365, ybottom = 0, ytop = 1.0, col=\"#009E7340\", lwd=0)"
  },
  {
    "objectID": "w12/slides.html#log-rank-test",
    "href": "w12/slides.html#log-rank-test",
    "title": "Week 12: Survival Analysis",
    "section": "Log-Rank Test",
    "text": "Log-Rank Test\n\nAt each death event \\(d_k\\), construct a table like:\n\n\n\n\n\nGroup 1\nGroup 2\nTotal\n\n\n\n\nDied\n\\(q_{1k}\\)\n\\(q_{2k}\\)\n\\(q_k\\)\n\n\nSurvived\n\\(r_{1k} - q_{1k}\\)\n\\(r_{2k} - q_{2k}\\)\n\\(r_k - q_k\\)\n\n\nTotal\n\\(r_{1k}\\)\n\\(r_{2k}\\)\n\\(r_k\\)\n\n\n\n\nFocus on \\(q_{1k}\\)! Null hypothesis: across all \\(k \\in \\{1, \\ldots, K\\}\\), \\(q_{1k}\\) not systematically lower or higher than RHS:\n\n\\[\n\\mathbb{E}[ \\underbrace{q_{1k}}_{\\mathclap{\\small\\substack{\\text{Group 1 deaths} \\\\ \\text{at }d_k}}} ] =\n\\overbrace{r_{1k}}^{\\mathclap{\\small\\substack{\\text{At risk in} \\\\[0.2em] \\text{Group 1}}}} \\cdot \\underbrace{\\left( \\frac{q_k}{r_k} \\right)}_{\\mathclap{\\small\\text{Overall death rate at }d_k}}\n\\]"
  },
  {
    "objectID": "w12/slides.html#log-rank-test-statistic",
    "href": "w12/slides.html#log-rank-test-statistic",
    "title": "Week 12: Survival Analysis",
    "section": "Log-Rank Test Statistic",
    "text": "Log-Rank Test Statistic\n\nTest statistic \\(W\\) should ‚Äúdetect‚Äù the alternative hypothesis, on basis of information \\(X\\)‚Ä¶ We can use \\(\\boxed{\\textstyle X = \\sum_{k=1}^{K}q_{1k}}\\)!\nHere, log-rank test statistic \\(W\\) detects how much \\(q_{1k}\\) deviates from expected value from prev slide:\n\n\\[\n\\begin{align*}\nW &= \\frac{X - \\mathbb{E}[X]}{\\sqrt{\\text{Var}[X]}} = \\frac{\n  \\sum_{k=1}^{K}q_{1k} - \\mathbb{E}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]\n}{\n  \\sqrt{\\text{Var}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]}\n} = \\frac{\n  \\sum_{k=1}^{K} \\left( q_{1k} - \\mathbb{E}[q_{1k}] \\right)\n}{\n  \\sqrt{\\text{Var}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]}\n} \\\\\n&= \\frac{\n  \\sum_{k=1}^{K}\\left( q_{1k} - r_{1k}\\cdot \\frac{q_k}{r_k} \\right)\n}{\n  \\sqrt{\\text{Var}\\mkern-3mu\\left[ \\sum_{k=1}^{K}q_{1k} \\right]}\n} \\underset{\\small\\text{Ex 11.7}}{\\overset{\\small\\text{ISLR}}{=}}\n\\frac{\n  \\sum_{k=1}^{K}\\left( q_{1k} - r_{1k}\\cdot \\frac{q_k}{r_k} \\right)\n}{\n  \\sqrt{\\sum_{k=1}^{K} \\frac{\n    q_k(r_{1k}/r_k)(1 - r_{1k}/r_k)(r_k - q_k)\n  }{\n    r_k - 1\n  }}\n}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w12/slides.html#the-hazard-function",
    "href": "w12/slides.html#the-hazard-function",
    "title": "Week 12: Survival Analysis",
    "section": "The Hazard Function",
    "text": "The Hazard Function\n\nDeath rate at tiny instant after \\(t\\) (between \\(t\\) and \\(t + \\Delta t\\)), given survival past \\(t\\):\n\n\\[\nh(t) \\definedas \\lim_{\\Delta t \\rightarrow 0}\\frac{\\Pr(t &lt; T \\leq t + \\Delta t)}{\\Delta t}\n\\]\n\nIf we define a RV \\(T_{&gt;t} \\definedas [T \\mid T &gt; t]\\), \\(h(t)\\) is the pdf of \\(T_{&gt;t}\\)!\nCan relate \\(h(t)\\) to quantities we know (e.g., from 5100):\n\n\\[\n\\underbrace{h(t)}_{\\small\\text{pdf of }T_{&gt;t}} = \\frac{\\overbrace{f(t)}^{\\small\\text{pdf of }T}}{\\underbrace{S(t)}_{\\small \\Pr(T &gt; t)}}\n\\]"
  },
  {
    "objectID": "w12/slides.html#proportional-hazard-assumption",
    "href": "w12/slides.html#proportional-hazard-assumption",
    "title": "Week 12: Survival Analysis",
    "section": "Proportional Hazard Assumption",
    "text": "Proportional Hazard Assumption\n\\[\nh(t \\mid x_i) = h_0(t)\\exp\\left[ \\sum_{j=1}^{p}\\beta_j x_{ij} \\right] \\iff \\underbrace{\\log[h(t \\mid x_i)]}_{\\hbar(t \\mid x_i)} = \\underbrace{\\log[h_0(t)]}_{\\hbar_0(t)} + \\sum_{j=1}^{p}\\beta_j x_{ij}\n\\]\nBasically: Features \\(X_{j}\\) shift [log] baseline hazard function \\(\\hbar_0(t)\\) up and down by constant amounts, via multiplication by \\(e^{\\beta_j}\\)\n\n\n\n\nTop row: \\(\\hbar_0(t)\\) in black, \\(X_j = 1\\) shifts it down via multiplication by \\(e^{\\beta_j}\\) to form \\(\\hbar(t \\mid X_j)\\) in green\nBottom row: Proportional hazard violated, since \\(X_j = 1\\) associated with different changes to \\(\\hbar_0(t)\\) at different \\(t\\) values"
  },
  {
    "objectID": "w12/slides.html#cox-proportional-hazard-model",
    "href": "w12/slides.html#cox-proportional-hazard-model",
    "title": "Week 12: Survival Analysis",
    "section": "Cox Proportional Hazard Model",
    "text": "Cox Proportional Hazard Model\n\nIntuition: Best \\(\\beta\\)s are those which best predict \\(i\\)‚Äôs death among all at risk at same time. Called ‚ÄúPartial Likelihood‚Äù \\(\\text{PL}(\\boldsymbol\\beta)\\) since we don‚Äôt need to estimate \\(h_0(t)\\)!\n\n\\[\n\\prod_{i : \\, \\delta_i = 1} \\; \\frac{\n  {\\color{red}\\cancel{\\color{black}h_0(t)}}\\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{ij} \\right]\n}{\n  {\\displaystyle \\sum\\limits_{\\mathclap{i': \\, y_{i'} \\geq y_i}}} {\\color{red}\\cancel{\\color{black}h_0(t)}}\\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{i'j} \\right]\n} = \\prod_{i : \\, \\delta_i = 1} \\; \\frac{\n  \\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{ij} \\right]\n}{\n  {\\displaystyle \\sum\\limits_{\\mathclap{i': \\, y_{i'} \\geq y_i}}} \\exp\\mkern-3mu\\left[ \\sum_{j=1}^{p} \\beta_j x_{i'j} \\right]\n}\n\\]\n\nAlso note the missing intercept \\(\\beta_0\\): handled by the baseline hazard function \\(h_0(t)\\) (which we cancel out anyways!)"
  },
  {
    "objectID": "w12/slides.html#in-code",
    "href": "w12/slides.html#in-code",
    "title": "Week 12: Survival Analysis",
    "section": "In Code",
    "text": "In Code\n\nWe‚Äôll use the survival library in R, very similar syntax to lm(), glm(), etc.!\n\n\n\nCode\nlibrary(survival) |&gt; suppressPackageStartupMessages()\nlibrary(ISLR2) |&gt; suppressPackageStartupMessages()\nbc_df &lt;- BrainCancer |&gt; filter(diagnosis != \"Other\")\nbc_df$diagnosis = factor(bc_df$diagnosis)\nbc_df$sex &lt;- factor(substr(bc_df$sex, 1, 1))\nbc_df$loc &lt;- factor(substr(bc_df$loc, 1, 5))\noptions(width=130)\nsummary(bc_df)\n\n\n sex         diagnosis     loc           ki              gtv         stereo       status           time      \n F:39   Meningioma:42   Infra:10   Min.   : 40.00   Min.   : 0.040   SRS:19   Min.   :0.000   Min.   : 0.07  \n M:34   LG glioma : 9   Supra:63   1st Qu.: 80.00   1st Qu.: 2.500   SRT:54   1st Qu.:0.000   1st Qu.: 9.77  \n        HG glioma :22              Median : 80.00   Median : 6.480            Median :0.000   Median :26.46  \n                                   Mean   : 81.37   Mean   : 8.297            Mean   :0.411   Mean   :27.83  \n                                   3rd Qu.: 90.00   3rd Qu.:11.380            3rd Qu.:1.000   3rd Qu.:41.44  \n                                   Max.   :100.00   Max.   :33.690            Max.   :1.000   Max.   :82.56  \n\n\n\n\nCode\nbc_df |&gt; head(5)\n\n\n\n\n\n\nsex\ndiagnosis\nloc\nki\ngtv\nstereo\nstatus\ntime\n\n\n\n\nF\nMeningioma\nInfra\n90\n6.11\nSRS\n0\n57.64\n\n\nM\nHG glioma\nSupra\n90\n19.35\nSRT\n1\n8.98\n\n\nF\nMeningioma\nInfra\n70\n7.95\nSRS\n0\n26.46\n\n\nF\nLG glioma\nSupra\n80\n7.61\nSRT\n1\n47.80\n\n\nM\nHG glioma\nSupra\n90\n5.06\nSRT\n1\n6.30"
  },
  {
    "objectID": "w12/slides.html#coxph-estimation",
    "href": "w12/slides.html#coxph-estimation",
    "title": "Week 12: Survival Analysis",
    "section": "coxph() Estimation",
    "text": "coxph() Estimation\n\n\n\nCode\nlibrary(broom) |&gt; suppressPackageStartupMessages()\nfull_cox_model &lt;- coxph(\n  Surv(time, status) ~ sex + diagnosis + loc + ki + gtv + stereo,\n  data=bc_df\n)\nbroom::tidy(full_cox_model) |&gt; mutate_if(is.numeric, round, 3)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nsexM\n0.456\n0.396\n1.154\n0.249\n\n\ndiagnosisLG glioma\n0.864\n0.641\n1.347\n0.178\n\n\ndiagnosisHG glioma\n2.116\n0.470\n4.504\n0.000\n\n\nlocSupra\n1.482\n1.105\n1.342\n0.180\n\n\nki\n-0.048\n0.020\n-2.406\n0.016\n\n\ngtv\n0.036\n0.026\n1.361\n0.173\n\n\nstereoSRT\n-0.475\n0.591\n-0.803\n0.422\n\n\n\n\n\n\n\nDiagnosis: Relative to baseline of Meningioma, HG (High-Grade) glioma associated with \\(e^{2.155} \\approx 8.628\\) times greater hazard\nKarnofsky Index (ki): 1-unit increase associated with reduction of hazard to \\(e^{-0.055} \\approx 94.65\\)% of previous value [0-100 scale of self-functioning abilities]"
  },
  {
    "objectID": "w12/slides.html#survival-curves-for-each-diagnosis",
    "href": "w12/slides.html#survival-curves-for-each-diagnosis",
    "title": "Week 12: Survival Analysis",
    "section": "Survival Curves for Each Diagnosis",
    "text": "Survival Curves for Each Diagnosis\n\n\nCode\nlibrary(extrafont) |&gt; suppressPackageStartupMessages()\npar(cex=1.2, family=\"CMU Sans Serif\")\ndiag_levels &lt;- c(\"Meningioma\", \"LG glioma\", \"HG glioma\")\ndiag_df &lt;- tibble(\n  diagnosis = diag_levels,\n  sex = rep(\"F\", 3),\n  loc = rep(\"Supra\", 3),\n  ki = rep(mean(bc_df$ki), 3),\n  gtv = rep(mean(bc_df$gtv), 3),\n  stereo = rep(\"SRT\", 3)\n)\nsurvplots &lt;- survfit(full_cox_model, newdata = diag_df)\nplot(\n  survplots,\n  main = \"Survival Curves by Diagnosis\",\n  xlab = \"Months\", ylab = \"Survival Probability\",\n  col = cb_palette, lwd=1.5\n)\nlegend(\n  \"bottomleft\",\n  diag_levels,\n  col = cb_palette, lty = 1, lwd=1.5\n)"
  },
  {
    "objectID": "w12/slides.html#less-straightforward-curves-for-each-ki-gtv-val",
    "href": "w12/slides.html#less-straightforward-curves-for-each-ki-gtv-val",
    "title": "Week 12: Survival Analysis",
    "section": "Less Straightforward: Curves for Each ki, gtv Val",
    "text": "Less Straightforward: Curves for Each ki, gtv Val\n\nTechnically a different survival curve for each value of ki \\(\\in [0, 100]\\), gtv \\(\\in (0, \\infty)\\)\n\n\n\n\n\nCode\nbc_df |&gt; ggplot(aes(x=ki)) +\n  geom_density(\n    linewidth=g_linewidth,\n    fill=cb_palette[1], alpha=0.333\n  ) +\n  theme_dsan(base_size=28) +\n  labs(title = \"Karnofsky Index Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nbc_df |&gt; ggplot(aes(x=gtv)) +\n  geom_density(\n    linewidth=g_linewidth,\n    fill=cb_palette[1], alpha=0.333\n  ) +\n  theme_dsan(base_size=28) +\n  labs(title = \"Gross Tumor Volume (GTV) Distribution\")\n\n\n\n\n\n\n\n\n\n\n\nOne approach: bin into (low, medium, high) via terciles, one curve per bin median:\n\n\n\n\n\nCode\nki_terciles &lt;- quantile(bc_df$ki, c(1/3, 2/3))\nbc_df &lt;- bc_df |&gt; mutate(\n  tercile = ifelse(ki &lt; ki_terciles[1], 1, ifelse(ki &lt; ki_terciles[2], 2, 3))\n)\n(terc_df &lt;- bc_df |&gt;\n  group_by(tercile) |&gt;\n  summarize(med_ki=median(ki)))\n\n\n\n\n\n\ntercile\nmed_ki\n\n\n\n\n1\n70\n\n\n2\n80\n\n\n3\n90\n\n\n\n\n\n\n\n\n\nCode\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nki_df &lt;- tibble(\n  diagnosis = rep(\"Meningioma\", 3),\n  sex = rep(\"F\", 3),\n  loc = rep(\"Supra\", 3),\n  ki = terc_df$med_ki,\n  gtv = rep(mean(bc_df$gtv), 3),\n  stereo = rep(\"SRT\", 3)\n)\nki_plots &lt;- survfit(full_cox_model, newdata = ki_df)\npar(\n  mar=c(4.0,4.0,1.2,0.5),\n  cex=1.2,\n  family=\"CMU Sans Serif\"\n) # bltr\nplot(\n  ki_plots,\n  main = \"Survival Curves by KI Tercile\",\n  xlab = \"Months\",\n  ylab = TeX(\"$\\\\Pr(T &gt; t)$\"),\n  lwd = 1,\n  col = cb_palette\n)\nki_labs &lt;- c(\n  TeX(\"$h( t \\\\, | \\\\, KI = 70 )$\"),\n  TeX(\"$h( t \\\\, | \\\\, KI = 80 )$\"),\n  TeX(\"$h( t \\\\, | \\\\, KI = 90 )$\")\n)\nlegend(\n  \"bottomleft\",\n  ki_labs, lwd=1,\n  col = cb_palette, lty = 1, cex=0.8\n)"
  }
]